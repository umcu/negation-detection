{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation MetaCAT - BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stan3/Data/MedCAT/medcat/cat.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from medcat.cat import CAT\n",
    "from medcat.vocab import Vocab\n",
    "from medcat.cdb import CDB\n",
    "from medcat.config import Config\n",
    "from medcat.meta_cat import MetaCAT\n",
    "from medcat.preprocessing.tokenizers import TokenizerWrapperBPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "data_dir = os.path.join('..', 'data')\n",
    "cdb_file = os.path.join(data_dir, 'cdb.dat')\n",
    "vocab_file = os.path.join(data_dir, 'vocab.dat')\n",
    "\n",
    "# Output\n",
    "output_dir = 'output'\n",
    "tokenizer_name = 'emc_dcc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, train and save the tokenizer\n",
    "mc_negation = MetaCAT(save_dir=output_dir, device='cpu')\n",
    "mc_negation.load(tokenizer_name=tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cdb and vocab \n",
    "config = Config()\n",
    "\n",
    "vocab = Vocab.load(vocab_file)\n",
    "cdb = CDB.load(cdb_file)\n",
    "\n",
    "# Create MedCAT pipeline\n",
    "cat = CAT(cdb=cdb, vocab=vocab, config=config, meta_cats=[mc_negation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: heup\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.9943742, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: heupdysplasie\n",
      "Meta Annotations: {'Negation': {'value': 'negated', 'confidence': 0.9997235, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on DL1114 from DCC with negation\n",
    "text = 'Echo- en rontgenonderzoek van de heup toont geen evidente heupdysplasie.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: heup\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.99893314, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: heupdysplasie\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.99205214, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on DL1114 from DCC without negation\n",
    "text = 'Echo- en rontgenonderzoek van de heup toont evidente heupdysplasie.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on DL1112 from DCC\n",
    "text = 'Patient kan zich geen trauma herinneren.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Trauma is not identified as medical concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: operatie\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.999671, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: buikpijn\n",
      "Meta Annotations: {'Negation': {'value': 'negated', 'confidence': 0.94574124, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on NTvG article\n",
    "# https://www.ntvg.nl/artikelen/acute-buik-op-basis-van-een-wandelende-milt\n",
    "text = '1 maand na de operatie had patiënte geen buikpijn meer en was zij goed hersteld.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# The negation was missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: darmobstructie\n",
      "Meta Annotations: {'Negation': {'value': 'negated', 'confidence': 0.9341217, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: zien\n",
      "Meta Annotations: {'Negation': {'value': 'negated', 'confidence': 0.8449472, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on NTvG article\n",
    "# https://www.ntvg.nl/artikelen/een-bezoar-bij-een-vrouw-met-clomipramine-intoxicatie\n",
    "text = 'Er waren geen tekenen van darmobstructie te zien.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Correct identification of negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: patiënten\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.9957676, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: controlegroep\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.99965554, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: SARS-CoV\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.59226084, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: infectie\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.9611355, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on NTvG article\n",
    "# https://www.ntvg.nl/artikelen/nieuws/vaker-ziek-na-acute-fase-covid-19\n",
    "text = 'Alle patiënten werden gematcht met een controlegroep bij wie geen SARS-CoV-2-infectie was geregistreerd.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# Negation on SARS-CoV was missed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate MetaCat on subsets of the data\n",
    "The ContextD paper calculates precision, recall and F1-score on subsets of the data. In this section we calculate the same scores with the just created model. Note that this results in a calculation on a set of data that was included during the training phase. For proper score calculations, we will do cross validation at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_DL = os.path.join(data_dir, 'emc-dcc_ann_DL.json')\n",
    "json_file_GP = os.path.join(data_dir, 'emc-dcc_ann_GP.json')\n",
    "json_file_RD = os.path.join(data_dir, 'emc-dcc_ann_RD.json')\n",
    "json_file_SP = os.path.join(data_dir, 'emc-dcc_ann_SP.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.81      0.87       595\n",
      "           1       0.96      0.99      0.98      3088\n",
      "\n",
      "    accuracy                           0.96      3683\n",
      "   macro avg       0.95      0.90      0.92      3683\n",
      "weighted avg       0.96      0.96      0.96      3683\n",
      "\n",
      "Test Loss:  0.12919218332280177\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9412915851272016,\n",
       "  'recall': 0.8084033613445378,\n",
       "  'f1-score': 0.8698010849909584,\n",
       "  'support': 595},\n",
       " '1': {'precision': 0.9640605296343001,\n",
       "  'recall': 0.9902849740932642,\n",
       "  'f1-score': 0.9769968051118211,\n",
       "  'support': 3088},\n",
       " 'accuracy': 0.9609014390442574,\n",
       " 'macro avg': {'precision': 0.9526760573807509,\n",
       "  'recall': 0.899344167718901,\n",
       "  'f1-score': 0.9233989450513898,\n",
       "  'support': 3683},\n",
       " 'weighted avg': {'precision': 0.9603821364815106,\n",
       "  'recall': 0.9609014390442574,\n",
       "  'f1-score': 0.9596790061783664,\n",
       "  'support': 3683}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_negation.eval(json_file_RD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.69      0.78       416\n",
      "           1       0.95      0.99      0.97      2309\n",
      "\n",
      "    accuracy                           0.94      2725\n",
      "   macro avg       0.93      0.84      0.87      2725\n",
      "weighted avg       0.94      0.94      0.94      2725\n",
      "\n",
      "Test Loss:  0.23037438435546523\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9076433121019108,\n",
       "  'recall': 0.6850961538461539,\n",
       "  'f1-score': 0.7808219178082193,\n",
       "  'support': 416},\n",
       " '1': {'precision': 0.9456656988801327,\n",
       "  'recall': 0.9874404504114335,\n",
       "  'f1-score': 0.9661016949152542,\n",
       "  'support': 2309},\n",
       " 'accuracy': 0.9412844036697248,\n",
       " 'macro avg': {'precision': 0.9266545054910218,\n",
       "  'recall': 0.8362683021287937,\n",
       "  'f1-score': 0.8734618063617368,\n",
       "  'support': 2725},\n",
       " 'weighted avg': {'precision': 0.9398611803848151,\n",
       "  'recall': 0.9412844036697248,\n",
       "  'f1-score': 0.9378167821532262,\n",
       "  'support': 2725}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_negation.eval(json_file_SP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92       379\n",
      "           1       0.98      1.00      0.99      2417\n",
      "\n",
      "    accuracy                           0.98      2796\n",
      "   macro avg       0.97      0.94      0.95      2796\n",
      "weighted avg       0.98      0.98      0.98      2796\n",
      "\n",
      "Test Loss:  0.07555809380885746\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9652173913043478,\n",
       "  'recall': 0.8786279683377308,\n",
       "  'f1-score': 0.9198895027624309,\n",
       "  'support': 379},\n",
       " '1': {'precision': 0.9812321501427989,\n",
       "  'recall': 0.9950351675630947,\n",
       "  'f1-score': 0.9880854560394412,\n",
       "  'support': 2417},\n",
       " 'accuracy': 0.9792560801144492,\n",
       " 'macro avg': {'precision': 0.9732247707235733,\n",
       "  'recall': 0.9368315679504128,\n",
       "  'f1-score': 0.953987479400936,\n",
       "  'support': 2796},\n",
       " 'weighted avg': {'precision': 0.9790613369812207,\n",
       "  'recall': 0.9792560801144492,\n",
       "  'f1-score': 0.9788414409135517,\n",
       "  'support': 2796}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_negation.eval(json_file_DL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.72      0.79       383\n",
      "           1       0.96      0.99      0.98      3024\n",
      "\n",
      "    accuracy                           0.96      3407\n",
      "   macro avg       0.92      0.85      0.88      3407\n",
      "weighted avg       0.96      0.96      0.95      3407\n",
      "\n",
      "Test Loss:  0.1529528521001339\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.8782051282051282,\n",
       "  'recall': 0.7154046997389034,\n",
       "  'f1-score': 0.7884892086330936,\n",
       "  'support': 383},\n",
       " '1': {'precision': 0.9647819063004847,\n",
       "  'recall': 0.9874338624338624,\n",
       "  'f1-score': 0.9759764667429318,\n",
       "  'support': 3024},\n",
       " 'accuracy': 0.956853536835926,\n",
       " 'macro avg': {'precision': 0.9214935172528065,\n",
       "  'recall': 0.851419281086383,\n",
       "  'f1-score': 0.8822328376880126,\n",
       "  'support': 3407},\n",
       " 'weighted avg': {'precision': 0.9550493245539271,\n",
       "  'recall': 0.956853536835926,\n",
       "  'f1-score': 0.9548999713346348,\n",
       "  'support': 3407}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_negation.eval(json_file_GP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
