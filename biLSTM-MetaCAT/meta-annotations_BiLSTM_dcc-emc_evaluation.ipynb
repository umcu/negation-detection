{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation MetaCAT - BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stan3/Data/MedCAT/medcat/cat.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import os\n",
    "\n",
    "from medcat.cat import CAT\n",
    "from medcat.vocab import Vocab\n",
    "from medcat.cdb import CDB\n",
    "from medcat.config import Config\n",
    "from medcat.meta_cat import MetaCAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "data_dir = os.path.join('..', 'data')\n",
    "cdb_file = os.path.join(data_dir, 'cdb.dat')\n",
    "vocab_file = os.path.join(data_dir, 'vocab.dat')\n",
    "\n",
    "# Output\n",
    "output_dir = 'output'\n",
    "\n",
    "# Name should contain 'bbpe' for ByteLevelBPETokenizer or 'bert' for BertTokenizerFast\n",
    "tokenizer_name = 'bbpe_dutch-wikipedia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, train and save the tokenizer\n",
    "mc_negation = MetaCAT()\n",
    "mc_negation = mc_negation.load(save_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cdb and vocab \n",
    "config = Config()\n",
    "\n",
    "vocab = Vocab.load(vocab_file)\n",
    "cdb = CDB.load(cdb_file)\n",
    "\n",
    "# Create MedCAT pipeline\n",
    "cat = CAT(cdb=cdb, vocab=vocab, config=config, meta_cats=[mc_negation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: heup\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.9889043, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: heupdysplasie\n",
      "Meta Annotations: {'Negation': {'value': 'negated', 'confidence': 0.9601904, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on DL1114 from DCC with negation\n",
    "text = 'Echo- en rontgenonderzoek van de heup toont geen evidente heupdysplasie.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: heup\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.9992647, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: heupdysplasie\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.99737585, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on DL1114 from DCC without negation\n",
    "text = 'Echo- en rontgenonderzoek van de heup toont evidente heupdysplasie.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on DL1112 from DCC\n",
    "text = 'Patient kan zich geen trauma herinneren.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Trauma is not identified as medical concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: operatie\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.99938786, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: buikpijn\n",
      "Meta Annotations: {'Negation': {'value': 'negated', 'confidence': 0.90848744, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on NTvG article\n",
    "# https://www.ntvg.nl/artikelen/acute-buik-op-basis-van-een-wandelende-milt\n",
    "text = '1 maand na de operatie had patiënte geen buikpijn meer en was zij goed hersteld.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# The negation was missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: darmobstructie\n",
      "Meta Annotations: {'Negation': {'value': 'negated', 'confidence': 0.6965348, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: zien\n",
      "Meta Annotations: {'Negation': {'value': 'negated', 'confidence': 0.62835985, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on NTvG article\n",
    "# https://www.ntvg.nl/artikelen/een-bezoar-bij-een-vrouw-met-clomipramine-intoxicatie\n",
    "text = 'Er waren geen tekenen van darmobstructie te zien.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Correct identification of negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: patiënten\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.99108285, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: controlegroep\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.9961759, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: SARS-CoV\n",
      "Meta Annotations: {'Negation': {'value': 'negated', 'confidence': 0.9215692, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: infectie\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.9547335, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on NTvG article\n",
    "# https://www.ntvg.nl/artikelen/nieuws/vaker-ziek-na-acute-fase-covid-19\n",
    "text = 'Alle patiënten werden gematcht met een controlegroep bij wie geen SARS-CoV-2-infectie was geregistreerd.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# Negation on SARS-CoV was missed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate MetaCat on subsets of the data\n",
    "The ContextD paper calculates precision, recall and F1-score on subsets of the data. In this section we calculate the same scores with the just created model. Note that this results in a calculation on a set of data that was included during the training phase. For proper score calculations, we will do cross validation at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_DL = os.path.join(data_dir, 'emc-dcc_ann_DL.json')\n",
    "json_file_GP = os.path.join(data_dir, 'emc-dcc_ann_GP.json')\n",
    "json_file_RD = os.path.join(data_dir, 'emc-dcc_ann_RD.json')\n",
    "json_file_SP = os.path.join(data_dir, 'emc-dcc_ann_SP.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93       595\n",
      "           1       0.98      0.99      0.99      3088\n",
      "\n",
      "    accuracy                           0.98      3683\n",
      "   macro avg       0.97      0.95      0.96      3683\n",
      "weighted avg       0.98      0.98      0.98      3683\n",
      "\n",
      "Test Loss:  0.07618074667816227\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9621621621621622,\n",
       "  'recall': 0.8974789915966387,\n",
       "  'f1-score': 0.9286956521739131,\n",
       "  'support': 595},\n",
       " '1': {'precision': 0.9804987212276215,\n",
       "  'recall': 0.993199481865285,\n",
       "  'f1-score': 0.9868082368082367,\n",
       "  'support': 3088},\n",
       " 'accuracy': 0.9777355416779799,\n",
       " 'macro avg': {'precision': 0.9713304416948918,\n",
       "  'recall': 0.9453392367309619,\n",
       "  'f1-score': 0.9577519444910749,\n",
       "  'support': 3683},\n",
       " 'weighted avg': {'precision': 0.9775363936023301,\n",
       "  'recall': 0.9777355416779799,\n",
       "  'f1-score': 0.9774199696734492,\n",
       "  'support': 3683}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_negation.eval(json_file_RD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.71      0.81       416\n",
      "           1       0.95      0.99      0.97      2309\n",
      "\n",
      "    accuracy                           0.95      2725\n",
      "   macro avg       0.95      0.85      0.89      2725\n",
      "weighted avg       0.95      0.95      0.95      2725\n",
      "\n",
      "Test Loss:  0.2053001417246248\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9488817891373802,\n",
       "  'recall': 0.7139423076923077,\n",
       "  'f1-score': 0.8148148148148148,\n",
       "  'support': 416},\n",
       " '1': {'precision': 0.9506633499170812,\n",
       "  'recall': 0.9930705933304461,\n",
       "  'f1-score': 0.9714043634823131,\n",
       "  'support': 2309},\n",
       " 'accuracy': 0.9504587155963303,\n",
       " 'macro avg': {'precision': 0.9497725695272308,\n",
       "  'recall': 0.853506450511377,\n",
       "  'f1-score': 0.8931095891485639,\n",
       "  'support': 2725},\n",
       " 'weighted avg': {'precision': 0.9503913758677763,\n",
       "  'recall': 0.9504587155963303,\n",
       "  'f1-score': 0.947499316786651,\n",
       "  'support': 2725}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_negation.eval(json_file_SP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91       379\n",
      "           1       0.98      0.99      0.99      2417\n",
      "\n",
      "    accuracy                           0.98      2796\n",
      "   macro avg       0.97      0.93      0.95      2796\n",
      "weighted avg       0.98      0.98      0.98      2796\n",
      "\n",
      "Test Loss:  0.07943459508741009\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9620991253644315,\n",
       "  'recall': 0.8707124010554089,\n",
       "  'f1-score': 0.9141274238227147,\n",
       "  'support': 379},\n",
       " '1': {'precision': 0.9800244598450877,\n",
       "  'recall': 0.994621431526686,\n",
       "  'f1-score': 0.9872689938398358,\n",
       "  'support': 2417},\n",
       " 'accuracy': 0.9778254649499285,\n",
       " 'macro avg': {'precision': 0.9710617926047596,\n",
       "  'recall': 0.9326669162910475,\n",
       "  'f1-score': 0.9506982088312752,\n",
       "  'support': 2796},\n",
       " 'weighted avg': {'precision': 0.9775946666518943,\n",
       "  'recall': 0.9778254649499285,\n",
       "  'f1-score': 0.9773545964734235,\n",
       "  'support': 2796}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_negation.eval(json_file_DL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.73      0.82       383\n",
      "           1       0.97      0.99      0.98      3024\n",
      "\n",
      "    accuracy                           0.96      3407\n",
      "   macro avg       0.95      0.86      0.90      3407\n",
      "weighted avg       0.96      0.96      0.96      3407\n",
      "\n",
      "Test Loss:  0.11154603286247168\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9297658862876255,\n",
       "  'recall': 0.7258485639686684,\n",
       "  'f1-score': 0.81524926686217,\n",
       "  'support': 383},\n",
       " '1': {'precision': 0.9662162162162162,\n",
       "  'recall': 0.9930555555555556,\n",
       "  'f1-score': 0.9794520547945206,\n",
       "  'support': 3024},\n",
       " 'accuracy': 0.9630173172879366,\n",
       " 'macro avg': {'precision': 0.9479910512519208,\n",
       "  'recall': 0.859452059762112,\n",
       "  'f1-score': 0.8973506608283452,\n",
       "  'support': 3407},\n",
       " 'weighted avg': {'precision': 0.9621186299636039,\n",
       "  'recall': 0.9630173172879366,\n",
       "  'f1-score': 0.9609930974190904,\n",
       "  'support': 3407}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_negation.eval(json_file_GP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
