{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetaCAT - BiLSTM for negations of Dutch Clinical Corpus\n",
    "\n",
    "Based on https://colab.research.google.com/drive/1rxzBZCTDcqsIjRXZ3u4yRZFOkUCCuwyy#scrollTo=dukwUnN1TPCg\n",
    "and https://colab.research.google.com/drive/1zzV3XzFJ9ihhCJ680DaQV2QZ5XnHa06X#scrollTo=Sj29auXV8iPZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stan3/Data/MedCAT/medcat/cat.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from medcat.cat import CAT\n",
    "from medcat.vocab import Vocab\n",
    "from medcat.cdb import CDB\n",
    "from medcat.config import Config\n",
    "from medcat.meta_cat import MetaCAT\n",
    "from medcat.preprocessing.tokenizers import TokenizerWrapperBPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "data_dir = os.path.join('..', 'data')\n",
    "cdb_file = os.path.join(data_dir, 'cdb.dat')\n",
    "vocab_file = os.path.join(data_dir, 'vocab.dat')\n",
    "json_file_all = os.path.join(data_dir, 'emc-dcc_ann.json')\n",
    "text_file = os.path.join(data_dir, 'data.txt')\n",
    "\n",
    "# Output\n",
    "output_dir = 'output'\n",
    "tokenizer_name = 'emc_dcc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, train and save the tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/emc_dcc-vocab.json', 'output/emc_dcc-merges.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tokenizer\n",
    "tokenizer.save_model(output_dir, tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenize text and train with Word2Vec\n",
    "text_data = []\n",
    "with open(text_file, encoding='utf-8') as text:\n",
    "    for line in text:\n",
    "        text_data.append(tokenizer.encode(line).tokens)\n",
    "w2v = Word2Vec(text_data, size=300, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ġkortademigheid', 0.9008769989013672),\n",
       " ('Ġjeuk', 0.8975282311439514),\n",
       " ('Ġniezen', 0.8891239762306213),\n",
       " ('Ġbraken', 0.8868833780288696),\n",
       " ('Ġdiarree', 0.8819828629493713),\n",
       " ('Ġirritatie', 0.87400221824646),\n",
       " ('Ġovergeven', 0.8697969913482666),\n",
       " ('Ġzweten', 0.8684862852096558),\n",
       " ('Ġmisselijkheid', 0.8676348924636841),\n",
       " ('Ġbuikpijn', 0.8667157888412476)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check trained word2vec model\n",
    "# Ġ denotes start of word (a space)\n",
    "w2v.wv.most_similar('Ġhoesten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings matrix\n",
    "embeddings = []\n",
    "for i in range(tokenizer.get_vocab_size()):\n",
    "    word = tokenizer.id_to_token(i)\n",
    "    if word in w2v.wv:\n",
    "        embeddings.append(w2v.wv[word])\n",
    "    else:\n",
    "        # Assign a random vector if the word was not frequent enough to receive an embedding\n",
    "        embeddings.append(np.random.rand(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embeddings\n",
    "embeddings_file = os.path.join(output_dir, \"embeddings.npy\")\n",
    "np.save(open(embeddings_file, 'wb'), np.array(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change tokenizer to MedCAT's TokenizerWrapperBPE\n",
    "tokenizer = TokenizerWrapperBPE(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MetaCAT on all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 **************************************************  Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97      9758\n",
      "           1       0.88      0.71      0.78      1591\n",
      "\n",
      "    accuracy                           0.95     11349\n",
      "   macro avg       0.92      0.85      0.88     11349\n",
      "weighted avg       0.94      0.95      0.94     11349\n",
      "\n",
      "Epoch: 0 **************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      1080\n",
      "           1       0.89      0.75      0.81       182\n",
      "\n",
      "    accuracy                           0.95      1262\n",
      "   macro avg       0.93      0.87      0.89      1262\n",
      "weighted avg       0.95      0.95      0.95      1262\n",
      "\n",
      "Train Loss: 0.18905627497957206\n",
      "Test Loss:  0.18934118520701304\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "Model saved at epoch: 0 and f1: 0.9490017641810792\n",
      "[[1064   16]\n",
      " [  46  136]]\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 1 **************************************************  Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98      9758\n",
      "           1       0.92      0.77      0.84      1591\n",
      "\n",
      "    accuracy                           0.96     11349\n",
      "   macro avg       0.94      0.88      0.91     11349\n",
      "weighted avg       0.96      0.96      0.96     11349\n",
      "\n",
      "Epoch: 1 **************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      1080\n",
      "           1       0.91      0.76      0.83       182\n",
      "\n",
      "    accuracy                           0.95      1262\n",
      "   macro avg       0.93      0.87      0.90      1262\n",
      "weighted avg       0.95      0.95      0.95      1262\n",
      "\n",
      "Train Loss: 0.15149532011251005\n",
      "Test Loss:  0.1667821314476896\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "Model saved at epoch: 1 and f1: 0.9522919729435904\n",
      "[[1066   14]\n",
      " [  44  138]]\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 2 **************************************************  Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      9758\n",
      "           1       0.92      0.78      0.84      1591\n",
      "\n",
      "    accuracy                           0.96     11349\n",
      "   macro avg       0.94      0.89      0.91     11349\n",
      "weighted avg       0.96      0.96      0.96     11349\n",
      "\n",
      "Epoch: 2 **************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      1080\n",
      "           1       0.90      0.76      0.82       182\n",
      "\n",
      "    accuracy                           0.95      1262\n",
      "   macro avg       0.93      0.87      0.90      1262\n",
      "weighted avg       0.95      0.95      0.95      1262\n",
      "\n",
      "Train Loss: 0.1465309141859622\n",
      "Test Loss:  0.1598691398394294\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 3 **************************************************  Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      9758\n",
      "           1       0.93      0.80      0.86      1591\n",
      "\n",
      "    accuracy                           0.96     11349\n",
      "   macro avg       0.95      0.90      0.92     11349\n",
      "weighted avg       0.96      0.96      0.96     11349\n",
      "\n",
      "Epoch: 3 **************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      1080\n",
      "           1       0.90      0.77      0.83       182\n",
      "\n",
      "    accuracy                           0.95      1262\n",
      "   macro avg       0.93      0.88      0.90      1262\n",
      "weighted avg       0.95      0.95      0.95      1262\n",
      "\n",
      "Train Loss: 0.1375184376579775\n",
      "Test Loss:  0.16282077721552923\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "Model saved at epoch: 3 and f1: 0.9534265586727555\n",
      "[[1064   16]\n",
      " [  41  141]]\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 4 **************************************************  Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      9758\n",
      "           1       0.92      0.80      0.85      1591\n",
      "\n",
      "    accuracy                           0.96     11349\n",
      "   macro avg       0.95      0.89      0.92     11349\n",
      "weighted avg       0.96      0.96      0.96     11349\n",
      "\n",
      "Epoch: 4 **************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98      1080\n",
      "           1       0.92      0.77      0.84       182\n",
      "\n",
      "    accuracy                           0.96      1262\n",
      "   macro avg       0.94      0.88      0.91      1262\n",
      "weighted avg       0.96      0.96      0.96      1262\n",
      "\n",
      "Train Loss: 0.13479504755019328\n",
      "Test Loss:  0.1826689198933309\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "Model saved at epoch: 4 and f1: 0.9557016806844422\n",
      "[[1067   13]\n",
      " [  41  141]]\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 5 **************************************************  Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      9758\n",
      "           1       0.92      0.82      0.87      1591\n",
      "\n",
      "    accuracy                           0.96     11349\n",
      "   macro avg       0.95      0.90      0.92     11349\n",
      "weighted avg       0.96      0.96      0.96     11349\n",
      "\n",
      "Epoch: 5 **************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97      1080\n",
      "           1       0.89      0.77      0.83       182\n",
      "\n",
      "    accuracy                           0.95      1262\n",
      "   macro avg       0.93      0.88      0.90      1262\n",
      "weighted avg       0.95      0.95      0.95      1262\n",
      "\n",
      "Train Loss: 0.12913937253949664\n",
      "Test Loss:  0.1490169076132588\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 6 **************************************************  Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      9758\n",
      "           1       0.92      0.82      0.87      1591\n",
      "\n",
      "    accuracy                           0.96     11349\n",
      "   macro avg       0.95      0.90      0.92     11349\n",
      "weighted avg       0.96      0.96      0.96     11349\n",
      "\n",
      "Epoch: 6 **************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97      1080\n",
      "           1       0.87      0.79      0.83       182\n",
      "\n",
      "    accuracy                           0.95      1262\n",
      "   macro avg       0.92      0.89      0.90      1262\n",
      "weighted avg       0.95      0.95      0.95      1262\n",
      "\n",
      "Train Loss: 0.12644962929221998\n",
      "Test Loss:  0.15563840905088\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 7 **************************************************  Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      9758\n",
      "           1       0.92      0.82      0.87      1591\n",
      "\n",
      "    accuracy                           0.96     11349\n",
      "   macro avg       0.95      0.90      0.92     11349\n",
      "weighted avg       0.96      0.96      0.96     11349\n",
      "\n",
      "Epoch: 7 **************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      1080\n",
      "           1       0.93      0.75      0.83       182\n",
      "\n",
      "    accuracy                           0.96      1262\n",
      "   macro avg       0.94      0.87      0.90      1262\n",
      "weighted avg       0.95      0.96      0.95      1262\n",
      "\n",
      "Train Loss: 0.12458753882264587\n",
      "Test Loss:  0.15714872218086384\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 8 **************************************************  Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      9758\n",
      "           1       0.92      0.81      0.86      1591\n",
      "\n",
      "    accuracy                           0.96     11349\n",
      "   macro avg       0.94      0.90      0.92     11349\n",
      "weighted avg       0.96      0.96      0.96     11349\n",
      "\n",
      "Epoch: 8 **************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97      1080\n",
      "           1       0.88      0.75      0.81       182\n",
      "\n",
      "    accuracy                           0.95      1262\n",
      "   macro avg       0.92      0.87      0.89      1262\n",
      "weighted avg       0.95      0.95      0.95      1262\n",
      "\n",
      "Train Loss: 0.12421824666671455\n",
      "Test Loss:  0.1594420277979225\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 **************************************************  Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      9758\n",
      "           1       0.92      0.82      0.86      1591\n",
      "\n",
      "    accuracy                           0.96     11349\n",
      "   macro avg       0.94      0.90      0.92     11349\n",
      "weighted avg       0.96      0.96      0.96     11349\n",
      "\n",
      "Epoch: 9 **************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98      1080\n",
      "           1       0.95      0.76      0.84       182\n",
      "\n",
      "    accuracy                           0.96      1262\n",
      "   macro avg       0.95      0.88      0.91      1262\n",
      "weighted avg       0.96      0.96      0.96      1262\n",
      "\n",
      "Train Loss: 0.12629681482786495\n",
      "Test Loss:  0.14113614341476932\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "Model saved at epoch: 9 and f1: 0.9568720847635588\n",
      "[[1072    8]\n",
      " [  44  138]]\n",
      "\n",
      "\n",
      "\n",
      "Best/Average scores: F1: 0.9568720847635588, P: 0.9583571728201138, R: 0.9587955625990491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.9568720847635588,\n",
       " 'p': 0.9583571728201138,\n",
       " 'r': 0.9587955625990491,\n",
       " 'cls_report': {'0': {'precision': 0.9605734767025089,\n",
       "   'recall': 0.9925925925925926,\n",
       "   'f1-score': 0.9763205828779599,\n",
       "   'support': 1080},\n",
       "  '1': {'precision': 0.9452054794520548,\n",
       "   'recall': 0.7582417582417582,\n",
       "   'f1-score': 0.8414634146341463,\n",
       "   'support': 182},\n",
       "  'accuracy': 0.9587955625990491,\n",
       "  'macro avg': {'precision': 0.9528894780772819,\n",
       "   'recall': 0.8754171754171753,\n",
       "   'f1-score': 0.9088919987560531,\n",
       "   'support': 1262},\n",
       "  'weighted avg': {'precision': 0.9583571728201138,\n",
       "   'recall': 0.9587955625990491,\n",
       "   'f1-score': 0.9568720847635588,\n",
       "   'support': 1262}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and evaluate MetaCAT on all negations of the EMC DCC dataset\n",
    "mc_negation = MetaCAT(tokenizer=tokenizer, embeddings=embeddings, pad_id=len(embeddings) -1, save_dir=output_dir, device='cpu')\n",
    "mc_negation.train(json_file_all, 'Negation', nepochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cdb and vocab \n",
    "config = Config()\n",
    "\n",
    "vocab = Vocab.load(vocab_file)\n",
    "cdb = CDB.load(cdb_file)\n",
    "\n",
    "# Create MedCAT pipeline\n",
    "cat = CAT(cdb=cdb, vocab=vocab, config=config, meta_cats=[mc_negation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: heup\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.97756976, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: heupdysplasie\n",
      "Meta Annotations: {'Negation': {'value': 'negated', 'confidence': 0.9964055, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on DL1114 from DCC with negation\n",
    "text = 'Echo- en rontgenonderzoek van de heup toont geen evidente heupdysplasie.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: heup\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.99538463, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: heupdysplasie\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.990191, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on DL1114 from DCC without negation\n",
    "text = 'Echo- en rontgenonderzoek van de heup toont evidente heupdysplasie.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional testing and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More tests and evaluation can be found in the evaluation notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
