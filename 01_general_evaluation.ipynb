{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271b4fe3",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c28e36",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2297603330.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/hn/_9bf2hvd5hvb_2g5tx8jhhvw0000gp/T/ipykernel_25797/2297603330.py\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    'average_method': 'Weighting average',\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from evaluation_utils import print_statistics, get_document_text\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Categories in DCC dataset\n",
    "DOCUMENT_CATEGORIES = ['DL', 'GP', 'RD', 'SP']\n",
    "\n",
    "# Primary predictions methods\n",
    "PREDICTION_METHODS = ['rule_based', 'bilstm_cv', 'robbert_512']\n",
    "\n",
    "# Pretty names:\n",
    "PRETTY_NAMES = {'rule_based': 'Rule-based', \n",
    "                'bilstm_cv': 'BiLSTM', \n",
    "                'robbert_512': 'RobBERT',\n",
    "                'ensemble': 'Ensemble'\n",
    "                'average_method': 'Weighting average', \n",
    "                'category': 'Letter category',\n",
    "                'prediction_method': 'Prediction method',\n",
    "                'precision': 'Precision', \n",
    "                'recall': 'Recall',\n",
    "                'f1': 'F1',\n",
    "                'GP': 'General Practitioner entries',\n",
    "                'SP': 'Specialist letters',\n",
    "                'RD': 'Radiology reports',\n",
    "                'DL': 'Discharge letters'}\n",
    "\n",
    "# See https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
    "AVERAGE_METHODS = ['binary', 'micro', 'macro', 'weighted']\n",
    "\n",
    "# Input and output\n",
    "data_dir = Path('data')\n",
    "annotation_file = data_dir / 'emc-dcc_ann.json'\n",
    "dcc_dir = data_dir / 'EMCDutchClinicalCorpus'\n",
    "result_dir = Path('results')\n",
    "bilstm_predictions_file = result_dir / 'bilstm_predictions.csv.gz'\n",
    "bilstm_predictions_cv_file = result_dir / 'bilstm_predictions_cv.csv.gz'\n",
    "rule_based_predictions_file = result_dir / 'rule-based_predictions.csv.gz'\n",
    "robbert_predictions_file = result_dir / 'robbert_predictions.csv.gz'\n",
    "merged_predictions_file = result_dir / 'merged_predictions.csv.gz'\n",
    "\n",
    "# Load annotated data\n",
    "with open(annotation_file) as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0c13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load biLSTM (complete model) predictions\n",
    "bilstm_predictions = pd.read_csv(bilstm_predictions_file, sep=',')\n",
    "print(bilstm_predictions.shape)\n",
    "bilstm_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4da107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load biLSTM (cross validation) predictions\n",
    "bilstm_predictions_cv = pd.read_csv(bilstm_predictions_cv_file)\n",
    "print(bilstm_predictions_cv.shape)\n",
    "bilstm_predictions_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rule based predictions\n",
    "ruled_based_predictions = pd.read_csv(rule_based_predictions_file)\n",
    "ruled_based_predictions.drop(['annotation'], axis=1, inplace=True)\n",
    "print(ruled_based_predictions.shape)\n",
    "ruled_based_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load robbert predictions\n",
    "robbert_predictions = pd.read_csv(robbert_predictions_file)\n",
    "robbert_predictions.drop(['category', 'label', 'bilstm', 'bilstm_cv', 'rule_based'], axis=1, inplace=True)\n",
    "robbert_predictions.rename(columns={'robbert_512_2': 'robbert_512',\n",
    "                                    'robbert_128_2': 'robbert_128',\n",
    "                                    'robbert_32_2': 'robbert_32'}, inplace=True)\n",
    "robbert_predictions.shape\n",
    "robbert_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d62d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotated data\n",
    "annotated_records = []\n",
    "for document in annotations['projects'][0]['documents']:\n",
    "    document_name = document['name']\n",
    "    text = document['text']\n",
    "\n",
    "    for annotation in document['annotations']:\n",
    "\n",
    "        # Extract data\n",
    "        start_char = annotation['start']\n",
    "        end_char = annotation['end']\n",
    "        negation_value = annotation['meta_anns']['Negation']['value']\n",
    "\n",
    "        # Create custom ID\n",
    "        entity_id = f'{document_name}_{start_char}_{end_char}'\n",
    "        \n",
    "        # Extract category\n",
    "        if 'DL' in document_name:\n",
    "            category = 'DL'\n",
    "        elif 'GP' in document_name:\n",
    "            category = 'GP'\n",
    "        elif 'RD' in document_name:\n",
    "            category = 'RD'\n",
    "        else:\n",
    "            category = 'SP'\n",
    "        \n",
    "        # Create row\n",
    "        annotated_records.append([entity_id, category, negation_value])\n",
    "\n",
    "annotated_data = pd.DataFrame(annotated_records, columns=['entity_id', 'category', 'label'])\n",
    "print(annotated_data.shape)\n",
    "annotated_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb1ee5",
   "metadata": {},
   "source": [
    "## Merge annotations from different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca76bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.merge(left=annotated_data, right = bilstm_predictions, left_on='entity_id', right_on='entity_id')\n",
    "predictions = pd.merge(left=predictions, right = bilstm_predictions_cv, left_on='entity_id', right_on='entity_id')\n",
    "predictions = pd.merge(left=predictions, right = ruled_based_predictions, left_on='entity_id', right_on='entity_id')\n",
    "predictions = pd.merge(left=predictions, right = robbert_predictions, left_on='entity_id', right_on='entity_id')\n",
    "predictions.to_csv(merged_predictions_file, index=False, compression='gzip', line_terminator='\\n')\n",
    "print(predictions.shape)\n",
    "predictions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1ed72e",
   "metadata": {},
   "source": [
    "## Function to calculate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f3ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_score_record(prediction_df, prediction_method, average_method, category='all'):\n",
    "    \"\"\"\n",
    "    Wrapper for precision_recall_fscore_support().\n",
    "\n",
    "    Returns list containing method parameters and scores that can be used as row in DataFrame.\n",
    "    See https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
    "    \"\"\"\n",
    "    if average_method == 'binary':\n",
    "        pos_label = 'negated'\n",
    "    else:\n",
    "        pos_label=1\n",
    "    \n",
    "     # Using dropna because our robbert implementation does not work for some examples.\n",
    "    if category == 'all':\n",
    "        # Calculate score for all categories. \n",
    "        subset = prediction_df[['label', prediction_method]].dropna()\n",
    "    else:\n",
    "        # Calculate scores for specific category\n",
    "        subset = prediction_df.loc[prediction_df.category == category, ['label', prediction_method]].dropna()\n",
    "    return [average_method, category, prediction_method] + (list(precision_recall_fscore_support(subset.label, subset[prediction_method], labels=['negated', 'not negated'], average = average_method, pos_label=pos_label)))\n",
    "\n",
    "def create_score_dataframe(score_list):\n",
    "    scores = pd.DataFrame(score_list, columns=['average_method', 'category', 'prediction_method', 'precision', 'recall', 'f1', 'support'])\n",
    "    scores.drop(['support'], axis=1, inplace=True)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae6d6b",
   "metadata": {},
   "source": [
    "## Compare different ways of calculating scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5c9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_method = 'bilstm_cv'\n",
    "score_list = []\n",
    "for average_method in AVERAGE_METHODS:\n",
    "    score_list.append(create_score_record(predictions, prediction_method, average_method, category = 'all'))\n",
    "create_score_dataframe(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f288d15",
   "metadata": {},
   "source": [
    "## Scores per category (average: binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8de822",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_method = 'binary'\n",
    "score_list = []\n",
    "for category in DOCUMENT_CATEGORIES:\n",
    "    for prediction_method in PREDICTION_METHODS:\n",
    "        score_list.append(create_score_record(predictions, prediction_method, average_method, category = category))\n",
    "score_df = create_score_dataframe(score_list)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf0c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pretty table\n",
    "pretty_df = score_df.round(3)\n",
    "pretty_df = pretty_df[['category', 'prediction_method', 'precision', 'recall', 'f1']]\n",
    "pretty_df.rename(columns=PRETTY_NAMES, inplace=True)\n",
    "pretty_df.replace(PRETTY_NAMES, inplace=True)\n",
    "pretty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6edafbd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretty_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hn/_9bf2hvd5hvb_2g5tx8jhhvw0000gp/T/ipykernel_25797/4274615558.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretty_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_latex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pretty_df' is not defined"
     ]
    }
   ],
   "source": [
    "print(pretty_df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3985aad",
   "metadata": {},
   "source": [
    "## Scores per category (average: micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c6c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_method = 'micro'\n",
    "score_list = []\n",
    "for category in DOCUMENT_CATEGORIES:\n",
    "    for prediction_method in PREDICTION_METHODS:\n",
    "        score_list.append(create_score_record(predictions, prediction_method, average_method, category = category))\n",
    "create_score_dataframe(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee30de",
   "metadata": {},
   "source": [
    "## Scores per category (average: macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97682b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_method = 'macro'\n",
    "score_list = []\n",
    "for category in DOCUMENT_CATEGORIES:\n",
    "    for prediction_method in PREDICTION_METHODS:\n",
    "        score_list.append(create_score_record(predictions, prediction_method, average_method, category = category))\n",
    "create_score_dataframe(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3486a167",
   "metadata": {},
   "source": [
    "## Scores per category (average: weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_method = 'weighted'\n",
    "score_list = []\n",
    "for category in DOCUMENT_CATEGORIES:\n",
    "    for prediction_method in PREDICTION_METHODS:\n",
    "        score_list.append(create_score_record(predictions, prediction_method, average_method, category = category))\n",
    "create_score_dataframe(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dbcab6",
   "metadata": {},
   "source": [
    "## Scores for all categories combined (average: binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 Scores, drops rows containing NA for the only method that has NA's (robbert)\n",
    "score_list = []\n",
    "for prediction_method in PREDICTION_METHODS:\n",
    "    score_list.append(create_score_record(predictions, prediction_method, average_method='binary', category='all'))\n",
    "print(len(predictions))\n",
    "create_score_dataframe(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0200ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate F1 Scores, drops rows containing NA for all methods\n",
    "predictions_no_nan = predictions.copy()[['entity_id', 'label', 'bilstm_cv', 'rule_based', 'robbert_512']].dropna()\n",
    "score_list = []\n",
    "for prediction_method in PREDICTION_METHODS:\n",
    "    score_list.append(create_score_record(predictions_no_nan, prediction_method, average_method='binary', category = 'all'))\n",
    "print(len(predictions_no_nan))\n",
    "create_score_dataframe(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a126d32",
   "metadata": {},
   "source": [
    "## Investigate equal recall rule based and bilstm\n",
    "Equal recall in both methods could be an indictation of a processing error. Investigate this by looking at examples and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572ea3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "# predictions_no_nan.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e5efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_no_nan.loc[1486]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397344fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_no_nan.loc[1970]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c90b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(predictions_no_nan.label, predictions_no_nan.rule_based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8457c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(predictions_no_nan.label, predictions_no_nan.bilstm_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb9d7d",
   "metadata": {},
   "source": [
    "## Ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf09e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(row):\n",
    "    predicted_negated = 0\n",
    "    for method in PREDICTION_METHODS:\n",
    "        if row[method] == 'negated':\n",
    "            predicted_negated += 1\n",
    "    if predicted_negated > 1:\n",
    "        return 'negated'\n",
    "    else:\n",
    "        return 'not negated'\n",
    "\n",
    "predictions_no_nan['ensemble'] = predictions_no_nan.apply(majority_vote, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a3a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list.append(create_score_record(predictions_no_nan, 'ensemble', average_method='binary', category = 'all'))\n",
    "score_df = create_score_dataframe(score_list)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cffc469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pretty table\n",
    "pretty_df = score_df.round(3)\n",
    "pretty_df = pretty_df[['prediction_method', 'precision', 'recall', 'f1']]\n",
    "pretty_df.rename(columns=PRETTY_NAMES, inplace=True)\n",
    "pretty_df.replace(PRETTY_NAMES, inplace=True)\n",
    "print(pretty_df.to_latex(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
