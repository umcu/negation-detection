{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271b4fe3",
   "metadata": {},
   "source": [
    "# Evaluate biLSTM on complete DCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f3ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from medcat.meta_cat import MetaCAT\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data_dir = Path.cwd().parents[0] / 'data'\n",
    "annotation_file = data_dir / 'emc-dcc_ann.json'\n",
    "dcc_dir = data_dir / 'EMCDutchClinicalCorpus'\n",
    "model_dir = Path.cwd().parents[0] / 'models' / 'bilstm'\n",
    "result_dir = Path.cwd().parents[0] / 'results'\n",
    "bilstm_result_file = result_dir / 'bilstm_predictions.csv.gz'\n",
    "\n",
    "# Load annotated data\n",
    "with open(annotation_file) as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Load biLSTM\n",
    "meta_cat = MetaCAT.load(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8828bac",
   "metadata": {},
   "source": [
    "## Simple evaluation without retrieving the document IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5043e80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 **************************************************  Eval\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1760\n",
      "           1       1.00      1.00      1.00     10791\n",
      "\n",
      "    accuracy                           1.00     12551\n",
      "   macro avg       0.99      0.99      0.99     12551\n",
      "weighted avg       1.00      1.00      1.00     12551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = meta_cat.eval(json_path=annotation_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647d0f54",
   "metadata": {},
   "source": [
    "## Evaluation including document IDs\n",
    "Based on MedCATTrainer JSON format, create custom spaCy Doc and Span object with custom entities\n",
    "and use the biLSTM to predict negations for these entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a591fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom Span and Doc objects to provide data in the expected format for MetaCAT. This allows for using\n",
    "# the tokens as they are annotated in the labeled data. SpaCy's Span and Doc objects create entities from tokenized\n",
    "# text, which could be different from how human annotators tokenize text.\n",
    "# This code is based on MedCAT's json_to_fake_spacy(), see:\n",
    "# https://github.com/CogStack/MedCAT/blob/bbb2dc8aa452d0561709993078ce4f0297a63ff6/medcat/utils/meta_cat/data_utils.py#L133\n",
    "class Empty(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class CustomSpan(object):\n",
    "    def __init__(self, start_char, end_char, id):\n",
    "        self._ = Empty()\n",
    "        self.start_char = start_char\n",
    "        self.end_char = end_char\n",
    "        self._.id = id\n",
    "        self._.meta_anns = None\n",
    "\n",
    "class CustomDoc(object):\n",
    "    def __init__(self, text, id):\n",
    "        self._ = Empty()\n",
    "        self._.share_tokens = None\n",
    "        self.ents = []\n",
    "        self._ents = self.ents\n",
    "        self.text = text\n",
    "        self.id = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26451ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store predictions for each entity\n",
    "result = []\n",
    "\n",
    "# Loop over every document\n",
    "for document in annotations['projects'][0]['documents']:\n",
    "    document_name = document['name']\n",
    "    text = document['text']\n",
    "    doc = CustomDoc(text=text, id=document_name)\n",
    "\n",
    "    # Loop over every annotated entity\n",
    "    for annotation in document['annotations']:\n",
    "\n",
    "        # Extract data\n",
    "        start_char = annotation['start']\n",
    "        end_char = annotation['end']\n",
    "\n",
    "        # Create custom ID\n",
    "        entity_id = f'{document_name}_{start_char}_{end_char}'\n",
    "\n",
    "        # Add entity as custom Span to custom Doc object\n",
    "        doc.ents.append(CustomSpan(start_char, end_char, entity_id))\n",
    "\n",
    "    doc = meta_cat(doc)\n",
    "\n",
    "    # Retrieve predictions\n",
    "    for ent in doc.ents:\n",
    "        entity_id = ent._.id\n",
    "        annotation = ent._.meta_anns['Negation']['value']\n",
    "        result.append([entity_id, annotation])\n",
    "\n",
    "bilstm_predictions = pd.DataFrame(result, columns=['entity_id', 'bilstm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db801879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "bilstm_predictions.to_csv(bilstm_result_file, index=False, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
