{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation MetaCAT - BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import os\n",
    "\n",
    "from medcat.cat import CAT\n",
    "from medcat.vocab import Vocab\n",
    "from medcat.cdb import CDB\n",
    "from medcat.config import Config\n",
    "from medcat.meta_cat import MetaCAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "data_dir = os.path.join('..', 'data')\n",
    "cdb_file = os.path.join(data_dir, 'cdb.dat')\n",
    "vocab_file = os.path.join(data_dir, 'vocab.dat')\n",
    "\n",
    "# Output\n",
    "output_dir = 'output'\n",
    "\n",
    "# Name should contain 'bbpe' for ByteLevelBPETokenizer or 'bert' for BertTokenizerFast\n",
    "tokenizer_name = 'bbpe_dutch-wikipedia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, train and save the tokenizer\n",
    "mc_negation = MetaCAT()\n",
    "mc_negation = mc_negation.load(save_dir=output_dir, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cdb and vocab \n",
    "config = Config()\n",
    "\n",
    "vocab = Vocab.load(vocab_file)\n",
    "cdb = CDB.load(cdb_file)\n",
    "\n",
    "# Create MedCAT pipeline\n",
    "cat = CAT(cdb=cdb, vocab=vocab, config=config, meta_cats=[mc_negation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: heup\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.9724474, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: heupdysplasie\n",
      "Meta Annotations: {'Negation': {'value': 'negated', 'confidence': 0.9999876, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on DL1114 from DCC with negation\n",
    "text = 'Echo- en rontgenonderzoek van de heup toont geen evidente heupdysplasie.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: heup\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.9985293, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: heupdysplasie\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.99687064, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on DL1114 from DCC without negation\n",
    "text = 'Echo- en rontgenonderzoek van de heup toont evidente heupdysplasie.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on DL1112 from DCC\n",
    "text = 'Patient kan zich geen trauma herinneren.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Trauma is not identified as medical concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: operatie\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.99955237, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: buikpijn\n",
      "Meta Annotations: {'Negation': {'value': 'negated', 'confidence': 0.72627723, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on NTvG article\n",
    "# https://www.ntvg.nl/artikelen/acute-buik-op-basis-van-een-wandelende-milt\n",
    "text = '1 maand na de operatie had patiënte geen buikpijn meer en was zij goed hersteld.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# The negation was correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: darmobstructie\n",
      "Meta Annotations: {'Negation': {'value': 'negated', 'confidence': 0.98679256, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: zien\n",
      "Meta Annotations: {'Negation': {'value': 'negated', 'confidence': 0.9872715, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on NTvG article\n",
    "# https://www.ntvg.nl/artikelen/een-bezoar-bij-een-vrouw-met-clomipramine-intoxicatie\n",
    "text = 'Er waren geen tekenen van darmobstructie te zien.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Correct identification of negation, but incorrect linking of zien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: patiënten\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.9977733, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: controlegroep\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.99956954, 'name': 'Negation'}}\n",
      "\n",
      "\n",
      "Entity: SARS-CoV-2-infectie\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.845927, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on NTvG article\n",
    "# https://www.ntvg.nl/artikelen/nieuws/vaker-ziek-na-acute-fase-covid-19\n",
    "text = 'Alle patiënten werden gematcht met een controlegroep bij wie geen SARS-CoV-2-infectie was geregistreerd.'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# Correct identification of negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: scan\n",
      "Meta Annotations: {'Negation': {'value': 'negated', 'confidence': 0.8478781, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'Er zijn geen bijwerkingen gemeld van de scan'\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")\n",
    "# Negation was incorrectly identified\n",
    "# Entity linking of bijwerkingen was missed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check add_prefix_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Zwelling\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.967743, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \" Zwelling treedt niet op.\"\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Negation was not identified. But difficult one, see next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Zwelling\n",
      "Meta Annotations: {'Negation': {'value': 'not negated', 'confidence': 0.967743, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"Zwelling treedt niet op.\"\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Negation was not identified. But difficult one, see next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: zwelling\n",
      "Meta Annotations: {'Negation': {'value': 'negated', 'confidence': 0.99528426, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"Geen zwelling treedt op.\"\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Negation was identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: zwelling\n",
      "Meta Annotations: {'Negation': {'value': 'negated', 'confidence': 0.9988198, 'name': 'Negation'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"Geen zwelling treedt niet op.\"\n",
    "doc = cat(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: \" + ent.text)\n",
    "    print(\"Meta Annotations: \" + str(ent._.meta_anns))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Negation was identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate MetaCat on subsets of the data\n",
    "The ContextD paper calculates precision, recall and F1-score on subsets of the data. In this section we calculate the same scores with the just created model. Note that this results in a calculation on a set of data that was included during the training phase. For proper score calculations, we will do cross validation at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_DL = os.path.join(data_dir, 'emc-dcc_ann_DL.json')\n",
    "json_file_GP = os.path.join(data_dir, 'emc-dcc_ann_GP.json')\n",
    "json_file_RD = os.path.join(data_dir, 'emc-dcc_ann_RD.json')\n",
    "json_file_SP = os.path.join(data_dir, 'emc-dcc_ann_SP.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93       595\n",
      "           1       0.98      0.99      0.99      3088\n",
      "\n",
      "    accuracy                           0.98      3683\n",
      "   macro avg       0.97      0.95      0.96      3683\n",
      "weighted avg       0.98      0.98      0.98      3683\n",
      "\n",
      "Test Loss:  0.0826881697678284\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9605734767025089,\n",
       "  'recall': 0.9008403361344538,\n",
       "  'f1-score': 0.9297484822202948,\n",
       "  'support': 595},\n",
       " '1': {'precision': 0.98112,\n",
       "  'recall': 0.9928756476683938,\n",
       "  'f1-score': 0.9869628198937711,\n",
       "  'support': 3088},\n",
       " 'accuracy': 0.9780070594623947,\n",
       " 'macro avg': {'precision': 0.9708467383512545,\n",
       "  'recall': 0.9468579919014237,\n",
       "  'f1-score': 0.958355651057033,\n",
       "  'support': 3683},\n",
       " 'weighted avg': {'precision': 0.9778006458425177,\n",
       "  'recall': 0.9780070594623947,\n",
       "  'f1-score': 0.9777196673236602,\n",
       "  'support': 3683}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_negation.eval(json_file_RD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.74      0.83       416\n",
      "           1       0.95      0.99      0.97      2309\n",
      "\n",
      "    accuracy                           0.95      2725\n",
      "   macro avg       0.95      0.86      0.90      2725\n",
      "weighted avg       0.95      0.95      0.95      2725\n",
      "\n",
      "Test Loss:  0.15795741708383762\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9503105590062112,\n",
       "  'recall': 0.7355769230769231,\n",
       "  'f1-score': 0.8292682926829268,\n",
       "  'support': 416},\n",
       " '1': {'precision': 0.9542238868081565,\n",
       "  'recall': 0.9930705933304461,\n",
       "  'f1-score': 0.9732597623089984,\n",
       "  'support': 2309},\n",
       " 'accuracy': 0.9537614678899082,\n",
       " 'macro avg': {'precision': 0.9522672229071838,\n",
       "  'recall': 0.8643237582036847,\n",
       "  'f1-score': 0.9012640274959626,\n",
       "  'support': 2725},\n",
       " 'weighted avg': {'precision': 0.9536264760317861,\n",
       "  'recall': 0.9537614678899082,\n",
       "  'f1-score': 0.9512779452945229,\n",
       "  'support': 2725}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_negation.eval(json_file_SP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94       379\n",
      "           1       0.98      1.00      0.99      2417\n",
      "\n",
      "    accuracy                           0.98      2796\n",
      "   macro avg       0.98      0.95      0.96      2796\n",
      "weighted avg       0.98      0.98      0.98      2796\n",
      "\n",
      "Test Loss:  0.059427651310605664\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9798850574712644,\n",
       "  'recall': 0.899736147757256,\n",
       "  'f1-score': 0.938101788170564,\n",
       "  'support': 379},\n",
       " '1': {'precision': 0.9844771241830066,\n",
       "  'recall': 0.9971038477451386,\n",
       "  'f1-score': 0.9907502569373073,\n",
       "  'support': 2417},\n",
       " 'accuracy': 0.9839055793991416,\n",
       " 'macro avg': {'precision': 0.9821810908271356,\n",
       "  'recall': 0.9484199977511973,\n",
       "  'f1-score': 0.9644260225539356,\n",
       "  'support': 2796},\n",
       " 'weighted avg': {'precision': 0.9838546659270158,\n",
       "  'recall': 0.9839055793991416,\n",
       "  'f1-score': 0.9836137155701415,\n",
       "  'support': 2796}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_negation.eval(json_file_DL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.71      0.81       383\n",
      "           1       0.96      1.00      0.98      3024\n",
      "\n",
      "    accuracy                           0.96      3407\n",
      "   macro avg       0.96      0.85      0.90      3407\n",
      "weighted avg       0.96      0.96      0.96      3407\n",
      "\n",
      "Test Loss:  0.1171220576257578\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.951048951048951,\n",
       "  'recall': 0.7101827676240209,\n",
       "  'f1-score': 0.8131539611360239,\n",
       "  'support': 383},\n",
       " '1': {'precision': 0.9644344761294457,\n",
       "  'recall': 0.9953703703703703,\n",
       "  'f1-score': 0.9796582587469487,\n",
       "  'support': 3024},\n",
       " 'accuracy': 0.9633108306427942,\n",
       " 'macro avg': {'precision': 0.9577417135891984,\n",
       "  'recall': 0.8527765689971956,\n",
       "  'f1-score': 0.8964061099414863,\n",
       "  'support': 3407},\n",
       " 'weighted avg': {'precision': 0.9629297340966222,\n",
       "  'recall': 0.9633108306427942,\n",
       "  'f1-score': 0.9609405757457793,\n",
       "  'support': 3407}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_negation.eval(json_file_GP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}