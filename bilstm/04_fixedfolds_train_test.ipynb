{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetaCAT - Training biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from medcat.tokenizers.meta_cat_tokenizers import TokenizerWrapperBPE\n",
    "from pathlib import Path\n",
    "from medcat.meta_cat import MetaCAT\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "data_dir = Path.cwd().parents[0] / 'data'\n",
    "annotation_file = data_dir / 'emc-dcc_ann.json'\n",
    "split_list_file = data_dir / 'split_list.json'\n",
    "model_dir = Path.cwd().parents[0] / 'models' / 'bilstm'\n",
    "embeddings_file = model_dir / 'embeddings.npy'\n",
    "split_model_dir = model_dir / 'splits'\n",
    "\n",
    "# Output\n",
    "annotations_split_dir = data_dir / 'annotations_split'\n",
    "\n",
    "# Name should contain 'bbpe' for ByteLevelBPETokenizer or 'bert' for BertTokenizerFast\n",
    "# This name is saved in the model_config dict and subssequently in vars.dat on disk.\n",
    "#tokenizer_name = 'bbpe_dutch-wikipedia'\n",
    "\n",
    "# Create output dir\n",
    "annotations_split_dir.mkdir(exist_ok=True)\n",
    "\n",
    "split_model_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer and embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TokenizerWrapperBPE.load(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load(embeddings_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load annotated data\n",
    "with open(annotation_file) as f:\n",
    "    annotations = json.load(f)\n",
    "    \n",
    "# Load split lists\n",
    "with open(split_list_file) as f:\n",
    "    split_lists = json.load(f)\n",
    "\n",
    "split_list = split_lists[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations = []\n",
    "test_annotations = []\n",
    "                         \n",
    "for document in annotations['projects'][0]['documents']:\n",
    "    if document['name'] in split_list['train']:\n",
    "        train_annotations.append(document)\n",
    "    elif document['name'] in split_list['test']:\n",
    "        test_annotations.append(document)\n",
    "#     else:\n",
    "#         print(f'{document[\"name\"]} not found in either train or test')\n",
    "\n",
    "# Create an annotation file for the split following MetaCAT's annotation format\n",
    "project_train_annotations = {'projects': [{'documents': train_annotations}]}\n",
    "project_test_annotations = {'projects': [{'documents': test_annotations}]}\n",
    "\n",
    "# Write output files\n",
    "train_output_file = annotations_split_dir / f'train_annotations_{split_list[\"split_id\"]}.json'\n",
    "with open(train_output_file, \"w\") as fp:\n",
    "    json.dump(project_train_annotations, fp)\n",
    "    \n",
    "test_output_file = annotations_split_dir / f'test_annotations_{split_list[\"split_id\"]}.json'\n",
    "with open(test_output_file, \"w\") as fp:\n",
    "    json.dump(project_test_annotations, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train biLSTM from training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medcat.config_meta_cat import ConfigMetaCAT\n",
    "config_metacat = ConfigMetaCAT()\n",
    "config_metacat.general['category_name'] = 'Negation'\n",
    "config_metacat.train['nepochs'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Repositories\\negation-detection\\data\\annotations_split\\train_annotations_0.json\n"
     ]
    }
   ],
   "source": [
    "for train_file in annotations_split_dir.rglob(\"train_annotations_*.json\"):\n",
    "    print(train_file)\n",
    "    split_id = train_file.stem.split('_')[2]\n",
    "    split_dir = split_model_dir / split_id\n",
    "    split_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Initiate MetaCAT\n",
    "    mc_negation = MetaCAT(tokenizer=tokenizer, config=config_metacat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 **************************************************  Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97      8729\n",
      "           1       0.88      0.71      0.79      1411\n",
      "\n",
      "    accuracy                           0.95     10140\n",
      "   macro avg       0.92      0.85      0.88     10140\n",
      "weighted avg       0.94      0.95      0.94     10140\n",
      "\n",
      "Epoch: 0 **************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       965\n",
      "           1       0.95      0.83      0.89       161\n",
      "\n",
      "    accuracy                           0.97      1126\n",
      "   macro avg       0.96      0.91      0.93      1126\n",
      "weighted avg       0.97      0.97      0.97      1126\n",
      "\n",
      "\n",
      "##### Model saved to D:\\Repositories\\negation-detection\\models\\bilstm\\splits\\0\\model.dat at epoch: 0 and f1: 0.9689596191472376 #####\n",
      "\n",
      "Epoch: 1 **************************************************  Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      8729\n",
      "           1       0.95      0.91      0.93      1411\n",
      "\n",
      "    accuracy                           0.98     10140\n",
      "   macro avg       0.97      0.95      0.96     10140\n",
      "weighted avg       0.98      0.98      0.98     10140\n",
      "\n",
      "Epoch: 1 **************************************************  Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       965\n",
      "           1       0.92      0.91      0.92       161\n",
      "\n",
      "    accuracy                           0.98      1126\n",
      "   macro avg       0.95      0.95      0.95      1126\n",
      "weighted avg       0.98      0.98      0.98      1126\n",
      "\n",
      "\n",
      "##### Model saved to D:\\Repositories\\negation-detection\\models\\bilstm\\splits\\0\\model.dat at epoch: 1 and f1: 0.9759901732873049 #####\n",
      "\n"
     ]
    }
   ],
   "source": [
    "            # Train model\n",
    "results = mc_negation.train(json_path=train_file, save_dir_path=split_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Repositories\\negation-detection\\data\\annotations_split\\test_annotations_0.json\n"
     ]
    }
   ],
   "source": [
    "test_file = Path('D:/Repositories/negation-detection/data/annotations_split/test_annotations_0.json')\n",
    "print(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 **************************************************  Eval\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1097\n",
      "           1       0.88      0.90      0.89       188\n",
      "\n",
      "    accuracy                           0.97      1285\n",
      "   macro avg       0.93      0.94      0.94      1285\n",
      "weighted avg       0.97      0.97      0.97      1285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = mc_negation.eval(json_path=test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['examples']['FP']['negated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
