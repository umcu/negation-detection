{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing using cross-validation\n",
    "This notebook uses predefined subsets of examples to train and test models.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import evaluate_per_example\n",
    "from medcat.tokenizers.meta_cat_tokenizers import TokenizerWrapperBPE\n",
    "from medcat.config_meta_cat import ConfigMetaCAT\n",
    "from medcat.meta_cat import MetaCAT\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MetaCAT\n",
    "CLASS = 'Temporality'\n",
    "AVG = 'micro' # binary or micro\n",
    "config_metacat = ConfigMetaCAT()\n",
    "config_metacat.general['category_name'] = CLASS\n",
    "config_metacat.train['nepochs'] = 10\n",
    "config_metacat.train['score_average'] = AVG\n",
    "config_metacat.model['n_classes'] = 3\n",
    "\n",
    "# Input\n",
    "data_dir = Path.cwd().parents[0] / 'data'\n",
    "annotation_file = data_dir / 'emc-dcc_ann_Augmented.json'\n",
    "model_dir = Path.cwd().parents[0] / 'models' / 'bilstm'\n",
    "embeddings_file = model_dir / 'embeddings.npy'\n",
    "\n",
    "# Output\n",
    "annotations_split_dir = data_dir / 'annotations_split'\n",
    "models_split_dir = model_dir / 'model_splits' / CLASS\n",
    "result_dir = Path.cwd().parents[0] / 'results'\n",
    "score_result_file = result_dir / 'bilstm_scores_cv_augmented.csv.gz'\n",
    "predictions_result_file = result_dir / 'bilstm_predictions_cv_augmented.csv.gz'\n",
    "\n",
    "# Create output dirs\n",
    "annotations_split_dir.mkdir(exist_ok=True)\n",
    "models_split_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# num folds\n",
    "n_folds = 10\n",
    "group = 'name'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer and embeddings matrix\n",
    "Load a project-wide tokenizer and embeddings matrix which are created in `01_tokenizer_embeddings.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TokenizerWrapperBPE.load(model_dir)\n",
    "embeddings = np.load(embeddings_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = json.load(open(annotation_file, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [d['name'] for d in annotations['projects'][0]['documents']]\n",
    "groups = [n.split(\"|\")[0] for n in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Splitter = GroupKFold(n_splits=n_folds)\n",
    "Texts = [d['text'] for d in annotations['projects'][0]['documents']]\n",
    "Splitter.get_n_splits(Texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_train = {'projects': [{'name': 'emc-dcc-synthAug', 'documents': []}]}\n",
    "annotations_test = {'projects': [{'name': 'emc-dcc-synthAug', 'documents': []}]}\n",
    "for i, (train_index, test_index) in enumerate(Splitter.split(Texts, groups=groups)):\n",
    "    # collect train and test data\n",
    "    \n",
    "    annotations_train['projects'][0]['documents'] = [annotations['projects'][0]['documents'][i] for i in train_index]\n",
    "    annotations_test['projects'][0]['documents'] = [annotations['projects'][0]['documents'][i] for i in test_index]\n",
    "        \n",
    "    # write train and test data to file\n",
    "    fname_train = annotations_split_dir / f'train_annotations_{i}.json'\n",
    "    fname_test = annotations_split_dir / f'test_annotations_{i}.json'\n",
    "    json.dump(annotations_train, open(fname_train, 'w'))\n",
    "    json.dump(annotations_test, open(fname_test, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test on folds\n",
    "Per fold, a MetaCAT model is trained and tested. Testing is done using MetaCAT's eval() function, which contains functionality to evaluate the model on a testset and returns a dictionary with scores and examples, but does not include the example ID, which we use to compare examples between different methods. Therefor we use a different evaluation function later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results['examples']['FN']['hypothetical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store results of individual folds\n",
    "score_result_list = []\n",
    "\n",
    "for train_file in annotations_split_dir.rglob(\"train_annotations_*.json\"):\n",
    "    print(train_file)\n",
    "    split_id = train_file.stem.split('_')[2]\n",
    "    split_id_dir = models_split_dir / split_id\n",
    "    split_id_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Initiate MetaCAT\n",
    "    meta_cat = MetaCAT(tokenizer=tokenizer, embeddings=embeddings, config=config_metacat)\n",
    "    \n",
    "    # Train model\n",
    "    train_results = meta_cat.train_from_json(json_path=str(train_file), save_dir_path=str(split_id_dir))\n",
    "    \n",
    "    # Evaluate using MetaCAT's eval function\n",
    "    test_file = train_file.parent / train_file.name.replace('train_annotations_', 'test_annotations_')\n",
    "    test_results = meta_cat.eval(json_path=test_file)\n",
    "    \n",
    "    # Count positive and negatives\n",
    "    tp = 0\n",
    "    if 'negated' in test_results['examples']['TP']:\n",
    "        tp = len(test_results['examples']['TP']['negated'])\n",
    "    \n",
    "    fp = 0\n",
    "    if 'negated' in test_results['examples']['FP']:\n",
    "        fp = len(test_results['examples']['FP']['negated'])\n",
    "        \n",
    "    fn = 0\n",
    "    if 'negated' in test_results['examples']['FN']:\n",
    "        fn = len(test_results['examples']['FN']['negated'])\n",
    "    # Save test results\n",
    "    score_result_list.append([split_id,\n",
    "                              round(test_results['f1'], 2),\n",
    "                              round(test_results['precision'], 2),\n",
    "                              round(test_results['recall'], 2),\n",
    "                              tp,\n",
    "                              fp,\n",
    "                              fn]\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on folds\n",
    "Use this cell to test on the test data if the models are already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store results of individual folds\n",
    "score_result_list = []\n",
    "\n",
    "for test_file in annotations_split_dir.rglob(\"test_annotations_*.json\"):\n",
    "    split_id = test_file.stem.split('_')[2]\n",
    "    split_id_dir = models_split_dir / split_id\n",
    "    \n",
    "    # Load biLSTM\n",
    "    meta_cat = MetaCAT.load(str(split_id_dir))\n",
    "    \n",
    "    # Evaluate using MetaCAT's eval function\n",
    "    test_results = meta_cat.eval(json_path=test_file)\n",
    "    \n",
    "    # Save test results\n",
    "    score_result_list.append([split_id,\n",
    "                              round(test_results['f1'], 2),\n",
    "                              round(test_results['precision'], 2),\n",
    "                              round(test_results['recall'], 2),\n",
    "                              len(test_results['examples']['TP']['hypothetical']),\n",
    "                              len(test_results['examples']['FP']['hypothetical']),\n",
    "                              len(test_results['examples']['FN']['hypothetical'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather scores from folds\n",
    "In this section, results are gathered from the folds and saved in a single CSV.\n",
    "\n",
    "Currently, recall and precision are not returned by MetaCAT's eval() function. A future release will add this functionality (https://github.com/CogStack/MedCAT/pull/172)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(row):\n",
    "    tp = row.tp\n",
    "    fp = row.fp\n",
    "    fn = row.fn\n",
    "    recall = round(tp / (tp + fn), 2)\n",
    "    return recall\n",
    "\n",
    "def calculate_precision(row):\n",
    "    tp = row.tp\n",
    "    fp = row.fp\n",
    "    fn = row.fn\n",
    "    precision = round(tp / (tp + fp), 2)\n",
    "    return precision\n",
    "\n",
    "def calculate_f1(row):\n",
    "    tp = row.tp\n",
    "    fp = row.fp\n",
    "    fn = row.fn\n",
    "    f1 = round((2*tp) / ((2*tp) + fp + fn), 2)\n",
    "    return f1\n",
    "\n",
    "score_results = pd.DataFrame(score_result_list, columns=['split_id', 'weighted_f1', 'weighted_precision', 'weighted_recall', 'tp', 'fp', 'fn'])\n",
    "score_results['manual_recall'] = score_results.apply(calculate_recall, axis=1)\n",
    "score_results['manual_precision'] = score_results.apply(calculate_precision, axis=1)\n",
    "score_results['manual_f1'] = score_results.apply(calculate_f1, axis=1)\n",
    "score_results.to_csv(score_result_file, index=False, compression='gzip')\n",
    "score_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom evaluation per example per fold\n",
    "In this project we are interested per example whether a negation has been correctly predicted or not. MetaCAT does not have such functionality; it only returns scores, predictions and examples.\n",
    "\n",
    "In this section we iterate through all annotations from an annotation file (MedCAT Trainer format), create an ID for every example (`exampleID = documentID_start_end`), collect the prediction per example and save all predictions in a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models on their respective test sets\n",
    "predictions_on_test_list = []\n",
    "for annotation_filename in annotations_split_dir.rglob(\"test_annotations_*.json\"):\n",
    "    \n",
    "    # Extract split ID\n",
    "    split_id = annotation_filename.stem.split('_')[2]\n",
    "    split_id_dir = models_split_dir / split_id\n",
    "    print(f'Evaluating test set {split_id}')\n",
    "    \n",
    "    # Load MetaCAT model\n",
    "    meta_cat = MetaCAT.load(split_id_dir)\n",
    "    \n",
    "    # Gather the predictions on every example in the provided annotation file.\n",
    "    predictions_on_test_list.append(evaluate_per_example(annotation_filename, meta_cat, f'bilstm_cv'))\n",
    "    \n",
    "# Save al predictions in a single dataframe\n",
    "predictions_on_test_df = pd.DataFrame(columns=['entity_id', 'bilstm_cv'])\n",
    "for i in predictions_on_test_list:\n",
    "    predictions_on_test_df = predictions_on_test_df.append(i)\n",
    "\n",
    "# Save predictions in a csv\n",
    "predictions_on_test_df.reset_index(drop=True, inplace=True)\n",
    "predictions_on_test_df.to_csv(predictions_result_file, index=False, compression='gzip', line_terminator='\\n')\n",
    "predictions_on_test_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
