{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5fa31f8",
   "metadata": {},
   "source": [
    "Starting from a \n",
    "* pre-trained model with a \n",
    "* pre-trained tokenizer\n",
    "\n",
    "we perform finetuning on a negation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02705f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import device, cuda, version\n",
    "\n",
    "import apex\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple\n",
    "from transformers import AutoTokenizer, RobertaTokenizer, AutoModelForTokenClassification\n",
    "from transformers import Auto\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from utils import dcc_splitter as splitter\n",
    "from utils import ner_training as trainer\n",
    "#import dcc_splitter as splitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f132a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2af5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = device(\"cuda:0\") if cuda.is_available() else device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e2d04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fb57b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcc_dir = None\n",
    "output_dir = None\n",
    "skip_file = None\n",
    "n_splits = 10\n",
    "random_state = None\n",
    "base_folder = \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels\"\n",
    "output_folder = \"fine_tuned_token_classification\"\n",
    "mod_name = \"MedRoBERTa\" # \"robbert-v2-dutch-base\" # belabBERT_115k # bert-base-dutch, #MedRoBERTa\n",
    " \n",
    "\n",
    "args = namedtuple\n",
    "args.task = \"negation\" # experiencer, temporality\n",
    "args.model_path = os.path.join(base_folder, mod_name)\n",
    "args.model_type = \"roberta\" # bertje \n",
    "args.output_dir = os.path.join(base_folder, output_folder)\n",
    "args.num_epochs = 1\n",
    "args.eval_steps = 10 \n",
    "args.lr = 5e-5\n",
    "args.batch_size= 32\n",
    "args.gradient_accumulation_steps=1\n",
    "args.block_size = 32 # block size determines inclusion \n",
    "args.save_model=False\n",
    "args.bio=True\n",
    "args.do_eval=False\n",
    "args.do_write=False\n",
    "args.bootstrap=False\n",
    "args.do_print_class_report=False\n",
    "\n",
    "random.seed(77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb1c0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.block_size determines how many text snippets are  used for training, see ner_training.py lines 118--141\n",
    "# obviously this is a code-design flaw that should be mended.\n",
    "# the dataset loader should  include the id_begin_end in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bafdb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcc-splitter for folds\n",
    "dcc_splitter = splitter.DCCSplitter(dcc_dir, output_dir, skip_file, n_splits, random_state, write_to_file=False)\n",
    "splits = dcc_splitter.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bcb19ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load NER DCC set\n",
    "dcc = pd.read_csv(\"../data/RobBERT/DCC_df.csv\", \n",
    "                  sep=\"\\t\", \n",
    "                  skip_blank_lines=True, \n",
    "                  engine=\"python\", \n",
    "                  encoding=\"latin-1\",\n",
    "                  on_bad_lines=\"warn\", \n",
    "                  keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccc8b55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O             146395\n",
       "NotNegated     15713\n",
       "Negated         3017\n",
       "Name: Negation, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcc.Negation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d71ec0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11882"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcc.loc[dcc.BIO!='O'][['Id', 'Begin', 'End']].apply(lambda x: \"_\".join(x), axis=1).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3234908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Texts = dcc.groupby('Id').Word.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52a64c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_ids = {'negation':{'B-Negated':0,'B-NotNegated':1,'I-Negated':2,'I-NotNegated':3},\n",
    "          'temporality':{'B-Recent':0,'B-Historical':1,'B-Hypothetical':2,'I-Recent':3,\n",
    "                         'I-Historical':4,'I-Hypothetical':5},\n",
    "          'experiencer':{'B-Patient':0,'B-Other':1,'I-Patient':2,'I-Other':3}}\n",
    "\n",
    "tag2id = tag_ids[args.task]\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017aa7e5",
   "metadata": {},
   "source": [
    "## Over all document sources\n",
    "\n",
    "improvement: only output best model based on validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2865323a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████| 151/151 [00:23<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [01:12, 72.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.885 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.86      0.82      0.84       125\n",
      "  NotNegated       0.87      0.92      0.89       663\n",
      "\n",
      "   micro avg       0.87      0.90      0.88       788\n",
      "   macro avg       0.86      0.87      0.86       788\n",
      "weighted avg       0.87      0.90      0.88       788\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████| 151/151 [00:23<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [02:22, 71.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.855 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.82      0.86      0.84       102\n",
      "  NotNegated       0.84      0.88      0.86       646\n",
      "\n",
      "   micro avg       0.84      0.87      0.85       748\n",
      "   macro avg       0.83      0.87      0.85       748\n",
      "weighted avg       0.84      0.87      0.85       748\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████| 151/151 [00:23<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [03:34, 71.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.885 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.91      0.91      0.91       147\n",
      "  NotNegated       0.86      0.90      0.88       663\n",
      "\n",
      "   micro avg       0.87      0.90      0.88       810\n",
      "   macro avg       0.88      0.91      0.89       810\n",
      "weighted avg       0.87      0.90      0.88       810\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████| 151/151 [00:23<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [04:45, 71.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.833 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.78      0.78      0.78       103\n",
      "  NotNegated       0.80      0.88      0.84       649\n",
      "\n",
      "   micro avg       0.80      0.87      0.83       752\n",
      "   macro avg       0.79      0.83      0.81       752\n",
      "weighted avg       0.80      0.87      0.83       752\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████| 151/151 [00:23<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [05:55, 70.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.881 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.96      0.91      0.93       106\n",
      "  NotNegated       0.85      0.89      0.87       660\n",
      "\n",
      "   micro avg       0.87      0.89      0.88       766\n",
      "   macro avg       0.91      0.90      0.90       766\n",
      "weighted avg       0.87      0.89      0.88       766\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████| 151/151 [00:24<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [07:06, 70.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.895 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.86      0.86      0.86       109\n",
      "  NotNegated       0.88      0.93      0.90       664\n",
      "\n",
      "   micro avg       0.87      0.92      0.90       773\n",
      "   macro avg       0.87      0.90      0.88       773\n",
      "weighted avg       0.87      0.92      0.90       773\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████| 151/151 [00:23<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [08:16, 70.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.875 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.92      0.93      0.92       118\n",
      "  NotNegated       0.86      0.87      0.87       657\n",
      "\n",
      "   micro avg       0.87      0.88      0.87       775\n",
      "   macro avg       0.89      0.90      0.89       775\n",
      "weighted avg       0.87      0.88      0.87       775\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████| 151/151 [00:23<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [09:27, 70.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.886 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.87      0.85      0.86       130\n",
      "  NotNegated       0.87      0.92      0.89       648\n",
      "\n",
      "   micro avg       0.87      0.90      0.89       778\n",
      "   macro avg       0.87      0.88      0.88       778\n",
      "weighted avg       0.87      0.90      0.89       778\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████| 151/151 [00:23<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [10:38, 70.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.885 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.80      0.80      0.80       110\n",
      "  NotNegated       0.87      0.92      0.90       687\n",
      "\n",
      "   micro avg       0.86      0.91      0.88       797\n",
      "   macro avg       0.84      0.86      0.85       797\n",
      "weighted avg       0.86      0.91      0.88       797\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████| 151/151 [00:24<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [11:50, 71.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.858 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.86      0.89      0.87       116\n",
      "  NotNegated       0.83      0.88      0.86       628\n",
      "\n",
      "   micro avg       0.83      0.88      0.86       744\n",
      "   macro avg       0.84      0.89      0.86       744\n",
      "weighted avg       0.83      0.88      0.86       744\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# cycle through folds\n",
    "scores = []\n",
    "predlist = []\n",
    "test_lists = []\n",
    "loss_history = {}\n",
    "for idx, fold in tqdm(enumerate(splits)):\n",
    "    # re-init model for each fold, otherwise it keeps on training the same throughout all folds..\n",
    "    token_model = AutoModelForTokenClassification.from_pretrained(args.model_path, num_labels = len(tag2id))\n",
    "    \n",
    "    train_list, test_list = fold['train'], fold['test']\n",
    "    \n",
    "    ## eval is optional (to gauge the best number of steps/epochs)\n",
    "    eval_list = random.choices(train_list,k=int(len(train_list)/10)) if args.do_eval else []\n",
    "    eval_dcc = dcc.loc[dcc.Id.isin(eval_list)]\n",
    "    test_dcc = dcc.loc[dcc.Id.isin(test_list)]\n",
    "    train_dcc = dcc.loc[(dcc.Id.isin(train_list)) & (~dcc.Id.isin(eval_list))]\n",
    "    \n",
    "    test_list = test_dcc.Id.tolist()\n",
    "    eval_list = eval_dcc.Id.tolist()\n",
    "\n",
    "    ###\n",
    "    train_dataset = trainer.TextDatasetFromDataFrame(train_dcc, tokenizer, args) \n",
    "    test_dataset = trainer.TextDatasetFromDataFrame(test_dcc, tokenizer, args)\n",
    "    eval_dataset = trainer.TextDatasetFromDataFrame(eval_dcc, tokenizer, args)\n",
    "    \n",
    "    args.do_print_class_report=False\n",
    "    # Train on all document sources\n",
    "    trained_model, eval_loss_history = trainer.train_model(model=token_model.to(device), \n",
    "                                                            tokenizer=tokenizer, \n",
    "                                                            train_dataset=train_dataset, \n",
    "                                                            eval_dataset=eval_dataset, \n",
    "                                                            tag2id=tag2id,\n",
    "                                                            device=device, \n",
    "                                                            args=args,\n",
    "                                                            max_grad_norm=1.0,\n",
    "                                                            amp=False)\n",
    "    args.do_print_class_report=True\n",
    "    # Evaluate on all document sources\n",
    "    f1, prec, rec, preds, truth, test_ids = trainer.eval_model(model=trained_model, \n",
    "                                       tokenizer=tokenizer, \n",
    "                                       eval_dataset=test_dataset, \n",
    "                                       tag2id=tag2id, \n",
    "                                       device=device, \n",
    "                                       args=args, \n",
    "                                       return_pred=True)\n",
    "    \n",
    "    loss_history[idx]=eval_loss_history\n",
    "    \n",
    "    #test_ids = [\"_\".join(t) for t in zip(test_dcc.Id, test_dcc.Begin, test_dcc.End)]\n",
    "    scores.append({'fold': idx, 'f1': f1, 'precision': prec, 'recall': rec})\n",
    "    predlist.append({'fold': idx, 'prediction': preds, 'truth': truth, 'ids': test_ids})\n",
    "    test_lists.append(test_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "466041d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predlist_prep = []\n",
    "robbert_col_name = 'robbert_'+str(args.block_size)+'_'+str(args.num_epochs)\n",
    "\n",
    "for foldnum, foldres in enumerate(predlist):\n",
    "    ids = foldres['ids']\n",
    "    for prs, trs, ids in zip(foldres['prediction'], foldres['truth'], foldres['ids']):\n",
    "        for pr, tr, _id in zip(prs, trs, ids):\n",
    "            tmp_dict={}\n",
    "            if len(pr)==len(tr)==0:\n",
    "                tmp_dict['fold'] = foldnum\n",
    "                tmp_dict['entity_id'] = _id\n",
    "                tmp_dict['label'] = \"n/a\"\n",
    "                tmp_dict[robbert_col_name] = \"n/a\"\n",
    "            elif len(pr)>0:\n",
    "                tmp_dict['fold'] = foldnum\n",
    "                tmp_dict['entity_id'] = _id\n",
    "                tmp_dict['label'] = tr\n",
    "                tmp_dict[robbert_col_name] = pr                \n",
    "            else:\n",
    "                raise ValueError(\"predictions are empty while truth is not\")    \n",
    "            predlist_prep.append(tmp_dict)\n",
    "predlist_df = pd.DataFrame(predlist_prep)\n",
    "predlist_df['bio_label'] = predlist_df['label'].str.replace(r\"([BI])\\-[A-z]+\", \"\\\\1\", \n",
    "                                                        regex=True, case=True).str.strip()\n",
    "predlist_df['bio_robbert'] = predlist_df[robbert_col_name].str.replace(r\"([BI])\\-[A-z]+\", \"\\\\1\", \n",
    "                                                        regex=True, case=True).str.strip()\n",
    "\n",
    "bio_pred = predlist_df[['entity_id', 'bio_label', 'bio_robbert']]\n",
    "predlist_df = predlist_df.loc[predlist_df.bio_label=='B']\n",
    "predlist_df.drop(['bio_label', 'bio_robbert', 'fold'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c94104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predlist_df['label'] = predlist_df.label.map({'B-NotNegated': 'not negated', 'B-Negated': 'negated'})\n",
    "predlist_df[robbert_col_name] = predlist_df[robbert_col_name].map({'B-NotNegated': 'not negated', 'B-Negated': 'negated'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35165cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>label</th>\n",
       "      <th>robbert_32_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DL1667_66_74</td>\n",
       "      <td>not negated</td>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DL1866_88_99</td>\n",
       "      <td>not negated</td>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DL1614_5_12</td>\n",
       "      <td>negated</td>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DL1614_41_51</td>\n",
       "      <td>negated</td>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DL1614_62_71</td>\n",
       "      <td>negated</td>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12710</th>\n",
       "      <td>SP2037_49_73</td>\n",
       "      <td>negated</td>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12713</th>\n",
       "      <td>SP1321_23_30</td>\n",
       "      <td>negated</td>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12715</th>\n",
       "      <td>SP1321_159_168</td>\n",
       "      <td>not negated</td>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12716</th>\n",
       "      <td>SP1508_89_104</td>\n",
       "      <td>not negated</td>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12717</th>\n",
       "      <td>SP1838_85_92</td>\n",
       "      <td>not negated</td>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7646 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            entity_id        label robbert_32_1\n",
       "0        DL1667_66_74  not negated  not negated\n",
       "1        DL1866_88_99  not negated  not negated\n",
       "2         DL1614_5_12      negated      negated\n",
       "4        DL1614_41_51      negated      negated\n",
       "6        DL1614_62_71      negated      negated\n",
       "...               ...          ...          ...\n",
       "12710    SP2037_49_73      negated      negated\n",
       "12713    SP1321_23_30      negated      negated\n",
       "12715  SP1321_159_168  not negated  not negated\n",
       "12716   SP1508_89_104  not negated  not negated\n",
       "12717    SP1838_85_92  not negated  not negated\n",
       "\n",
       "[7646 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predlist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff4f85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ac68a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.do_eval:\n",
    "    dfl = []\n",
    "    for i in range(2):\n",
    "        df = pd.DataFrame(loss_history[i])\n",
    "        df['fold']=i\n",
    "        dfl.append(df)\n",
    "    eval_history = pd.concat(dfl).reset_index()\n",
    "    eval_history['step'] = eval_history['step'].astype(int)\n",
    "    eval_history['fold'] = eval_history['fold'].astype(int)\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=3, figsize=(18,5))\n",
    "    seaborn.lineplot(data=eval_history, x='step', y='f1', hue='epoch', ax=ax[0])\n",
    "    seaborn.lineplot(data=eval_history, x='step', y='recall', hue='epoch', ax=ax[1])\n",
    "    seaborn.lineplot(data=eval_history, x='step', y='precision', hue='epoch', ax=ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85b6d764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.884879</td>\n",
       "      <td>0.868132</td>\n",
       "      <td>0.902284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.836317</td>\n",
       "      <td>0.874332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.884592</td>\n",
       "      <td>0.866272</td>\n",
       "      <td>0.903704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.832695</td>\n",
       "      <td>0.800983</td>\n",
       "      <td>0.867021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.881029</td>\n",
       "      <td>0.868188</td>\n",
       "      <td>0.894256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.895334</td>\n",
       "      <td>0.873309</td>\n",
       "      <td>0.918499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.874520</td>\n",
       "      <td>0.867853</td>\n",
       "      <td>0.881290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.886092</td>\n",
       "      <td>0.868064</td>\n",
       "      <td>0.904884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.884804</td>\n",
       "      <td>0.864671</td>\n",
       "      <td>0.905897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.857888</td>\n",
       "      <td>0.832911</td>\n",
       "      <td>0.884409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fold        f1  precision    recall\n",
       "0     0  0.884879   0.868132  0.902284\n",
       "1     1  0.854902   0.836317  0.874332\n",
       "2     2  0.884592   0.866272  0.903704\n",
       "3     3  0.832695   0.800983  0.867021\n",
       "4     4  0.881029   0.868188  0.894256\n",
       "5     5  0.895334   0.873309  0.918499\n",
       "6     6  0.874520   0.867853  0.881290\n",
       "7     7  0.886092   0.868064  0.904884\n",
       "8     8  0.884804   0.864671  0.905897\n",
       "9     9  0.857888   0.832911  0.884409"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# micro-averaged scores\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fabec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "909081f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B-NotNegated, B-Negated\n",
    "labmap = {'B-NotNegated': False, 'B-Negated': True, 'I-NotNegated': False, 'I-Negated': True}\n",
    "\n",
    "manual_scores = []\n",
    "confusion_matrix = []\n",
    "for i in range(len(predlist)):\n",
    "    # accuracy over all documents, flattened\n",
    "    _predlist = [(labmap[t], 'B' if 'B-' in t else 'I') for l in predlist[i]['prediction'] for t in l if len(l)>0]\n",
    "    _truthlist = [(labmap[t], 'B' if 'B-' in t else 'I') for l in predlist[i]['truth'] for t in l if len(l)>0]\n",
    "\n",
    "    tr_c = []\n",
    "    pr_c = []\n",
    "    tr_r = []\n",
    "    pr_r = []\n",
    "\n",
    "    b_truth = []\n",
    "    b_pred = []\n",
    "    for _t,_p in zip(_truthlist, _predlist):\n",
    "        if _t[1]==_p[1]=='B':\n",
    "            tr_c.append(_t[0])\n",
    "            pr_c.append(_p[0])\n",
    "        tr_r.append(_t[0])\n",
    "        pr_r.append(_p[0])\n",
    "\n",
    "        b_truth.append(_t[1]=='B')\n",
    "        b_pred.append(_p[1]=='B')\n",
    "\n",
    "    tr_c, pr_c, tr_r, pr_r = np.array(tr_c), np.array(pr_c), np.array(tr_r), np.array(pr_r)\n",
    "    b_truth, b_pred = np.array(b_truth), np.array(b_pred)\n",
    "\n",
    "    TN_c = np.sum((pr_c==tr_c) & (pr_c==False))\n",
    "    TP_c = np.sum((pr_c==tr_c) & (pr_c==True))\n",
    "    FP_c = np.sum((pr_c!=tr_c) & (pr_c==True))\n",
    "    FN_c = np.sum((pr_c!=tr_c) & (pr_c==False))\n",
    "\n",
    "    TN_r = np.sum((pr_r==tr_r) & (pr_r==False))\n",
    "    TP_r = np.sum((pr_r==tr_r) & (pr_r==True))\n",
    "    FP_r = np.sum((pr_r!=tr_r) & (pr_r==True))\n",
    "    FN_r = np.sum((pr_r!=tr_r) & (pr_r==False))\n",
    "\n",
    "    TN_b = np.sum((b_pred==b_truth) & (b_pred==False))\n",
    "    TP_b = np.sum((b_pred==b_truth) & (b_pred==True))\n",
    "    FP_b = np.sum((b_pred!=b_truth) & (b_pred==True))\n",
    "    FN_b = np.sum((b_pred!=b_truth) & (b_pred==False))\n",
    "\n",
    "\n",
    "    # micro\n",
    "    f1 = f1_score(tr_r, pr_r, average='micro')\n",
    "    precision = precision_score(tr_r, pr_r, average='micro')\n",
    "    recall = recall_score(tr_r, pr_r, average='micro')\n",
    "    manual_scores.append({'list': 'raw', \n",
    "                          'fold': i, \n",
    "                          'focus': 'micro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # macro\n",
    "    f1 = f1_score(tr_r, pr_r, average='macro')\n",
    "    precision = precision_score(tr_r, pr_r, average='macro')\n",
    "    recall = recall_score(tr_r, pr_r, average='macro')\n",
    "    manual_scores.append({'list': 'raw', \n",
    "                          'fold': i, \n",
    "                          'focus': 'macro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # Negated\n",
    "    f1 = f1_score(tr_r, pr_r)\n",
    "    precision = precision_score(tr_r, pr_r)\n",
    "    recall = recall_score(tr_r, pr_r)\n",
    "    manual_scores.append({'list': 'raw', \n",
    "                          'fold': i, \n",
    "                          'focus': 'negated', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # NotNegated\n",
    "    f1 = f1_score(~tr_r, ~pr_r)\n",
    "    precision = precision_score(~tr_r, ~pr_r)\n",
    "    recall = recall_score(~tr_r, ~pr_r)\n",
    "    manual_scores.append({'list': 'raw',\n",
    "                          'fold': i,\n",
    "                          'focus': 'notnegated', \n",
    "                          'f1': f1,\n",
    "                          'precision': precision,\n",
    "                          'recall': recall})\n",
    "    \n",
    "    confusion_matrix.append({'list': 'raw', 'fold': i, 'TN': TN_r, 'TP': TP_r, 'FN': FN_r, 'FP': FP_r})    \n",
    "    ######################################\n",
    "    # micro\n",
    "    f1 = f1_score(tr_c, pr_c, average='micro')\n",
    "    precision = precision_score(tr_c, pr_c, average='micro')\n",
    "    recall = recall_score(tr_c, pr_c, average='micro')\n",
    "    manual_scores.append({'list': 'clean', \n",
    "                          'fold': i, \n",
    "                          'focus': 'micro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # macro\n",
    "    f1 = f1_score(tr_c, pr_c, average='macro')\n",
    "    precision = precision_score(tr_c, pr_c, average='macro')\n",
    "    recall = recall_score(tr_c, pr_c, average='macro')\n",
    "    manual_scores.append({'list': 'clean', \n",
    "                          'fold': i, \n",
    "                          'focus': 'macro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # Negated\n",
    "    f1 = f1_score(tr_c, pr_c)\n",
    "    precision = precision_score(tr_c, pr_c)\n",
    "    recall = recall_score(tr_c, pr_c)\n",
    "    manual_scores.append({'list': 'clean', \n",
    "                          'fold': i, \n",
    "                          'focus': 'negated', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # NotNegated\n",
    "    f1 = f1_score(~tr_c, ~pr_c)\n",
    "    precision = precision_score(~tr_c, ~pr_c)\n",
    "    recall = recall_score(~tr_c, ~pr_c)\n",
    "    manual_scores.append({'list': 'clean',\n",
    "                          'fold': i,\n",
    "                          'focus': 'notnegated', \n",
    "                          'f1': f1,\n",
    "                          'precision': precision,\n",
    "                          'recall': recall})\n",
    "    \n",
    "    confusion_matrix.append({'list': 'clean', 'fold': i, 'TN': TN_c, 'TP': TP_c, 'FN': FN_c, 'FP': FP_c})    \n",
    "    ######################################\n",
    "    # micro\n",
    "    f1 = f1_score(b_truth, b_pred, average='micro')\n",
    "    precision = precision_score(b_truth, b_pred, average='micro')\n",
    "    recall = recall_score(b_truth, b_pred, average='micro')\n",
    "    manual_scores.append({'list': 'B_I', \n",
    "                          'fold': i, \n",
    "                          'focus': 'micro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # macro\n",
    "    f1 = f1_score(b_truth, b_pred, average='macro')\n",
    "    precision = precision_score(b_truth, b_pred, average='macro')\n",
    "    recall = recall_score(b_truth, b_pred, average='macro')\n",
    "    manual_scores.append({'list': 'B_I', \n",
    "                          'fold': i, \n",
    "                          'focus': 'macro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # Negated\n",
    "    f1 = f1_score(b_truth, b_pred)\n",
    "    precision = precision_score(b_truth, b_pred)\n",
    "    recall = recall_score(b_truth, b_pred)\n",
    "    manual_scores.append({'list': 'B_I', \n",
    "                          'fold': i, \n",
    "                          'focus': 'negated', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # NotNegated\n",
    "    f1 = f1_score(~b_truth, ~b_pred)\n",
    "    precision = precision_score(~b_truth, ~b_pred)\n",
    "    recall = recall_score(~b_truth, ~b_pred)\n",
    "    manual_scores.append({'list': 'B_I',\n",
    "                          'fold': i,\n",
    "                          'focus': 'notnegated', \n",
    "                          'f1': f1,\n",
    "                          'precision': precision,\n",
    "                          'recall': recall})\n",
    "    \n",
    "    confusion_matrix.append({'list': 'B_I', 'fold': i, 'TN': TN_b, 'TP': TP_b, 'FN': FN_b, 'FP': FP_b})    \n",
    "    \n",
    "manual_scores_df = pd.DataFrame(data=manual_scores)\n",
    "confusion_matrix_df = pd.DataFrame(data=confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fe90a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>list</th>\n",
       "      <th>focus</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">B_I</th>\n",
       "      <th>macro</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.934879</td>\n",
       "      <td>0.938559</td>\n",
       "      <td>0.932071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.938006</td>\n",
       "      <td>0.938006</td>\n",
       "      <td>0.938006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negated</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.949112</td>\n",
       "      <td>0.936842</td>\n",
       "      <td>0.961902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notnegated</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.920646</td>\n",
       "      <td>0.940276</td>\n",
       "      <td>0.902240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">clean</th>\n",
       "      <th>macro</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.963970</td>\n",
       "      <td>0.973451</td>\n",
       "      <td>0.955262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.981753</td>\n",
       "      <td>0.981753</td>\n",
       "      <td>0.981753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negated</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.938682</td>\n",
       "      <td>0.961692</td>\n",
       "      <td>0.917171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notnegated</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.989258</td>\n",
       "      <td>0.985210</td>\n",
       "      <td>0.993353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">raw</th>\n",
       "      <th>macro</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.960234</td>\n",
       "      <td>0.969542</td>\n",
       "      <td>0.951751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.979470</td>\n",
       "      <td>0.979470</td>\n",
       "      <td>0.979470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negated</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.932604</td>\n",
       "      <td>0.955326</td>\n",
       "      <td>0.911477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notnegated</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.987865</td>\n",
       "      <td>0.983758</td>\n",
       "      <td>0.992026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  fold        f1  precision    recall\n",
       "list  focus                                          \n",
       "B_I   macro        4.5  0.934879   0.938559  0.932071\n",
       "      micro        4.5  0.938006   0.938006  0.938006\n",
       "      negated      4.5  0.949112   0.936842  0.961902\n",
       "      notnegated   4.5  0.920646   0.940276  0.902240\n",
       "clean macro        4.5  0.963970   0.973451  0.955262\n",
       "      micro        4.5  0.981753   0.981753  0.981753\n",
       "      negated      4.5  0.938682   0.961692  0.917171\n",
       "      notnegated   4.5  0.989258   0.985210  0.993353\n",
       "raw   macro        4.5  0.960234   0.969542  0.951751\n",
       "      micro        4.5  0.979470   0.979470  0.979470\n",
       "      negated      4.5  0.932604   0.955326  0.911477\n",
       "      notnegated   4.5  0.987865   0.983758  0.992026"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_scores_df.groupby(['list', 'focus']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c16f81",
   "metadata": {},
   "source": [
    "## Append to other results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3909f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_result_file = '../results/merged_results.csv.gz'\n",
    "results = pd.read_csv(merged_result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0d2b801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8116/2061256545.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  entities = results.entity_id.str.replace(r\"\\_[0-9]+\\_[0-9]+\", \"\").unique()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5365"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = results.entity_id.str.replace(r\"\\_[0-9]+\\_[0-9]+\", \"\").unique()\n",
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d67edca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_check = predlist_df[['entity_id', 'label']].set_index('entity_id').join(results[['entity_id', 'label']].set_index('entity_id'),\n",
    "                                                                how='inner',rsuffix='_or')\n",
    "\n",
    "(sanity_check['label'] == sanity_check['label_or']).sum()==sanity_check.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cdd2d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predlist_df.set_index('entity_id', inplace=True)\n",
    "results.set_index('entity_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bb825cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>robbert_32_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entity_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DL1667_66_74</th>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DL1866_88_99</th>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DL1614_5_12</th>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DL1614_41_51</th>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DL1614_62_71</th>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP2037_49_73</th>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP1321_23_30</th>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP1321_159_168</th>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP1508_89_104</th>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP1838_85_92</th>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7646 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               robbert_32_1\n",
       "entity_id                  \n",
       "DL1667_66_74    not negated\n",
       "DL1866_88_99    not negated\n",
       "DL1614_5_12         negated\n",
       "DL1614_41_51        negated\n",
       "DL1614_62_71        negated\n",
       "...                     ...\n",
       "SP2037_49_73        negated\n",
       "SP1321_23_30        negated\n",
       "SP1321_159_168  not negated\n",
       "SP1508_89_104   not negated\n",
       "SP1838_85_92    not negated\n",
       "\n",
       "[7646 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predlist_df[[robbert_col_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2bef0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results = results.join(predlist_df[[robbert_col_name]], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf9a8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results.to_csv(\"../results/robbert_predictions.csv.gz\", index=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2525293",
   "metadata": {},
   "outputs": [],
   "source": [
    "robberts = [c for c in total_results.columns if 'robbert' in c]\n",
    "unanimous = total_results.dropna()[['label', 'bilstm_cv', 'rule_based']+robberts].apply(lambda x: x[0]==x[1]==x[2]==x[3]==x[4], \n",
    "                                                                         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf4026ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7923474945533769"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(unanimous)/total_results.loc[total_results[robbert_col_name].isna()==False].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "704c4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_dissenters(x):\n",
    "    return int(x[0] != x[1])+\\\n",
    "           int(x[0] != x[2])+\\\n",
    "           int(x[0] != x[3])+\\\n",
    "           int(x[0] != x[4])+\\\n",
    "           int(x[0] != x[5])+\\\n",
    "           int(x[0] != x[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdf8171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dissenters = total_results.dropna()[['label','bilstm', 'bilstm_cv','rule_based']+robberts]\\\n",
    "                            .apply(number_of_dissenters, axis=1)\n",
    "\n",
    "total_results.dissenters = np.nan\n",
    "total_results.loc[dissenters.index, 'dissenters'] = dissenters.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b51e6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80*2*10/60 hours for 2 epochs and block size 512\n",
    "# 16*2*10/60 hours for 2 epochs and block size 128\n",
    "# 6*2*10/60 hours for 2 epochs and block size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4afe09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
