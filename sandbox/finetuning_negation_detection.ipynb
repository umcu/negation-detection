{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53affcce",
   "metadata": {},
   "source": [
    "Starting from a \n",
    "* pre-trained model with a \n",
    "* pre-trained tokenizer\n",
    "\n",
    "we perform finetuning on a negation task\n",
    "\n",
    "Tweakable:\n",
    "* test-time augmentation\n",
    "* flattening B-, I-\n",
    "* block-size\n",
    "* batch-size\n",
    "* gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393fea2f",
   "metadata": {},
   "source": [
    "Experiments:\n",
    "* (negation paper) 32 block size, 32 batch size, with/without centered validation\n",
    "  * MedRoBERTa.nl\n",
    "  * RobBERT v2\n",
    "* 32 block size, 32 batch size, with centered validation, 1-12 layers \n",
    "  * MedRoBERTa.nl\n",
    "  * RobBERT v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39671c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "except ImportError:\n",
    "    # load .env\n",
    "    dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "    os.environ['LD_LIBRARY_PATH'] = os.getenv('LD_LIBRARY_PATH')\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "    \n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import device, cuda, version\n",
    "\n",
    "#import apex\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple\n",
    "from transformers import AutoTokenizer, RobertaTokenizer, AutoModelForTokenClassification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from utils import dcc_splitter as splitter\n",
    "from utils import ner_training as trainer\n",
    "#import dcc_splitter as splitter\n",
    "\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cc061",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a190030",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda.is_available():\n",
    "    print('CUDA, GPU, is working')\n",
    "    os.system(\"espeak 'CUDA, GPU, is working!'\")\n",
    "else:\n",
    "    print('CUDA, GPU, is not working')\n",
    "    os.system(\"espeak 'CUDA, GPU, is NOT working!'\")\n",
    "    \n",
    "device = device(\"cuda:0\") if (cuda.is_available()) & (use_gpu==True) else device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c680fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_folder = \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels\"\n",
    "base_folder = \"//Ds/data/LAB/laupodteam/AIOS/Bram/language_modeling/Models/language_models\"\n",
    "\n",
    "dcc_dir = None\n",
    "output_dir = os.path.join(base_folder, \"finetuned\")\n",
    "skip_file = '../data/EMCDutchClinicalCorpus/DCC_files_to_exclude.json'\n",
    "n_splits = 10\n",
    "random_state = 42\n",
    "reduce_tags = False\n",
    "centered_testing = True\n",
    "train_on_all = True\n",
    "zero_shot = False # do not train the attention layers, only the final linear layer\n",
    "#num_trainable_layers = None # None: all, overrides zero_shot\n",
    "mod_name =  \"finetuned/negation_512\"  # \"base/medroberta\"\n",
    "\n",
    "args = namedtuple\n",
    "args.task = \"experiencer\" # experiencer, temporality, negation\n",
    "args.model_path = os.path.join(base_folder, mod_name)\n",
    "args.model_type = \"roberta\" # bertje \n",
    "args.num_epochs = 3 # 10 for zero-shot\n",
    "args.eval_steps = 500 \n",
    "args.lr =  4e-5 # 5e-5 for non-zeroshot 64 bs, 5e-3 for zero-shot 128 bs, 1e-4 for nonzeroshot 32 bs\n",
    "args.batch_size= 32 #4\n",
    "args.gradient_accumulation_steps = 1 #16\n",
    "args.block_size = 64 # 512 \n",
    "args.save_model=True\n",
    "args.bio=True\n",
    "args.do_eval=True\n",
    "args.do_write=True\n",
    "args.bootstrap=False\n",
    "args.do_print_class_report=False\n",
    "args.amp=True\n",
    "\n",
    "mod_name = mod_name.replace(\"/\",\"_\").replace(\"-\",\"_\")+\"_\"+\"_\".join([str(args.num_epochs), \n",
    "                                                   str(args.batch_size*args.gradient_accumulation_steps), \n",
    "                                                   str(args.block_size),\n",
    "                                                   \"zeroShot\" if zero_shot else \"\",\n",
    "                                                   \"centeredVal\" if centered_testing else \"\",\n",
    "                                                   args.task])\n",
    "mod_name = mod_name.strip(\"_\")\n",
    "output_folder = \"fine_tuned_token_\"+args.task+\"_\"+mod_name\n",
    "\n",
    "args.output_dir = os.path.join(output_dir, output_folder)\n",
    "\n",
    "# check if the output folder exists\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "\n",
    "random.seed(77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69942f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.block_size determines how many text snippets are  used for training, see ner_training.py lines 118--141\n",
    "# obviously this is a code-design flaw that should be mended.\n",
    "# the dataset loader should  include the id_begin_end in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce9d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load NER DCC set\n",
    "dcc = pd.read_csv(\"../data/DCC_df_ASL1_hypothetical.csv\", \n",
    "                  sep=\"\\t\", \n",
    "                  skip_blank_lines=True, \n",
    "                  engine=\"python\", \n",
    "                  encoding=\"latin1\",\n",
    "                  on_bad_lines='warn', \n",
    "                  keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcc.Temporality.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcc.Negation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcc.Experiencer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e74f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcc.BIO.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a85c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcc.loc[dcc.BIO!='O'][['Id', 'Begin', 'End']].apply(lambda x: \"_\".join(str(x)), axis=1).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a61e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Texts = dcc.groupby('Id').Word.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reduce_tags:\n",
    "    tag_ids = {'negation':{'B-Negated':0,'B-NotNegated':1,'I-Negated':0,'I-NotNegated':1},\n",
    "              'temporality':{'B-Recent':0,'B-Historical':1,'B-Hypothetical':2,'I-Recent':0,\n",
    "                             'I-Historical':1,'I-Hypothetical':2},\n",
    "              'experiencer':{'B-Patient':0,'B-Other':1,'I-Patient':0,'I-Other':1}}\n",
    "else:\n",
    "    tag_ids = {'negation':{'B-Negated':0,'B-NotNegated':1,'I-Negated':2,'I-NotNegated':3},\n",
    "              'temporality':{'B-Recent':0,'B-Historical':1,'B-Hypothetical':2,'I-Recent':3,\n",
    "                             'I-Historical':4,'I-Hypothetical':5},\n",
    "              'experiencer':{'B-Patient':0,'B-Other':1,'I-Patient':2,'I-Other':3}}\n",
    "\n",
    "tag2id = tag_ids[args.task]\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b462f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcc['num_tokens'] = dcc.Word.apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "dcc['cumsum_tokens'] = dcc.groupby('Id').num_tokens.cumsum()\n",
    "dcc['letter_type'] = dcc.Id.str.slice(0,2)\n",
    "tokens_df = dcc.groupby(['Id', 'letter_type']).cumsum_tokens.agg(num_tokens=max).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "tokens_df[(tokens_df.letter_type=='DL')]\\\n",
    "                        .num_tokens.plot.kde(lw=4, alpha=0.75, label='discharge letters')\n",
    "tokens_df[(tokens_df.letter_type=='SP')]\\\n",
    "                        .num_tokens.plot.kde(lw=4, alpha=0.75, label='specialist letters')\n",
    "tokens_df[(tokens_df.letter_type=='RD')]\\\n",
    "                        .num_tokens.plot.kde(lw=4, alpha=0.75, label='radiology reports')\n",
    "tokens_df[(tokens_df.letter_type=='GP')]\\\n",
    "                        .num_tokens.plot.kde(lw=4, alpha=0.75, label='general practitioner entries')\n",
    "\n",
    "tokens_df[(tokens_df.letter_type=='DL')]\\\n",
    "                        .num_tokens.plot.hist(bins=60, alpha=0.35, density=True, color='blue',\n",
    "                                              label='discharge letters')\n",
    "tokens_df[(tokens_df.letter_type=='SP')]\\\n",
    "                        .num_tokens.plot.hist(bins=60, alpha=0.35, density=True, color='orange',\n",
    "                                              label='specialist letters')\n",
    "tokens_df[(tokens_df.letter_type=='RD')]\\\n",
    "                        .num_tokens.plot.hist(bins=60, alpha=0.35, density=True, color='green',\n",
    "                                              label='radiology reports')\n",
    "tokens_df[(tokens_df.letter_type=='GP')]\\\n",
    "                        .num_tokens.plot.hist(bins=60, alpha=0.35, density=True, color='red',\n",
    "                                              label='general practitioner entries')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "#plt.semilogx()\n",
    "plt.xlabel(\"Number of tokens\")\n",
    "plt.xlim(1,512)\n",
    "\n",
    "print(f\"Mean \\nDL tokens: {tokens_df[(tokens_df.letter_type=='DL')].num_tokens.mean()}\\n\\\n",
    "SP tokens :{tokens_df[(tokens_df.letter_type=='SP')].num_tokens.mean()}\\n\\\n",
    "RD tokens :{tokens_df[(tokens_df.letter_type=='RD')].num_tokens.mean()}\\n\\\n",
    "GP tokens :{tokens_df[(tokens_df.letter_type=='GP')].num_tokens.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dfa1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find I that is not preceeded by a B..\n",
    "def prior_equal_to(x, equal_to='O'):\n",
    "    if isinstance(x, pd.Series):\n",
    "        _x = np.array(x.values)\n",
    "        xout= np.zeros_like(_x)\n",
    "        for i in range(1, len(_x)):\n",
    "            xout[i] = (_x[i-1] == equal_to)\n",
    "        return xout\n",
    "    else:\n",
    "        x = np.nan\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf923e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcc['O_as_prior'] = dcc.groupby('Id').BIO.transform(prior_equal_to, equal_to='O')\n",
    "dcc['I_as_prior'] = dcc.groupby('Id').BIO.transform(prior_equal_to,  equal_to='I')\n",
    "dcc['SpecialChar_as_prior'] = dcc.groupby('Id').Word.transform(prior_equal_to,\n",
    "                                                           **{'equal_to':['.', ',', ':', ';', \n",
    "                                                                          '!', '?','-','+']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcc.loc[(dcc.BIO=='I') & (dcc.O_as_prior==True), 'BIO'] = 'B'\n",
    "dcc.loc[(dcc.BIO=='I') & (dcc.SpecialChar_as_prior==True), 'BIO'] = 'B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a07ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for centering, just change the dcc..\n",
    "# improvement: add boolean to indicate whether entity is in center, process in training routine :)\n",
    "sub_dfs = []\n",
    "for _id in tqdm(dcc.Id.unique()):\n",
    "    cst_list = dcc[(dcc.Id==_id) & (dcc.BIO=='B')].cumsum_tokens.to_list()\n",
    "    for l, cst in enumerate(cst_list):\n",
    "        sub_df = dcc[(dcc.Id==_id) & \n",
    "                (dcc.cumsum_tokens<cst+int(0.5*args.block_size)) &\n",
    "                (dcc.cumsum_tokens>cst-int(0.5*args.block_size))].reset_index(drop=True)\n",
    "        sub_df['center']=False\n",
    "        sub_df.loc[sub_df.cumsum_tokens==cst, 'center'] = True\n",
    "        sub_df.loc[:, 'sub_Id'] = l\n",
    "        sub_dfs.append(sub_df)\n",
    "_dcc = pd.concat(sub_dfs, axis=0).reset_index(drop=True)\n",
    "_dcc['sub_Id'] = _dcc['sub_Id'].astype(str)\n",
    "_dcc['Id'] = _dcc['Id'] + \"_\" + _dcc['sub_Id']\n",
    "dcc = _dcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4051ff0c",
   "metadata": {},
   "source": [
    "## Over all document sources\n",
    "\n",
    "improvement: \n",
    "* only output best model based on validation scores\n",
    "* save results after each epoch --> should be included in train_model\n",
    "* allow freezing specific layers: https://discuss.huggingface.co/t/how-to-freeze-some-layers-of-bertmodel/917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb187ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcc.loc[:,'Begin'] = dcc['Begin'].astype(str)\n",
    "dcc.loc[:, 'End'] = dcc['End'].astype(str)\n",
    "dcc=dcc[['Id', 'Word', 'BIO', 'Negation', 'Experiencer', \n",
    "         'Temporality', 'Begin', 'End', 'center']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7bf63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcc-splitter for folds\n",
    "docs_ids = [\"_\".join(d.split(\"_\")[:-1])for d in dcc.Id.unique().tolist()]\n",
    "group_ids = [d[:6] for d in docs_ids]\n",
    "\n",
    "dcc_splitter = splitter.DCCSplitter(None,    # dcc_dir                                 \n",
    "                                    None,    # output_dir\n",
    "                                    skip_file, \n",
    "                                    n_splits, \n",
    "                                    random_state, \n",
    "                                    doc_ids=docs_ids,\n",
    "                                    group_ids = group_ids,\n",
    "                                    write_to_file=False)\n",
    "splits = dcc_splitter.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is no leakage\n",
    "overlap_counts = []\n",
    "print(f'Checking {len(splits)} splits for overlap of document IDs')\n",
    "for _split in splits:\n",
    "    test =  [_id[:7] for _id in _split['test']]\n",
    "    train = [_id[:7] for _id in _split['train']]\n",
    "\n",
    "    overlap_counts.append(len(set(test).intersection(set(train))))\n",
    "print(overlap_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a615fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659dd996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle through folds\n",
    "scores = []\n",
    "predlist = []\n",
    "test_lists = []\n",
    "loss_history = {}\n",
    "for idx, fold in tqdm(enumerate(splits)):\n",
    "    # re-init model for each fold, otherwise it keeps on training the same throughout all folds..\n",
    "    token_model = AutoModelForTokenClassification.from_pretrained(args.model_path, \n",
    "                                                        num_labels = len(tag2id))\n",
    "    \n",
    "    if zero_shot:\n",
    "        for name, param in token_model.named_parameters():\n",
    "            if 'classifier' not in name: # classifier layer\n",
    "                param.requires_grad = False\n",
    "\n",
    "    train_list, test_list = fold['train'], list(set(fold['test']))\n",
    "    \n",
    "    ## eval is optional (to gauge the best number of steps/epochs)\n",
    "    eval_list = random.choices(train_list,k=int(len(train_list)/10)) if args.do_eval else []\n",
    "    \n",
    "    eval_dcc = dcc.loc[dcc.Id.str.slice(0,6).isin(eval_list)]\n",
    "    test_dcc = dcc.loc[dcc.Id.str.slice(0,6).isin(test_list)]\n",
    "\n",
    "    if centered_testing:                \n",
    "        '''Note\n",
    "                We do this to ensure that we only validate on the centered terms.\n",
    "        '''\n",
    "        eval_dcc.loc[eval_dcc.center==False, ['BIO', 'Negation', 'Experiencer', 'Temporality']] = 'O'\n",
    "        test_dcc.loc[test_dcc.center==False, ['BIO', 'Negation', 'Experiencer', 'Temporality']] = 'O'\n",
    "\n",
    "    \n",
    "    train_dcc = dcc.loc[(dcc.Id.str.slice(0,6).isin(train_list)) & (~dcc.Id.isin(eval_list))]\n",
    "    \n",
    "    test_list = test_dcc.Id.tolist()\n",
    "    eval_list = eval_dcc.Id.tolist()\n",
    "\n",
    "    ###\n",
    "    train_dataset = trainer.TextDatasetFromDataFrame(train_dcc, tokenizer, args) \n",
    "    test_dataset = trainer.TextDatasetFromDataFrame(test_dcc, tokenizer, args)\n",
    "    eval_dataset = trainer.TextDatasetFromDataFrame(eval_dcc, tokenizer, args)\n",
    "    \n",
    "    args.do_print_class_report=False\n",
    "    # Train on all document sources\n",
    "    trained_model, eval_loss_history = trainer.train_model(model=token_model.to(device), \n",
    "                                                           tokenizer=tokenizer, \n",
    "                                                           train_dataset=train_dataset, \n",
    "                                                           eval_dataset=eval_dataset, \n",
    "                                                           tag2id=tag2id,\n",
    "                                                           device=device, \n",
    "                                                           args=args,\n",
    "                                                           max_grad_norm=1.0,\n",
    "                                                           amp=False)\n",
    "    args.do_print_class_report=True\n",
    "    # Evaluate on all document sources\n",
    "    f1, prec, rec, preds, truth, test_ids = trainer.eval_model(model=trained_model, \n",
    "                                       tokenizer=tokenizer, \n",
    "                                       eval_dataset=test_dataset, \n",
    "                                       tag2id=tag2id, \n",
    "                                       device=device, \n",
    "                                       args=args, \n",
    "                                       return_pred=True)\n",
    "    \n",
    "    loss_history[idx]=eval_loss_history\n",
    "    \n",
    "    #test_ids = [\"_\".join(t) for t in zip(test_dcc.Id, test_dcc.Begin, test_dcc.End)]\n",
    "    scores.append({'fold': idx, 'f1': f1, 'precision': prec, 'recall': rec})\n",
    "    predlist.append({'fold': idx, 'prediction': preds, 'truth': truth, 'ids': test_ids})\n",
    "    test_lists.append(test_ids)\n",
    "\n",
    "if  train_on_all:\n",
    "    args.save_model=True\n",
    "    args.do_eval=False\n",
    "    dataset = trainer.TextDatasetFromDataFrame(dcc, tokenizer, args)    \n",
    "    token_model = AutoModelForTokenClassification.from_pretrained(args.model_path, \n",
    "                                                                  num_labels = len(tag2id))\n",
    "    final_model, eval_loss_history = trainer.train_model(model=token_model.to(device), \n",
    "                                                        tokenizer=tokenizer, \n",
    "                                                        train_dataset=dataset, \n",
    "                                                        eval_dataset=None, \n",
    "                                                        tag2id=tag2id,\n",
    "                                                        device=device, \n",
    "                                                        args=args,\n",
    "                                                        max_grad_norm=1.0,\n",
    "                                                        amp=False)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ad477",
   "metadata": {},
   "outputs": [],
   "source": [
    "predlist_prep = []\n",
    "robbert_col_name = mod_name\n",
    "\n",
    "for foldnum, foldres in enumerate(predlist):\n",
    "    ids = foldres['ids']\n",
    "    for prs, trs, ids in zip(foldres['prediction'], foldres['truth'], foldres['ids']):\n",
    "        for pr, tr, _id in zip(prs, trs, ids):\n",
    "            tmp_dict={}\n",
    "            if len(pr)==len(tr)==0:\n",
    "                tmp_dict['fold'] = foldnum\n",
    "                tmp_dict['entity_id'] = _id\n",
    "                tmp_dict['label'] = \"n/a\"\n",
    "                tmp_dict[robbert_col_name] = \"n/a\"\n",
    "            elif len(pr)>0:\n",
    "                tmp_dict['fold'] = foldnum\n",
    "                tmp_dict['entity_id'] = _id\n",
    "                tmp_dict['label'] = tr\n",
    "                tmp_dict[robbert_col_name] = pr                \n",
    "            else:\n",
    "                raise ValueError(\"predictions are empty while truth is not\")    \n",
    "            predlist_prep.append(tmp_dict)\n",
    "predlist_df = pd.DataFrame(predlist_prep)\n",
    "predlist_df['bio_label'] = predlist_df['label'].str.replace(r\"([BI])\\-[A-z]+\", \"\\\\1\", \n",
    "                                                        regex=True, case=True).str.strip()\n",
    "predlist_df['bio_robbert'] = predlist_df[robbert_col_name].str.replace(r\"([BI])\\-[A-z]+\", \"\\\\1\", \n",
    "                                                        regex=True, case=True).str.strip()\n",
    "\n",
    "bio_pred = predlist_df[['entity_id', 'bio_label', 'bio_robbert']]\n",
    "predlist_df = predlist_df.loc[predlist_df.bio_label.isin(['B', 'I'])]\n",
    "predlist_df.drop(['bio_label', 'bio_robbert', 'fold'], axis=1, inplace=True)\n",
    "\n",
    "neg_map = {'B-NotNegated': 'not negated', 'B-Negated': 'negated', \n",
    "           'I-NotNegated': 'not negated', 'I-Negated': 'negated'}\n",
    "temp_map = {'B-Recent': 'recent', 'B-Hypothetical': 'hypothetical', 'B-Historical': 'historical',\n",
    "            'I-Recent': 'recent', 'I-Hypothetical': 'hypothetical', 'I-Historical': 'historical'}\n",
    "exp_map  = {'B-Patient': 'patient', 'B-Other': 'other',\n",
    "            'I-Patient': 'patient', 'I-Other': 'other'}\n",
    "\n",
    "if args.task == 'negation':\n",
    "    lab_map = neg_map\n",
    "elif args.task == 'temporality':\n",
    "    lab_map = temp_map\n",
    "elif args.task == 'experiencer':\n",
    "    lab_map = exp_map\n",
    "    \n",
    "predlist_df['label'] = predlist_df.label.map(lab_map)\n",
    "predlist_df[robbert_col_name] = predlist_df[robbert_col_name].map(lab_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad65f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(predlist_df.label, predlist_df[robbert_col_name]))\n",
    "\n",
    "\n",
    "print(confusion_matrix(predlist_df.label, predlist_df[robbert_col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415cc9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have multiple entries for each token\n",
    "# ideally we pick only the token that was centered\n",
    "tmp = predlist_df.copy()\n",
    "tmp.loc[:, 'sub_id'] = tmp.entity_id.str.split(\"_\").apply(lambda x: x[1])\n",
    "tmp.loc[:, 'entity_id'] = tmp.entity_id.str.split(\"_\").apply(lambda x: \"_\".join([x[0],x[2],x[3]]))\n",
    "tmp.drop_duplicates(subset=['entity_id', 'label'], keep='first', inplace=True)\n",
    "predlist_df = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9aa59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(predlist_df.label, predlist_df[robbert_col_name]))\n",
    "\n",
    "\n",
    "print(confusion_matrix(predlist_df.label, predlist_df[robbert_col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab16c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.do_eval = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f103214",
   "metadata": {},
   "outputs": [],
   "source": [
    "predlist_df.loc[(predlist_df.label=='historical') &\n",
    "                 (predlist_df.finetuned_negation_512_5_32_64__centeredVal_experiencer=='recent'),\n",
    "                'entity_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11da3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.do_eval:\n",
    "    dfl = []\n",
    "    for i in range(2):\n",
    "        df = pd.DataFrame(loss_history[i])\n",
    "        df['fold']=i\n",
    "        dfl.append(df)\n",
    "    eval_history = pd.concat(dfl).reset_index()\n",
    "    eval_history['step'] = eval_history['step'].astype(int)\n",
    "    eval_history['fold'] = eval_history['fold'].astype(int)\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=3, figsize=(18,5))\n",
    "    seaborn.lineplot(data=eval_history, x='step', y='f1', hue='fold', ax=ax[0])\n",
    "    seaborn.lineplot(data=eval_history, x='step', y='recall', hue='fold', ax=ax[1])\n",
    "    seaborn.lineplot(data=eval_history, x='step', y='precision', hue='fold', ax=ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c932ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3fbf5f",
   "metadata": {},
   "source": [
    "## Append to other results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_result_file = '../results/merged_results.csv.gz'\n",
    "results = pd.read_csv(merged_result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = results.entity_id.str.replace(r\"\\_[0-9]+\\_[0-9]+\", \"\").unique()\n",
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc788923",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check = predlist_df[['entity_id', 'label']].set_index('entity_id').join(results[['entity_id', 'label']].set_index('entity_id'),\n",
    "                                                                how='inner',rsuffix='_or')\n",
    "\n",
    "(sanity_check['label'] == sanity_check['label_or']).sum()==sanity_check.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predlist_df.set_index('entity_id', inplace=True)\n",
    "results.set_index('entity_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d90bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results = results.drop(robbert_col_name, axis=1, errors='ignore').join(predlist_df[[robbert_col_name]], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fd5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results[['category', 'label', robbert_col_name]].to_csv(f\"../results/{mod_name}_predictions.csv.gz\", index=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03094264",
   "metadata": {},
   "outputs": [],
   "source": [
    "robberts = [c for c in total_results.columns if 'robbert' in c]\n",
    "unanimous = total_results.dropna()[['label', 'bilstm_cv', 'rule_based']+robberts].apply(lambda x: x[0]==x[1]==x[2]==x[3]==x[4], \n",
    "                                                                         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(unanimous)/total_results.loc[total_results[robbert_col_name].isna()==False].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279a84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_dissenters(x):\n",
    "    return int(x[0] != x[1])+\\\n",
    "           int(x[0] != x[2])+\\\n",
    "           int(x[0] != x[3])+\\\n",
    "           int(x[0] != x[4])+\\\n",
    "           int(x[0] != x[5])+\\\n",
    "           int(x[0] != x[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e7f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dissenters = total_results.dropna()[['label','bilstm', 'bilstm_cv','rule_based']+robberts]\\\n",
    "                            .apply(number_of_dissenters, axis=1)\n",
    "\n",
    "total_results.dissenters = np.nan\n",
    "total_results.loc[dissenters.index, 'dissenters'] = dissenters.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505396ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405752af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80*2*10/60 hours for 2 epochs and block size 512\n",
    "# 16*2*10/60 hours for 2 epochs and block size 128\n",
    "# 6*2*10/60 hours for 2 epochs and block size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcb36e9",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef376d4b",
   "metadata": {},
   "source": [
    "## Inference in practice\n",
    "\n",
    "Suppose we have some sentence:\n",
    "```De patient vertoont tekenen van ischemische hartziekte. Op de MRI waren sporen van infarctie te zien. Ik concludeer dat hier sprake is van significante stenose```\n",
    "\n",
    "Now, how do we deploy our model?\n",
    "First we need to establish **what** we wish to label, for this we can use MedCAT trained on Dutch clinical texts to recognize medical concepts. For our purpose we may wish the constrain the possible medical concepts\n",
    "to symptoms and pathologies. Now, given our approximated medical concepts we apply our negation model to \n",
    "estimate whether the concept is negated or not.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff7306b44ce94ea7a79c277741d02ad9c2d55e697cb76a3c651c54412a0f9d74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
