{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5fa31f8",
   "metadata": {},
   "source": [
    "Starting from a \n",
    "* pre-trained model with a \n",
    "* pre-trained tokenizer\n",
    "\n",
    "we perform finetuning on a negation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02705f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import device, cuda, version\n",
    "\n",
    "import apex\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple\n",
    "from transformers import AutoTokenizer, RobertaTokenizer, AutoModelForTokenClassification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from utils import dcc_splitter as splitter\n",
    "from utils import ner_training as trainer\n",
    "#import dcc_splitter as splitter\n",
    "\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f132a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2af5304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koekiemonster/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA, GPU, is not working\n"
     ]
    }
   ],
   "source": [
    "if cuda.is_available():\n",
    "    print('CUDA, GPU, is working')\n",
    "    os.system(\"espeak 'CUDA, GPU, is working!'\")\n",
    "else:\n",
    "    print('CUDA, GPU, is not working')\n",
    "    os.system(\"espeak 'CUDA, GPU, is NOT working!'\")\n",
    "    \n",
    "device = device(\"cuda:0\") if (cuda.is_available()) & (use_gpu==True) else device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fb57b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcc_dir = None\n",
    "output_dir = None\n",
    "skip_file = None\n",
    "n_splits = 10\n",
    "random_state = None\n",
    "base_folder = \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels\"\n",
    "mod_name = \"MedRoBERTa\" # \"robbert-v2-dutch-base\" # belabBERT_115k # bert-base-dutch, #MedRoBERTa\n",
    " \n",
    "\n",
    "args = namedtuple\n",
    "args.task = \"temporality\" # experiencer, temporality\n",
    "args.model_path = os.path.join(base_folder, mod_name)\n",
    "args.model_type = \"roberta\" # bertje \n",
    "args.num_epochs = 1 # 2\n",
    "args.eval_steps = 1 \n",
    "args.lr = 5e-5\n",
    "args.batch_size= 4\n",
    "args.gradient_accumulation_steps = 16\n",
    "args.block_size = 256 # block size determines inclusion   \n",
    "args.save_model=True\n",
    "args.bio=True\n",
    "args.do_eval=False\n",
    "args.do_write=False\n",
    "args.bootstrap=False\n",
    "args.do_print_class_report=False\n",
    "args.amp=True\n",
    "\n",
    "mod_name = mod_name.replace(\"-\",\"_\")+\"_\"+\"_\".join([str(args.num_epochs), \n",
    "                                                   str(args.batch_size*args.gradient_accumulation_steps), \n",
    "                                                   str(args.block_size)])\n",
    "output_folder = \"fine_tuned_token_\"+args.task+\"_\"+mod_name\n",
    "\n",
    "args.output_dir = os.path.join(base_folder, output_folder)\n",
    "\n",
    "\n",
    "random.seed(77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb1c0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.block_size determines how many text snippets are  used for training, see ner_training.py lines 118--141\n",
    "# obviously this is a code-design flaw that should be mended.\n",
    "# the dataset loader should  include the id_begin_end in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bafdb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcc-splitter for folds\n",
    "dcc_splitter = splitter.DCCSplitter(dcc_dir, output_dir, skip_file, n_splits, random_state, write_to_file=False)\n",
    "splits = dcc_splitter.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bcb19ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load NER DCC set\n",
    "dcc = pd.read_csv(\"../data/RobBERT/DCC_df.csv\", \n",
    "                  sep=\"\\t\", \n",
    "                  skip_blank_lines=True, \n",
    "                  engine=\"python\", \n",
    "                  encoding=\"latin-1\",\n",
    "                  on_bad_lines=\"warn\", \n",
    "                  keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccc8b55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O             146395\n",
       "NotNegated     15713\n",
       "Negated         3017\n",
       "Name: Negation, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcc.Negation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d71ec0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11882"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcc.loc[dcc.BIO!='O'][['Id', 'Begin', 'End']].apply(lambda x: \"_\".join(x), axis=1).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3234908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Texts = dcc.groupby('Id').Word.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52a64c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_ids = {'negation':{'B-Negated':0,'B-NotNegated':1,'I-Negated':2,'I-NotNegated':3},\n",
    "          'temporality':{'B-Recent':0,'B-Historical':1,'B-Hypothetical':2,'I-Recent':3,\n",
    "                         'I-Historical':4,'I-Hypothetical':5},\n",
    "          'experiencer':{'B-Patient':0,'B-Other':1,'I-Patient':2,'I-Other':3}}\n",
    "\n",
    "tag2id = tag_ids[args.task]\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017aa7e5",
   "metadata": {},
   "source": [
    "## Over all document sources\n",
    "\n",
    "improvement: only output best model based on validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2865323a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████| 1207/1207 [33:56<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koekiemonster/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\r",
      "1it [35:44, 2144.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.856 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Historical       0.00      0.00      0.00        34\n",
      "Hypothetical       0.00      0.00      0.00        12\n",
      "      Recent       0.83      0.92      0.87      1093\n",
      "\n",
      "   micro avg       0.83      0.88      0.86      1139\n",
      "   macro avg       0.28      0.31      0.29      1139\n",
      "weighted avg       0.80      0.88      0.84      1139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/MedRoBERTa\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  28%|████████▌                      | 332/1207 [09:35<25:16,  1.73s/it]\n",
      "1it [46:07, 2767.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_62254/1570574117.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_print_class_report\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Train on all document sources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     trained_model, eval_loss_history = trainer.train_model(model=token_model.to(device), \n\u001b[0m\u001b[1;32m     29\u001b[0m                                                             \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                                             \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/koekiemonster/home/bramiozo/DEV/GIT/UPOD/negation-detection/utils/ner_training.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, tokenizer, train_dataset, eval_dataset, tag2id, device, args, max_grad_norm, amp)\u001b[0m\n\u001b[1;32m    450\u001b[0m              \u001b[0;31m# forward and backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m              \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m              \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m              \u001b[0;31m# track loss, mumber of examples and steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# cycle through folds\n",
    "scores = []\n",
    "predlist = []\n",
    "test_lists = []\n",
    "loss_history = {}\n",
    "for idx, fold in tqdm(enumerate(splits)):\n",
    "    # re-init model for each fold, otherwise it keeps on training the same throughout all folds..\n",
    "    token_model = AutoModelForTokenClassification.from_pretrained(args.model_path, num_labels = len(tag2id))\n",
    "    \n",
    "    train_list, test_list = fold['train'], fold['test']\n",
    "    \n",
    "    ## eval is optional (to gauge the best number of steps/epochs)\n",
    "    eval_list = random.choices(train_list,k=int(len(train_list)/10)) if args.do_eval else []\n",
    "    eval_dcc = dcc.loc[dcc.Id.isin(eval_list)]\n",
    "    test_dcc = dcc.loc[dcc.Id.isin(test_list)]\n",
    "    train_dcc = dcc.loc[(dcc.Id.isin(train_list)) & (~dcc.Id.isin(eval_list))]\n",
    "    \n",
    "    test_list = test_dcc.Id.tolist()\n",
    "    eval_list = eval_dcc.Id.tolist()\n",
    "\n",
    "    ###\n",
    "    train_dataset = trainer.TextDatasetFromDataFrame(train_dcc, tokenizer, args) \n",
    "    test_dataset = trainer.TextDatasetFromDataFrame(test_dcc, tokenizer, args)\n",
    "    eval_dataset = trainer.TextDatasetFromDataFrame(eval_dcc, tokenizer, args)\n",
    "    \n",
    "    args.do_print_class_report=False\n",
    "    # Train on all document sources\n",
    "    trained_model, eval_loss_history = trainer.train_model(model=token_model.to(device), \n",
    "                                                            tokenizer=tokenizer, \n",
    "                                                            train_dataset=train_dataset, \n",
    "                                                            eval_dataset=eval_dataset, \n",
    "                                                            tag2id=tag2id,\n",
    "                                                            device=device, \n",
    "                                                            args=args,\n",
    "                                                            max_grad_norm=1.0,\n",
    "                                                            amp=False)\n",
    "    args.do_print_class_report=True\n",
    "    # Evaluate on all document sources\n",
    "    f1, prec, rec, preds, truth, test_ids = trainer.eval_model(model=trained_model, \n",
    "                                       tokenizer=tokenizer, \n",
    "                                       eval_dataset=test_dataset, \n",
    "                                       tag2id=tag2id, \n",
    "                                       device=device, \n",
    "                                       args=args, \n",
    "                                       return_pred=True)\n",
    "    \n",
    "    loss_history[idx]=eval_loss_history\n",
    "    \n",
    "    #test_ids = [\"_\".join(t) for t in zip(test_dcc.Id, test_dcc.Begin, test_dcc.End)]\n",
    "    scores.append({'fold': idx, 'f1': f1, 'precision': prec, 'recall': rec})\n",
    "    predlist.append({'fold': idx, 'prediction': preds, 'truth': truth, 'ids': test_ids})\n",
    "    test_lists.append(test_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466041d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predlist_prep = []\n",
    "robbert_col_name = mod_name\n",
    "\n",
    "for foldnum, foldres in enumerate(predlist):\n",
    "    ids = foldres['ids']\n",
    "    for prs, trs, ids in zip(foldres['prediction'], foldres['truth'], foldres['ids']):\n",
    "        for pr, tr, _id in zip(prs, trs, ids):\n",
    "            tmp_dict={}\n",
    "            if len(pr)==len(tr)==0:\n",
    "                tmp_dict['fold'] = foldnum\n",
    "                tmp_dict['entity_id'] = _id\n",
    "                tmp_dict['label'] = \"n/a\"\n",
    "                tmp_dict[robbert_col_name] = \"n/a\"\n",
    "            elif len(pr)>0:\n",
    "                tmp_dict['fold'] = foldnum\n",
    "                tmp_dict['entity_id'] = _id\n",
    "                tmp_dict['label'] = tr\n",
    "                tmp_dict[robbert_col_name] = pr                \n",
    "            else:\n",
    "                raise ValueError(\"predictions are empty while truth is not\")    \n",
    "            predlist_prep.append(tmp_dict)\n",
    "predlist_df = pd.DataFrame(predlist_prep)\n",
    "predlist_df['bio_label'] = predlist_df['label'].str.replace(r\"([BI])\\-[A-z]+\", \"\\\\1\", \n",
    "                                                        regex=True, case=True).str.strip()\n",
    "predlist_df['bio_robbert'] = predlist_df[robbert_col_name].str.replace(r\"([BI])\\-[A-z]+\", \"\\\\1\", \n",
    "                                                        regex=True, case=True).str.strip()\n",
    "\n",
    "bio_pred = predlist_df[['entity_id', 'bio_label', 'bio_robbert']]\n",
    "predlist_df = predlist_df.loc[predlist_df.bio_label=='B']\n",
    "predlist_df.drop(['bio_label', 'bio_robbert', 'fold'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c94104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_map = {'B-NotNegated': 'not negated', 'B-Negated': 'negated'}\n",
    "temp_map = {'B-Recent': 'recent', 'B-Hypothetical': 'hypothetical', 'B-Historical': 'historical'}\n",
    "exp_map  = {'B-Patient': 'patient', 'B-Other': 'other'}\n",
    "\n",
    "if args.task == 'negation':\n",
    "    lab_map = neg_map\n",
    "elif args.task == 'temporality':\n",
    "    lab_map = temp_map\n",
    "elif args.experiencer == 'experiencer':\n",
    "    lab_map = exp_map\n",
    "    \n",
    "predlist_df['label'] = predlist_df.label.map(lab_map)\n",
    "predlist_df[robbert_col_name] = predlist_df[robbert_col_name].map(lab_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35165cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predlist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4f85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac68a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.do_eval:\n",
    "    dfl = []\n",
    "    for i in range(2):\n",
    "        df = pd.DataFrame(loss_history[i])\n",
    "        df['fold']=i\n",
    "        dfl.append(df)\n",
    "    eval_history = pd.concat(dfl).reset_index()\n",
    "    eval_history['step'] = eval_history['step'].astype(int)\n",
    "    eval_history['fold'] = eval_history['fold'].astype(int)\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=3, figsize=(18,5))\n",
    "    seaborn.lineplot(data=eval_history, x='step', y='f1', hue='epoch', ax=ax[0])\n",
    "    seaborn.lineplot(data=eval_history, x='step', y='recall', hue='epoch', ax=ax[1])\n",
    "    seaborn.lineplot(data=eval_history, x='step', y='precision', hue='epoch', ax=ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# micro-averaged scores\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909081f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B-NotNegated, B-Negated\n",
    "labmap = {'B-NotNegated': False, 'B-Negated': True, 'I-NotNegated': False, 'I-Negated': True}\n",
    "\n",
    "manual_scores = []\n",
    "confusion_matrix = []\n",
    "for i in range(len(predlist)):\n",
    "    # accuracy over all documents, flattened\n",
    "    _predlist = [(labmap[t], 'B' if 'B-' in t else 'I') for l in predlist[i]['prediction'] for t in l if len(l)>0]\n",
    "    _truthlist = [(labmap[t], 'B' if 'B-' in t else 'I') for l in predlist[i]['truth'] for t in l if len(l)>0]\n",
    "\n",
    "    tr_c = []\n",
    "    pr_c = []\n",
    "    tr_r = []\n",
    "    pr_r = []\n",
    "\n",
    "    b_truth = []\n",
    "    b_pred = []\n",
    "    for _t,_p in zip(_truthlist, _predlist):\n",
    "        if _t[1]==_p[1]=='B':\n",
    "            tr_c.append(_t[0])\n",
    "            pr_c.append(_p[0])\n",
    "        tr_r.append(_t[0])\n",
    "        pr_r.append(_p[0])\n",
    "\n",
    "        b_truth.append(_t[1]=='B')\n",
    "        b_pred.append(_p[1]=='B')\n",
    "\n",
    "    tr_c, pr_c, tr_r, pr_r = np.array(tr_c), np.array(pr_c), np.array(tr_r), np.array(pr_r)\n",
    "    b_truth, b_pred = np.array(b_truth), np.array(b_pred)\n",
    "\n",
    "    TN_c = np.sum((pr_c==tr_c) & (pr_c==False))\n",
    "    TP_c = np.sum((pr_c==tr_c) & (pr_c==True))\n",
    "    FP_c = np.sum((pr_c!=tr_c) & (pr_c==True))\n",
    "    FN_c = np.sum((pr_c!=tr_c) & (pr_c==False))\n",
    "\n",
    "    TN_r = np.sum((pr_r==tr_r) & (pr_r==False))\n",
    "    TP_r = np.sum((pr_r==tr_r) & (pr_r==True))\n",
    "    FP_r = np.sum((pr_r!=tr_r) & (pr_r==True))\n",
    "    FN_r = np.sum((pr_r!=tr_r) & (pr_r==False))\n",
    "\n",
    "    TN_b = np.sum((b_pred==b_truth) & (b_pred==False))\n",
    "    TP_b = np.sum((b_pred==b_truth) & (b_pred==True))\n",
    "    FP_b = np.sum((b_pred!=b_truth) & (b_pred==True))\n",
    "    FN_b = np.sum((b_pred!=b_truth) & (b_pred==False))\n",
    "\n",
    "\n",
    "    # micro\n",
    "    f1 = f1_score(tr_r, pr_r, average='micro')\n",
    "    precision = precision_score(tr_r, pr_r, average='micro')\n",
    "    recall = recall_score(tr_r, pr_r, average='micro')\n",
    "    manual_scores.append({'list': 'raw', \n",
    "                          'fold': i, \n",
    "                          'focus': 'micro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # macro\n",
    "    f1 = f1_score(tr_r, pr_r, average='macro')\n",
    "    precision = precision_score(tr_r, pr_r, average='macro')\n",
    "    recall = recall_score(tr_r, pr_r, average='macro')\n",
    "    manual_scores.append({'list': 'raw', \n",
    "                          'fold': i, \n",
    "                          'focus': 'macro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # Negated\n",
    "    f1 = f1_score(tr_r, pr_r)\n",
    "    precision = precision_score(tr_r, pr_r)\n",
    "    recall = recall_score(tr_r, pr_r)\n",
    "    manual_scores.append({'list': 'raw', \n",
    "                          'fold': i, \n",
    "                          'focus': 'negated', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # NotNegated\n",
    "    f1 = f1_score(~tr_r, ~pr_r)\n",
    "    precision = precision_score(~tr_r, ~pr_r)\n",
    "    recall = recall_score(~tr_r, ~pr_r)\n",
    "    manual_scores.append({'list': 'raw',\n",
    "                          'fold': i,\n",
    "                          'focus': 'notnegated', \n",
    "                          'f1': f1,\n",
    "                          'precision': precision,\n",
    "                          'recall': recall})\n",
    "    \n",
    "    confusion_matrix.append({'list': 'raw', 'fold': i, 'TN': TN_r, 'TP': TP_r, 'FN': FN_r, 'FP': FP_r})    \n",
    "    ######################################\n",
    "    # micro\n",
    "    f1 = f1_score(tr_c, pr_c, average='micro')\n",
    "    precision = precision_score(tr_c, pr_c, average='micro')\n",
    "    recall = recall_score(tr_c, pr_c, average='micro')\n",
    "    manual_scores.append({'list': 'clean', \n",
    "                          'fold': i, \n",
    "                          'focus': 'micro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # macro\n",
    "    f1 = f1_score(tr_c, pr_c, average='macro')\n",
    "    precision = precision_score(tr_c, pr_c, average='macro')\n",
    "    recall = recall_score(tr_c, pr_c, average='macro')\n",
    "    manual_scores.append({'list': 'clean', \n",
    "                          'fold': i, \n",
    "                          'focus': 'macro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # Negated\n",
    "    f1 = f1_score(tr_c, pr_c)\n",
    "    precision = precision_score(tr_c, pr_c)\n",
    "    recall = recall_score(tr_c, pr_c)\n",
    "    manual_scores.append({'list': 'clean', \n",
    "                          'fold': i, \n",
    "                          'focus': 'negated', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # NotNegated\n",
    "    f1 = f1_score(~tr_c, ~pr_c)\n",
    "    precision = precision_score(~tr_c, ~pr_c)\n",
    "    recall = recall_score(~tr_c, ~pr_c)\n",
    "    manual_scores.append({'list': 'clean',\n",
    "                          'fold': i,\n",
    "                          'focus': 'notnegated', \n",
    "                          'f1': f1,\n",
    "                          'precision': precision,\n",
    "                          'recall': recall})\n",
    "    \n",
    "    confusion_matrix.append({'list': 'clean', 'fold': i, 'TN': TN_c, 'TP': TP_c, 'FN': FN_c, 'FP': FP_c})    \n",
    "    ######################################\n",
    "    # micro\n",
    "    f1 = f1_score(b_truth, b_pred, average='micro')\n",
    "    precision = precision_score(b_truth, b_pred, average='micro')\n",
    "    recall = recall_score(b_truth, b_pred, average='micro')\n",
    "    manual_scores.append({'list': 'B_I', \n",
    "                          'fold': i, \n",
    "                          'focus': 'micro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # macro\n",
    "    f1 = f1_score(b_truth, b_pred, average='macro')\n",
    "    precision = precision_score(b_truth, b_pred, average='macro')\n",
    "    recall = recall_score(b_truth, b_pred, average='macro')\n",
    "    manual_scores.append({'list': 'B_I', \n",
    "                          'fold': i, \n",
    "                          'focus': 'macro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # Negated\n",
    "    f1 = f1_score(b_truth, b_pred)\n",
    "    precision = precision_score(b_truth, b_pred)\n",
    "    recall = recall_score(b_truth, b_pred)\n",
    "    manual_scores.append({'list': 'B_I', \n",
    "                          'fold': i, \n",
    "                          'focus': 'negated', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # NotNegated\n",
    "    f1 = f1_score(~b_truth, ~b_pred)\n",
    "    precision = precision_score(~b_truth, ~b_pred)\n",
    "    recall = recall_score(~b_truth, ~b_pred)\n",
    "    manual_scores.append({'list': 'B_I',\n",
    "                          'fold': i,\n",
    "                          'focus': 'notnegated', \n",
    "                          'f1': f1,\n",
    "                          'precision': precision,\n",
    "                          'recall': recall})\n",
    "    \n",
    "    confusion_matrix.append({'list': 'B_I', 'fold': i, 'TN': TN_b, 'TP': TP_b, 'FN': FN_b, 'FP': FP_b})    \n",
    "    \n",
    "manual_scores_df = pd.DataFrame(data=manual_scores)\n",
    "confusion_matrix_df = pd.DataFrame(data=confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe90a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_scores_df.groupby(['list', 'focus']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c16f81",
   "metadata": {},
   "source": [
    "## Append to other results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3909f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_result_file = '../results/merged_results.csv.gz'\n",
    "results = pd.read_csv(merged_result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d2b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = results.entity_id.str.replace(r\"\\_[0-9]+\\_[0-9]+\", \"\").unique()\n",
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d67edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check = predlist_df[['entity_id', 'label']].set_index('entity_id').join(results[['entity_id', 'label']].set_index('entity_id'),\n",
    "                                                                how='inner',rsuffix='_or')\n",
    "\n",
    "(sanity_check['label'] == sanity_check['label_or']).sum()==sanity_check.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd2d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predlist_df.set_index('entity_id', inplace=True)\n",
    "results.set_index('entity_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bef0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results = results.join(predlist_df[[robbert_col_name]], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results[['category', 'label', robbert_col_name]].to_csv(f\"../results/{mod_name}_predictions.csv.gz\", index=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2525293",
   "metadata": {},
   "outputs": [],
   "source": [
    "robberts = [c for c in total_results.columns if 'robbert' in c]\n",
    "unanimous = total_results.dropna()[['label', 'bilstm_cv', 'rule_based']+robberts].apply(lambda x: x[0]==x[1]==x[2]==x[3]==x[4], \n",
    "                                                                         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4026ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(unanimous)/total_results.loc[total_results[robbert_col_name].isna()==False].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704c4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_dissenters(x):\n",
    "    return int(x[0] != x[1])+\\\n",
    "           int(x[0] != x[2])+\\\n",
    "           int(x[0] != x[3])+\\\n",
    "           int(x[0] != x[4])+\\\n",
    "           int(x[0] != x[5])+\\\n",
    "           int(x[0] != x[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf8171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dissenters = total_results.dropna()[['label','bilstm', 'bilstm_cv','rule_based']+robberts]\\\n",
    "                            .apply(number_of_dissenters, axis=1)\n",
    "\n",
    "total_results.dissenters = np.nan\n",
    "total_results.loc[dissenters.index, 'dissenters'] = dissenters.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80*2*10/60 hours for 2 epochs and block size 512\n",
    "# 16*2*10/60 hours for 2 epochs and block size 128\n",
    "# 6*2*10/60 hours for 2 epochs and block size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ce5f3c",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69651ccd",
   "metadata": {},
   "source": [
    "## Inference in practice\n",
    "\n",
    "Suppose we have some sentence:\n",
    "```De patient vertoont tekenen van ischemische hartziekte. Op de MRI waren sporen van infarctie te zien. Ik concludeer dat hier sprake is van significante stenose```\n",
    "\n",
    "Now, how do we deploy our model?\n",
    "First we need to establish **what** we wish to label, for this we can use MedCAT trained on Dutch clinical texts to recognize medical concepts. For our purpose we may wish the constrain the possible medical concepts\n",
    "to symptoms and pathologies. Now, given our approximated medical concepts we apply our negation model to \n",
    "estimate whether the concept is negated or not.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
