{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5fa31f8",
   "metadata": {},
   "source": [
    "Starting from a \n",
    "* pre-trained model with a \n",
    "* pre-trained tokenizer\n",
    "\n",
    "we perform finetuning on a negation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02705f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import device, cuda, version\n",
    "\n",
    "import apex\n",
    "\n",
    "import dcc_splitter as splitter\n",
    "import ner_training as trainer\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from transformers import AutoTokenizer, RobertaTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f132a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2af5304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koekiemonster/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = device(\"cuda:0\") if cuda.is_available() else device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e2d04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fb57b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcc_dir = None\n",
    "output_dir = None\n",
    "skip_file = None\n",
    "n_splits = 10\n",
    "random_state = None\n",
    "base_folder = \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels\"\n",
    "output_folder = \"fine_tuned_token_classification\"\n",
    "mod_name = \"robbert-v2-dutch-base\" # \"robbert-v2-dutch-base\" # belabBERT_115k # bert-base-dutch\n",
    " \n",
    "\n",
    "args = namedtuple\n",
    "args.task = \"negation\" # experiencer, temporality\n",
    "args.model_path = os.path.join(base_folder, mod_name)\n",
    "args.model_type = \"roberta\" # bertje \n",
    "args.output_dir = os.path.join(base_folder, output_folder)\n",
    "args.num_epochs = 2\n",
    "args.eval_steps = 10 \n",
    "args.lr = 5e-5\n",
    "args.batch_size= 16\n",
    "args.gradient_accumulation_steps=1\n",
    "args.block_size = 32 # block size determines inclusion \n",
    "args.save_model=False\n",
    "args.bio=True\n",
    "args.do_eval=False\n",
    "args.do_write=False\n",
    "args.bootstrap=False\n",
    "args.do_print_class_report=False\n",
    "\n",
    "random.seed(77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb1c0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.block_size determines how many text snippets are  used for training, see ner_training.py lines 118--141\n",
    "# obviously this is a code-design flaw that should be mended.\n",
    "# the dataset loader should  include the id_begin_end in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bafdb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcc-splitter for folds\n",
    "dcc_splitter = splitter.DCCSplitter(dcc_dir, output_dir, skip_file, n_splits, random_state, write_to_file=False)\n",
    "splits = dcc_splitter.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bcb19ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load NER DCC set\n",
    "dcc = pd.read_csv(\"../data/RobBERT/DCC_df.csv\", \n",
    "                  sep=\"\\t\", \n",
    "                  skip_blank_lines=True, \n",
    "                  engine=\"python\", \n",
    "                  encoding=\"latin-1\",\n",
    "                  on_bad_lines=\"warn\", \n",
    "                  keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccc8b55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O             146395\n",
       "NotNegated     15713\n",
       "Negated         3017\n",
       "Name: Negation, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcc.Negation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d71ec0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11882"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcc.loc[dcc.BIO!='O'][['Id', 'Begin', 'End']].apply(lambda x: \"_\".join(x), axis=1).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3234908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Texts = dcc.groupby('Id').Word.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52a64c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_ids = {'negation':{'B-Negated':0,'B-NotNegated':1,'I-Negated':2,'I-NotNegated':3},\n",
    "          'temporality':{'B-Recent':0,'B-Historical':1,'B-Hypothetical':2,'I-Recent':3,\n",
    "                         'I-Historical':4,'I-Hypothetical':5},\n",
    "          'experiencer':{'B-Patient':0,'B-Other':1,'I-Patient':2,'I-Other':3}}\n",
    "\n",
    "tag2id = tag_ids[args.task]\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017aa7e5",
   "metadata": {},
   "source": [
    "## Over all document sources\n",
    "\n",
    "improvement: only output best model based on validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2865323a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 40000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [06:29<00:00,  1.29s/it]\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [06:37<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [14:11, 851.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.895 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.86      0.88      0.87       108\n",
      "  NotNegated       0.87      0.92      0.90       573\n",
      "\n",
      "   micro avg       0.87      0.92      0.89       681\n",
      "   macro avg       0.87      0.90      0.89       681\n",
      "weighted avg       0.87      0.92      0.89       681\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 40000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [06:41<00:00,  1.33s/it]\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [06:37<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [28:28, 854.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.906 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.88      0.93      0.90        83\n",
      "  NotNegated       0.90      0.92      0.91       555\n",
      "\n",
      "   micro avg       0.89      0.92      0.91       638\n",
      "   macro avg       0.89      0.92      0.90       638\n",
      "weighted avg       0.89      0.92      0.91       638\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 40000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [06:43<00:00,  1.34s/it]\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [05:53<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [42:05, 837.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.908 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.95      0.93      0.94       107\n",
      "  NotNegated       0.88      0.93      0.90       575\n",
      "\n",
      "   micro avg       0.89      0.93      0.91       682\n",
      "   macro avg       0.92      0.93      0.92       682\n",
      "weighted avg       0.89      0.93      0.91       682\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 40000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [05:49<00:00,  1.16s/it]\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [05:49<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [54:36, 803.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.894 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.86      0.84      0.85        90\n",
      "  NotNegated       0.88      0.92      0.90       569\n",
      "\n",
      "   micro avg       0.88      0.91      0.89       659\n",
      "   macro avg       0.87      0.88      0.88       659\n",
      "weighted avg       0.88      0.91      0.89       659\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 40000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [05:50<00:00,  1.16s/it]\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [05:43<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [1:07:03, 783.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.899 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.92      0.89      0.90        90\n",
      "  NotNegated       0.88      0.92      0.90       566\n",
      "\n",
      "   micro avg       0.88      0.92      0.90       656\n",
      "   macro avg       0.90      0.91      0.90       656\n",
      "weighted avg       0.88      0.92      0.90       656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 40000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [05:45<00:00,  1.14s/it]\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [05:48<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [1:19:31, 770.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.917 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.81      0.83      0.82        90\n",
      "  NotNegated       0.92      0.95      0.93       585\n",
      "\n",
      "   micro avg       0.90      0.93      0.92       675\n",
      "   macro avg       0.86      0.89      0.88       675\n",
      "weighted avg       0.90      0.93      0.92       675\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 40000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [05:50<00:00,  1.16s/it]\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [05:44<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [1:31:59, 763.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.915 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.89      0.92      0.90       101\n",
      "  NotNegated       0.91      0.93      0.92       583\n",
      "\n",
      "   micro avg       0.90      0.93      0.91       684\n",
      "   macro avg       0.90      0.92      0.91       684\n",
      "weighted avg       0.90      0.93      0.91       684\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 40000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [05:47<00:00,  1.15s/it]\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [05:42<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [1:44:22, 756.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.912 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.93      0.86      0.89       102\n",
      "  NotNegated       0.90      0.93      0.92       579\n",
      "\n",
      "   micro avg       0.90      0.92      0.91       681\n",
      "   macro avg       0.91      0.90      0.90       681\n",
      "weighted avg       0.90      0.92      0.91       681\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 40000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [05:45<00:00,  1.14s/it]\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [05:46<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [1:56:47, 753.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.904 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.87      0.86      0.87        88\n",
      "  NotNegated       0.90      0.92      0.91       594\n",
      "\n",
      "   micro avg       0.89      0.91      0.90       682\n",
      "   macro avg       0.88      0.89      0.89       682\n",
      "weighted avg       0.89      0.91      0.90       682\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"/media/koekiemonster/DATA-FAST/text_data/word_vectors_and_language_models/dutch/Medical/languagemodels/robbert-v2-dutch-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 40000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [05:43<00:00,  1.14s/it]\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [05:46<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, best model f = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [2:09:09, 774.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.885 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negated       0.88      0.94      0.91        98\n",
      "  NotNegated       0.87      0.89      0.88       546\n",
      "\n",
      "   micro avg       0.87      0.90      0.89       644\n",
      "   macro avg       0.88      0.91      0.90       644\n",
      "weighted avg       0.87      0.90      0.89       644\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# cycle through folds\n",
    "scores = []\n",
    "predlist = []\n",
    "test_lists = []\n",
    "loss_history = {}\n",
    "for idx, fold in tqdm(enumerate(splits)):\n",
    "    # re-init model for each fold, otherwise it keeps on training the same throughout all folds..\n",
    "    token_model = AutoModelForTokenClassification.from_pretrained(args.model_path, num_labels = len(tag2id))\n",
    "    \n",
    "    train_list, test_list = fold['train'], fold['test']\n",
    "    \n",
    "    ## eval is optional (to gauge the best number of steps/epochs)\n",
    "    eval_list = random.choices(train_list,k=int(len(train_list)/10)) if args.do_eval else []\n",
    "    eval_dcc = dcc.loc[dcc.Id.isin(eval_list)]\n",
    "    test_dcc = dcc.loc[dcc.Id.isin(test_list)]\n",
    "    train_dcc = dcc.loc[(dcc.Id.isin(train_list)) & (~dcc.Id.isin(eval_list))]\n",
    "    \n",
    "    test_list = test_dcc.Id.tolist()\n",
    "    eval_list = eval_dcc.Id.tolist()\n",
    "\n",
    "    ###\n",
    "    train_dataset = trainer.TextDatasetFromDataFrame(train_dcc, tokenizer, args) \n",
    "    test_dataset = trainer.TextDatasetFromDataFrame(test_dcc, tokenizer, args)\n",
    "    eval_dataset = trainer.TextDatasetFromDataFrame(eval_dcc, tokenizer, args)\n",
    "    \n",
    "    args.do_print_class_report=False\n",
    "    # Train on all document sources\n",
    "    trained_model, eval_loss_history = trainer.train_model(model=token_model.to(device), \n",
    "                                                            tokenizer=tokenizer, \n",
    "                                                            train_dataset=train_dataset, \n",
    "                                                            eval_dataset=eval_dataset, \n",
    "                                                            tag2id=tag2id,\n",
    "                                                            device=device, \n",
    "                                                            args=args,\n",
    "                                                            max_grad_norm=1.0,\n",
    "                                                            amp=False)\n",
    "    args.do_print_class_report=True\n",
    "    # Evaluate on all document sources\n",
    "    f1, prec, rec, preds, truth, test_ids = trainer.eval_model(model=trained_model, \n",
    "                                       tokenizer=tokenizer, \n",
    "                                       eval_dataset=test_dataset, \n",
    "                                       tag2id=tag2id, \n",
    "                                       device=device, \n",
    "                                       args=args, \n",
    "                                       return_pred=True)\n",
    "    \n",
    "    loss_history[idx]=eval_loss_history\n",
    "    \n",
    "    #test_ids = [\"_\".join(t) for t in zip(test_dcc.Id, test_dcc.Begin, test_dcc.End)]\n",
    "    scores.append({'fold': idx, 'f1': f1, 'precision': prec, 'recall': rec})\n",
    "    predlist.append({'fold': idx, 'prediction': preds, 'truth': truth, 'ids': test_ids})\n",
    "    test_lists.append(test_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "466041d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predlist_prep = []\n",
    "robbert_col_name = 'robbert_'+str(args.block_size)+'_'+str(args.num_epochs)\n",
    "\n",
    "for foldnum, foldres in enumerate(predlist):\n",
    "    ids = foldres['ids']\n",
    "    for prs, trs, ids in zip(foldres['prediction'], foldres['truth'], foldres['ids']):\n",
    "        for pr, tr, _id in zip(prs, trs, ids):\n",
    "            tmp_dict={}\n",
    "            if len(pr)==len(tr)==0:\n",
    "                tmp_dict['fold'] = foldnum\n",
    "                tmp_dict['entity_id'] = _id\n",
    "                tmp_dict['label'] = \"n/a\"\n",
    "                tmp_dict[robbert_col_name] = \"n/a\"\n",
    "            elif len(pr)>0:\n",
    "                tmp_dict['fold'] = foldnum\n",
    "                tmp_dict['entity_id'] = _id\n",
    "                tmp_dict['label'] = tr\n",
    "                tmp_dict[robbert_col_name] = pr                \n",
    "            else:\n",
    "                raise ValueError(\"predictions are empty while truth is not\")    \n",
    "            predlist_prep.append(tmp_dict)\n",
    "predlist_df = pd.DataFrame(predlist_prep)\n",
    "predlist_df['bio_label'] = predlist_df['label'].str.replace(r\"([BI])\\-[A-z]+\", \"\\\\1\", \n",
    "                                                        regex=True, case=True).str.strip()\n",
    "predlist_df['bio_robbert'] = predlist_df[robbert_col_name].str.replace(r\"([BI])\\-[A-z]+\", \"\\\\1\", \n",
    "                                                        regex=True, case=True).str.strip()\n",
    "\n",
    "bio_pred = predlist_df[['entity_id', 'bio_label', 'bio_robbert']]\n",
    "predlist_df = predlist_df.loc[predlist_df.bio_label=='B']\n",
    "predlist_df.drop(['bio_label', 'bio_robbert', 'fold'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c94104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predlist_df['label'] = predlist_df.label.map({'B-NotNegated': 'not negated', 'B-Negated': 'negated'})\n",
    "predlist_df[robbert_col_name] = predlist_df[robbert_col_name].map({'B-NotNegated': 'not negated', 'B-Negated': 'negated'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35165cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>label</th>\n",
       "      <th>robbert_32_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DL1667_66_74</td>\n",
       "      <td>not negated</td>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DL1866_88_99</td>\n",
       "      <td>not negated</td>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DL1614_5_12</td>\n",
       "      <td>negated</td>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DL1614_41_51</td>\n",
       "      <td>negated</td>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DL1614_62_71</td>\n",
       "      <td>negated</td>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10951</th>\n",
       "      <td>SP1751_18_33</td>\n",
       "      <td>not negated</td>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952</th>\n",
       "      <td>SP1751_60_78</td>\n",
       "      <td>not negated</td>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10954</th>\n",
       "      <td>SP2037_49_73</td>\n",
       "      <td>negated</td>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10957</th>\n",
       "      <td>SP1321_23_30</td>\n",
       "      <td>negated</td>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10959</th>\n",
       "      <td>SP1838_85_92</td>\n",
       "      <td>not negated</td>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6603 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          entity_id        label robbert_32_2\n",
       "0      DL1667_66_74  not negated  not negated\n",
       "1      DL1866_88_99  not negated  not negated\n",
       "2       DL1614_5_12      negated      negated\n",
       "4      DL1614_41_51      negated      negated\n",
       "6      DL1614_62_71      negated      negated\n",
       "...             ...          ...          ...\n",
       "10951  SP1751_18_33  not negated  not negated\n",
       "10952  SP1751_60_78  not negated  not negated\n",
       "10954  SP2037_49_73      negated      negated\n",
       "10957  SP1321_23_30      negated      negated\n",
       "10959  SP1838_85_92  not negated  not negated\n",
       "\n",
       "[6603 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predlist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff4f85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ac68a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.do_eval:\n",
    "    dfl = []\n",
    "    for i in range(2):\n",
    "        df = pd.DataFrame(loss_history[i])\n",
    "        df['fold']=i\n",
    "        dfl.append(df)\n",
    "    eval_history = pd.concat(dfl).reset_index()\n",
    "    eval_history['step'] = eval_history['step'].astype(int)\n",
    "    eval_history['fold'] = eval_history['fold'].astype(int)\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=3, figsize=(18,5))\n",
    "    seaborn.lineplot(data=eval_history, x='step', y='f1', hue='epoch', ax=ax[0])\n",
    "    seaborn.lineplot(data=eval_history, x='step', y='recall', hue='epoch', ax=ax[1])\n",
    "    seaborn.lineplot(data=eval_history, x='step', y='precision', hue='epoch', ax=ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85b6d764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.894775</td>\n",
       "      <td>0.872905</td>\n",
       "      <td>0.917768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.905573</td>\n",
       "      <td>0.894495</td>\n",
       "      <td>0.916928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.908178</td>\n",
       "      <td>0.889045</td>\n",
       "      <td>0.928152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.894188</td>\n",
       "      <td>0.878477</td>\n",
       "      <td>0.910470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.899178</td>\n",
       "      <td>0.881406</td>\n",
       "      <td>0.917683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.917458</td>\n",
       "      <td>0.904899</td>\n",
       "      <td>0.930370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.914740</td>\n",
       "      <td>0.904286</td>\n",
       "      <td>0.925439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.912255</td>\n",
       "      <td>0.901146</td>\n",
       "      <td>0.923642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.903693</td>\n",
       "      <td>0.892704</td>\n",
       "      <td>0.914956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.885145</td>\n",
       "      <td>0.873112</td>\n",
       "      <td>0.897516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fold        f1  precision    recall\n",
       "0     0  0.894775   0.872905  0.917768\n",
       "1     1  0.905573   0.894495  0.916928\n",
       "2     2  0.908178   0.889045  0.928152\n",
       "3     3  0.894188   0.878477  0.910470\n",
       "4     4  0.899178   0.881406  0.917683\n",
       "5     5  0.917458   0.904899  0.930370\n",
       "6     6  0.914740   0.904286  0.925439\n",
       "7     7  0.912255   0.901146  0.923642\n",
       "8     8  0.903693   0.892704  0.914956\n",
       "9     9  0.885145   0.873112  0.897516"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# micro-averaged scores\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fabec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "909081f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B-NotNegated, B-Negated\n",
    "labmap = {'B-NotNegated': False, 'B-Negated': True, 'I-NotNegated': False, 'I-Negated': True}\n",
    "\n",
    "manual_scores = []\n",
    "confusion_matrix = []\n",
    "for i in range(len(predlist)):\n",
    "    # accuracy over all documents, flattened\n",
    "    _predlist = [(labmap[t], 'B' if 'B-' in t else 'I') for l in predlist[i]['prediction'] for t in l if len(l)>0]\n",
    "    _truthlist = [(labmap[t], 'B' if 'B-' in t else 'I') for l in predlist[i]['truth'] for t in l if len(l)>0]\n",
    "\n",
    "    tr_c = []\n",
    "    pr_c = []\n",
    "    tr_r = []\n",
    "    pr_r = []\n",
    "\n",
    "    b_truth = []\n",
    "    b_pred = []\n",
    "    for _t,_p in zip(_truthlist, _predlist):\n",
    "        if _t[1]==_p[1]=='B':\n",
    "            tr_c.append(_t[0])\n",
    "            pr_c.append(_p[0])\n",
    "        tr_r.append(_t[0])\n",
    "        pr_r.append(_p[0])\n",
    "\n",
    "        b_truth.append(_t[1]=='B')\n",
    "        b_pred.append(_p[1]=='B')\n",
    "\n",
    "    tr_c, pr_c, tr_r, pr_r = np.array(tr_c), np.array(pr_c), np.array(tr_r), np.array(pr_r)\n",
    "    b_truth, b_pred = np.array(b_truth), np.array(b_pred)\n",
    "\n",
    "    TN_c = np.sum((pr_c==tr_c) & (pr_c==False))\n",
    "    TP_c = np.sum((pr_c==tr_c) & (pr_c==True))\n",
    "    FP_c = np.sum((pr_c!=tr_c) & (pr_c==True))\n",
    "    FN_c = np.sum((pr_c!=tr_c) & (pr_c==False))\n",
    "\n",
    "    TN_r = np.sum((pr_r==tr_r) & (pr_r==False))\n",
    "    TP_r = np.sum((pr_r==tr_r) & (pr_r==True))\n",
    "    FP_r = np.sum((pr_r!=tr_r) & (pr_r==True))\n",
    "    FN_r = np.sum((pr_r!=tr_r) & (pr_r==False))\n",
    "\n",
    "    TN_b = np.sum((b_pred==b_truth) & (b_pred==False))\n",
    "    TP_b = np.sum((b_pred==b_truth) & (b_pred==True))\n",
    "    FP_b = np.sum((b_pred!=b_truth) & (b_pred==True))\n",
    "    FN_b = np.sum((b_pred!=b_truth) & (b_pred==False))\n",
    "\n",
    "\n",
    "    # micro\n",
    "    f1 = f1_score(tr_r, pr_r, average='micro')\n",
    "    precision = precision_score(tr_r, pr_r, average='micro')\n",
    "    recall = recall_score(tr_r, pr_r, average='micro')\n",
    "    manual_scores.append({'list': 'raw', \n",
    "                          'fold': i, \n",
    "                          'focus': 'micro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # macro\n",
    "    f1 = f1_score(tr_r, pr_r, average='macro')\n",
    "    precision = precision_score(tr_r, pr_r, average='macro')\n",
    "    recall = recall_score(tr_r, pr_r, average='macro')\n",
    "    manual_scores.append({'list': 'raw', \n",
    "                          'fold': i, \n",
    "                          'focus': 'macro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # Negated\n",
    "    f1 = f1_score(tr_r, pr_r)\n",
    "    precision = precision_score(tr_r, pr_r)\n",
    "    recall = recall_score(tr_r, pr_r)\n",
    "    manual_scores.append({'list': 'raw', \n",
    "                          'fold': i, \n",
    "                          'focus': 'negated', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # NotNegated\n",
    "    f1 = f1_score(~tr_r, ~pr_r)\n",
    "    precision = precision_score(~tr_r, ~pr_r)\n",
    "    recall = recall_score(~tr_r, ~pr_r)\n",
    "    manual_scores.append({'list': 'raw',\n",
    "                          'fold': i,\n",
    "                          'focus': 'notnegated', \n",
    "                          'f1': f1,\n",
    "                          'precision': precision,\n",
    "                          'recall': recall})\n",
    "    \n",
    "    confusion_matrix.append({'list': 'raw', 'fold': i, 'TN': TN_r, 'TP': TP_r, 'FN': FN_r, 'FP': FP_r})    \n",
    "    ######################################\n",
    "    # micro\n",
    "    f1 = f1_score(tr_c, pr_c, average='micro')\n",
    "    precision = precision_score(tr_c, pr_c, average='micro')\n",
    "    recall = recall_score(tr_c, pr_c, average='micro')\n",
    "    manual_scores.append({'list': 'clean', \n",
    "                          'fold': i, \n",
    "                          'focus': 'micro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # macro\n",
    "    f1 = f1_score(tr_c, pr_c, average='macro')\n",
    "    precision = precision_score(tr_c, pr_c, average='macro')\n",
    "    recall = recall_score(tr_c, pr_c, average='macro')\n",
    "    manual_scores.append({'list': 'clean', \n",
    "                          'fold': i, \n",
    "                          'focus': 'macro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # Negated\n",
    "    f1 = f1_score(tr_c, pr_c)\n",
    "    precision = precision_score(tr_c, pr_c)\n",
    "    recall = recall_score(tr_c, pr_c)\n",
    "    manual_scores.append({'list': 'clean', \n",
    "                          'fold': i, \n",
    "                          'focus': 'negated', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # NotNegated\n",
    "    f1 = f1_score(~tr_c, ~pr_c)\n",
    "    precision = precision_score(~tr_c, ~pr_c)\n",
    "    recall = recall_score(~tr_c, ~pr_c)\n",
    "    manual_scores.append({'list': 'clean',\n",
    "                          'fold': i,\n",
    "                          'focus': 'notnegated', \n",
    "                          'f1': f1,\n",
    "                          'precision': precision,\n",
    "                          'recall': recall})\n",
    "    \n",
    "    confusion_matrix.append({'list': 'clean', 'fold': i, 'TN': TN_c, 'TP': TP_c, 'FN': FN_c, 'FP': FP_c})    \n",
    "    ######################################\n",
    "    # micro\n",
    "    f1 = f1_score(b_truth, b_pred, average='micro')\n",
    "    precision = precision_score(b_truth, b_pred, average='micro')\n",
    "    recall = recall_score(b_truth, b_pred, average='micro')\n",
    "    manual_scores.append({'list': 'B_I', \n",
    "                          'fold': i, \n",
    "                          'focus': 'micro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # macro\n",
    "    f1 = f1_score(b_truth, b_pred, average='macro')\n",
    "    precision = precision_score(b_truth, b_pred, average='macro')\n",
    "    recall = recall_score(b_truth, b_pred, average='macro')\n",
    "    manual_scores.append({'list': 'B_I', \n",
    "                          'fold': i, \n",
    "                          'focus': 'macro', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # Negated\n",
    "    f1 = f1_score(b_truth, b_pred)\n",
    "    precision = precision_score(b_truth, b_pred)\n",
    "    recall = recall_score(b_truth, b_pred)\n",
    "    manual_scores.append({'list': 'B_I', \n",
    "                          'fold': i, \n",
    "                          'focus': 'negated', \n",
    "                          'f1': f1, \n",
    "                          'precision': precision, \n",
    "                          'recall': recall})\n",
    "\n",
    "    # NotNegated\n",
    "    f1 = f1_score(~b_truth, ~b_pred)\n",
    "    precision = precision_score(~b_truth, ~b_pred)\n",
    "    recall = recall_score(~b_truth, ~b_pred)\n",
    "    manual_scores.append({'list': 'B_I',\n",
    "                          'fold': i,\n",
    "                          'focus': 'notnegated', \n",
    "                          'f1': f1,\n",
    "                          'precision': precision,\n",
    "                          'recall': recall})\n",
    "    \n",
    "    confusion_matrix.append({'list': 'B_I', 'fold': i, 'TN': TN_b, 'TP': TP_b, 'FN': FN_b, 'FP': FP_b})    \n",
    "    \n",
    "manual_scores_df = pd.DataFrame(data=manual_scores)\n",
    "confusion_matrix_df = pd.DataFrame(data=confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fe90a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>list</th>\n",
       "      <th>focus</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">B_I</th>\n",
       "      <th>macro</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.950734</td>\n",
       "      <td>0.951559</td>\n",
       "      <td>0.950061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.952914</td>\n",
       "      <td>0.952914</td>\n",
       "      <td>0.952914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negated</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.961062</td>\n",
       "      <td>0.957913</td>\n",
       "      <td>0.964303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notnegated</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.940406</td>\n",
       "      <td>0.945204</td>\n",
       "      <td>0.935819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">clean</th>\n",
       "      <th>macro</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.963929</td>\n",
       "      <td>0.971918</td>\n",
       "      <td>0.956746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.982372</td>\n",
       "      <td>0.982372</td>\n",
       "      <td>0.982372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negated</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.938151</td>\n",
       "      <td>0.957107</td>\n",
       "      <td>0.920764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notnegated</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.989706</td>\n",
       "      <td>0.986728</td>\n",
       "      <td>0.992728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">raw</th>\n",
       "      <th>macro</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.964603</td>\n",
       "      <td>0.969298</td>\n",
       "      <td>0.960343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.982280</td>\n",
       "      <td>0.982280</td>\n",
       "      <td>0.982280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negated</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.939604</td>\n",
       "      <td>0.950806</td>\n",
       "      <td>0.929248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notnegated</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.989601</td>\n",
       "      <td>0.987790</td>\n",
       "      <td>0.991438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  fold        f1  precision    recall\n",
       "list  focus                                          \n",
       "B_I   macro        4.5  0.950734   0.951559  0.950061\n",
       "      micro        4.5  0.952914   0.952914  0.952914\n",
       "      negated      4.5  0.961062   0.957913  0.964303\n",
       "      notnegated   4.5  0.940406   0.945204  0.935819\n",
       "clean macro        4.5  0.963929   0.971918  0.956746\n",
       "      micro        4.5  0.982372   0.982372  0.982372\n",
       "      negated      4.5  0.938151   0.957107  0.920764\n",
       "      notnegated   4.5  0.989706   0.986728  0.992728\n",
       "raw   macro        4.5  0.964603   0.969298  0.960343\n",
       "      micro        4.5  0.982280   0.982280  0.982280\n",
       "      negated      4.5  0.939604   0.950806  0.929248\n",
       "      notnegated   4.5  0.989601   0.987790  0.991438"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_scores_df.groupby(['list', 'focus']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c16f81",
   "metadata": {},
   "source": [
    "## Append to other results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3909f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_result_file = '../results/merged_results_new.csv.gz'\n",
    "results = pd.read_csv(merged_result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0d2b801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_161216/2061256545.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  entities = results.entity_id.str.replace(r\"\\_[0-9]+\\_[0-9]+\", \"\").unique()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5365"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = results.entity_id.str.replace(r\"\\_[0-9]+\\_[0-9]+\", \"\").unique()\n",
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d67edca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_check = predlist_df[['entity_id', 'label']].set_index('entity_id').join(results[['entity_id', 'label']].set_index('entity_id'),\n",
    "                                                                how='inner',rsuffix='_or')\n",
    "\n",
    "(sanity_check['label'] == sanity_check['label_or']).sum()==sanity_check.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cdd2d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predlist_df.set_index('entity_id', inplace=True)\n",
    "results.set_index('entity_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bb825cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>robbert_32_2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entity_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DL1667_66_74</th>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DL1866_88_99</th>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DL1614_5_12</th>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DL1614_41_51</th>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DL1614_62_71</th>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP1751_18_33</th>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP1751_60_78</th>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP2037_49_73</th>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP1321_23_30</th>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP1838_85_92</th>\n",
       "      <td>not negated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6603 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             robbert_32_2\n",
       "entity_id                \n",
       "DL1667_66_74  not negated\n",
       "DL1866_88_99  not negated\n",
       "DL1614_5_12       negated\n",
       "DL1614_41_51      negated\n",
       "DL1614_62_71      negated\n",
       "...                   ...\n",
       "SP1751_18_33  not negated\n",
       "SP1751_60_78  not negated\n",
       "SP2037_49_73      negated\n",
       "SP1321_23_30      negated\n",
       "SP1838_85_92  not negated\n",
       "\n",
       "[6603 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predlist_df[[robbert_col_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2bef0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results = results.join(predlist_df[[robbert_col_name]], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf9a8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results.to_csv(\"../results/merged_results_new.csv.gz\", index=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2525293",
   "metadata": {},
   "outputs": [],
   "source": [
    "robberts = [c for c in total_results.columns if 'robbert' in c]\n",
    "unanimous = total_results.dropna()[['label', 'bilstm_cv', 'rule_based']+robberts].apply(lambda x: x[0]==x[1]==x[2]==x[3]==x[4], \n",
    "                                                                         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf4026ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9362907031618688"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(unanimous)/total_results.loc[total_results[robbert_col_name].isna()==False].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "704c4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_dissenters(x):\n",
    "    return int(x[0] != x[1])+\\\n",
    "           int(x[0] != x[2])+\\\n",
    "           int(x[0] != x[3])+\\\n",
    "           int(x[0] != x[4])+\\\n",
    "           int(x[0] != x[5])+\\\n",
    "           int(x[0] != x[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bdf8171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dissenters = total_results.dropna()[['label','bilstm', 'bilstm_cv','rule_based']+robberts]\\\n",
    "                            .apply(number_of_dissenters, axis=1)\n",
    "\n",
    "total_results.dissenters = np.nan\n",
    "total_results.loc[dissenters.index, 'dissenters'] = dissenters.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b51e6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80*2*10/60 hours for 2 epochs and block size 512\n",
    "# 16*2*10/60 hours for 2 epochs and block size 128\n",
    "# 6*2*10/60 hours for 2 epochs and block size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4afe09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
