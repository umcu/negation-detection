{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset (DCC) with a shortage of certain labels. We want to generate \n",
    "new samples synthetically using GPT-4. We will use the following approach:\n",
    "1. We take the existing samples for each document type and present these to GPT-4\n",
    "2. we ask to generate new sentences like it, where the token labels are provided in the BIO format\n",
    "\n",
    "We care specifically about the following labels:\n",
    "* Experiencer: Other\n",
    "* Historical: Hypothetical\n",
    "\n",
    "The task of the GPT model is to generate new sentences that are similar to the input sentences but with variations of the medical concepts. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions\n",
    "The definitions are taken from the ConText/ConTextD papers:\n",
    "\n",
    "## Negation\n",
    "\n",
    "This property has two values, ‘Negated’ or ‘Not negated’. A clinical condition or term is labeled as ‘Negated’ if there is evidence in the text suggesting that the condition does not occur or exist, e.g., ‘There was no sign of sinus infection’, otherwise it is ‘Not negated’.\n",
    "\n",
    "## Temporality\n",
    "\n",
    "The temporality property places a condition along a time line. There are three possible values for this property: ‘Recent’, ‘Historical’, and ‘Hypothetical’. A condition is considered ‘Recent’ if it is maximally 2 weeks old. Conditions that developed more than 2 weeks ago are labeled as ‘Historical’. A condition is labeled as ‘Hypothetical’ if it is not ‘Recent’ or ‘Historical’, e.g., ‘patient should return if she develops fever’ [13].\n",
    "\n",
    "**Adaptation**: *'Hypothetical' is specifically about (theoretical) concepts, concepts that are not (yet) realized, i.e. concepts that may materialize in the future. 'Historical' and 'Recent' can be used for realized concepts, in which we also include their negations. I.e. if a concept is explicitly denied historically or recently, we can label it as 'Historical' or 'Recent' respectively.*\n",
    "\n",
    "## Experiencer\n",
    "\n",
    "Clinical text may refer to subjects other than the actual patient. The experiencer property describes whether the patient experienced the condition or someone else. For simplicity, we have defined only two possible values for this property: ‘Patient’ or ‘Other’, where ‘Other’ refers to anyone but the actual patient, e.g., ‘Mother is recently diagnosed with cancer’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, re\n",
    "import json, dotenv\n",
    "import pprint\n",
    "\n",
    "import openai\n",
    "import asyncio\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from utils import preprocess_dcc_for_robbert\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_temporality = True\n",
    "run_experiencer = True\n",
    "#run_historical = True\n",
    "experiencer_file = '../data/synth_experiencer_gpt_4_1106_preview.parquet'\n",
    "hypothetical_file = '../data/synth_hypothetical_gpt_4_1106_preview.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCC = json.load(open('../data/emc-dcc_ann.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_dcc = DCC.copy()\n",
    "for c in update_dcc['projects'][0]['documents']:\n",
    "    c['source'] = 'EMC_DCC_ORIGINAL'\n",
    "with open('../data/emc-dcc_ann_ORIGNAL.json', 'w') as f:\n",
    "    json.dump(update_dcc, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = update_dcc['projects'][0]['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = {'Negation': defaultdict(int),\n",
    "                'Temporality': defaultdict(int),\n",
    "                'Experiencer': defaultdict(int)}\n",
    "\n",
    "for doc in docs:\n",
    "    for ann in doc['annotations']:\n",
    "        for _class, val in ann['meta_anns'].items():\n",
    "            class_counts[_class][val['value']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'Experiencer': defaultdict(<class 'int'>, {'patient': 12451, 'other': 100}),\n",
      "  'Negation': defaultdict(<class 'int'>,\n",
      "                          { 'negated': 1760,\n",
      "                            'not negated': 10791}),\n",
      "  'Temporality': defaultdict(<class 'int'>,\n",
      "                             { 'historical': 511,\n",
      "                               'hypothetical': 109,\n",
      "                               'recent': 11931})}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(class_counts, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(relevant_docs_hypothetical), len(relevant_docs_experiencer)\n",
    "#relevant_docs_hypothetical[0]['text'][0:110]\n",
    "#[(d['start'],d['end'], d['id']) for d in relevant_docs_hypothetical[0]['annotations'] \n",
    "#        if d['meta_anns']['Temporality']['value']=='hypothetical']\n",
    "#relevant_docs_hypothetical[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = [\n",
    "    {'doc_id': 'DL1616', 'annotation_id': 1873, 'meta': 'Experiencer', 'value': 'patient'},\n",
    "    {'doc_id': 'DL1139', 'annotation_id': 108, 'meta': 'Experiencer', 'value': 'patient'},\n",
    "    {'doc_id': 'GP2799', 'annotation_id': 8210, 'meta': 'Experiencer', 'value': 'patient'},\n",
    "    {'doc_id': 'SP1476', 'annotation_id': 15532, 'meta': 'Experiencer', 'value': 'patient'},\n",
    "    {'doc_id': 'DL1567', 'annotation_id': 1694, 'meta': 'Temporality', 'value': 'historical'},\n",
    "    {'doc_id': 'DL1711', 'annotation_id': 2232, 'meta': 'Temporality', 'value': 'recent'},\n",
    "    {'doc_id': 'DL1812', 'annotation_id': 2232, 'meta': 'Temporality', 'value': 'historical'},\n",
    "    {'doc_id': 'DL1814', 'annotation_id': 2703, 'meta': 'Temporality', 'value': 'historical'},\n",
    "    {'doc_id': 'GP1395', 'annotation_id': 4538, 'meta': 'Temporality', 'value': 'recent'},\n",
    "    {'doc_id': 'DL2111', 'annotation_id': 3779, 'meta': 'Temporality', 'value': 'historical'},\n",
    "    {'doc_id': 'DL2100', 'annotation_id': 3716, 'meta': 'Temporality', 'value': 'historical'},  \n",
    "    {'doc_id': 'DL2100', 'annotation_id': 3716, 'meta': 'Temporality', 'value': 'historical'},  \n",
    "    {'doc_id': 'DL2072', 'annotation_id': 3627, 'meta': 'Temporality', 'value': 'historical'},    \n",
    "    {'doc_id': 'DL2067', 'annotation_id': 3606, 'meta': 'Temporality', 'value': 'historical'},\n",
    "    {'doc_id': 'DL1931', 'annotation_id': 3113, 'meta': 'Temporality', 'value': 'historical'},\n",
    "    {'doc_id': 'DL1812', 'annotation_id': 2689, 'meta': 'Temporality', 'value': 'historical'},\n",
    "    {'doc_id': 'DL1507', 'annotation_id': 1467, 'meta': 'Temporality', 'value': 'historical'},\n",
    "    {'doc_id': 'DL1167', 'annotation_id': 212, 'meta': 'Temporality', 'value': 'historical'},\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the docs\n",
    "update_docs = docs.copy()\n",
    "for c in corrections:\n",
    "    for d in update_docs:        \n",
    "        if d['name']==c['doc_id']:\n",
    "            d['source'] = 'EMC_DCC_ORIGINAL_ADJUSTED'\n",
    "            for a in d['annotations']:\n",
    "                if a['id']==c['annotation_id']:\n",
    "                    a['meta_anns'][c['meta']]['value'] = c['value']\n",
    "# put updated docs in DCC\n",
    "DCC['projects'][0]['documents'] = update_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write DCC back to json \n",
    "with open('../data/emc-dcc_ann_ADJ.json', 'w') as f:\n",
    "    json.dump(DCC, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs_hypothetical = []\n",
    "for i, doc in enumerate(docs):\n",
    "    for concept in doc['annotations']:\n",
    "        if (concept['meta_anns']['Temporality']['value']=='hypothetical'):\n",
    "            doc['index'] = i\n",
    "            relevant_docs_hypothetical.append(doc)\n",
    "            break\n",
    "        \n",
    "relevant_docs_experiencer = []\n",
    "for i, doc in enumerate(docs):\n",
    "    for concept in doc['annotations']:\n",
    "        if (concept['meta_anns']['Experiencer']['value']=='other'):\n",
    "            doc['index'] = i\n",
    "            relevant_docs_experiencer.append(doc)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OAI_ASYNC_CLIENT = AsyncOpenAI(api_key=os.getenv(\"OPENAI_KEY\"), max_retries=2)\n",
    "OAI_CLIENT = OpenAI(api_key=os.getenv(\"OPENAI_KEY\"), max_retries=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_HYPOTHETICAL = \"\"\"\n",
    "    Je bent een kritische assistent die mij helpt om nieuwe tekst te bedenken.\n",
    "    De tekst moeten voldoen aan de volgende eisen:\n",
    "    - ze moeten semantisch correct zijn en vergelijkbaar zijn met de voorbeeltekst die ik je geef.\n",
    "    - de voorbeeltekst wordt voorafgegaan door de term VOORBEELDTEKST\n",
    "    - in de voorbeeldzin worden 1 of meer concepten benoemd die hypothethisch zijn, het is belangrijk\n",
    "    dat deze concepten in de nieuwe zin ook hypothetisch zijn, het mogen ook andere concepten zijn. \n",
    "    Een voorbeeld van een hypothetische concept = 'een voorafgaand trauma kan niet worden herinnerd', waarin 'trauma' het concept is.\n",
    "    Een ander voorbeeld = 'ter uitsluiting van epifysaire dysplasie' waarin 'epifysaire dysplasie' het concept is.\n",
    "    - de concepten die je moet vervangen zijn aangegeven met verticale streepjes, dus |concept|.\n",
    "    - het domein is medisch dus gebruik medische concepten.\n",
    "    - probeer de medische concepten te varieren, dus gebruik niet steeds dezelfde concepten.\n",
    "    - geef als antwoord ALLEEN de nieuw gegenereerde zinnen, voorafgaand met de term NIEUWE_TEKST\n",
    "    - in de NIEUWE_TEKST, plaats de concepten die hypothetisch zijn tussen verticale streepjes, dus '|', \n",
    "    dus bijvoorbeeld: 'ter uitsluiting van |epifysaire dysplasie|'\n",
    "    \n",
    "In case you have doubts:\n",
    "'Hypothetical' is specifically about (theoretical) concepts, which means concepts that are not (yet) realized OR\n",
    "concepts that may have occurred in the past. 'Historical' and 'Recent' can be used for realized concepts, in which we also include their negations. \n",
    "I.e. if a concept is explicitly denied historically or recently, we can label it as 'Historical' or 'Recent' respectively.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT_EXPERIENCER = \"\"\"\n",
    "    Je bent een kritische assistent die mij helpt om nieuwe text te bedenken.\n",
    "    Deze text moeten voldoen aan de volgende eisen:\n",
    "    - het moet semantisch correct zijn en vergelijkbaar zijn met de text die ik je geef.\n",
    "    - de voorbeeldtext wordt voorafgegaan door de term VOORBEELDTEKST\n",
    "    - in de voorbeeldtext worden 1 of meer concepten benoemd die verwijzen naar een persoon anders dan de patient, het is belangrijk\n",
    "    dat deze concepten in de nieuwe zin ook verwijzen naar iemand anders dan de patient (zoals een familielid), \n",
    "    het mogen ook andere medische concepten zijn.\n",
    "    - de concepten die je moet vervangen zijn aangegeven met verticale streepjes, dus |concept|.\n",
    "    - Een voorbeeld van een concept wat verwijst naar een ander persoon dan de patient =\n",
    "    'Een zusje van #Name# is elders operatief behandeld in verband met recidiverende patella luxaties', waarin 'luxaties' het concept is, en er \n",
    "    wordt verwezen naar de zus van de patient.    \n",
    "    - het domein is medisch dus gebruik medische concepten.\n",
    "    - probeer de medische concepten te varieren, dus gebruik niet steeds dezelfde concepten.\n",
    "    - varieer de ziektebeelden\n",
    "    - varieer de opmaak van de text, dus gebruik niet steeds dezelfde opmaak.\n",
    "    - geef als antwoord ALLEEN de nieuw gegenereerde text, voorafgaand met de term NIEUWE_TEKST\n",
    "    - in de NIEUWE_TEKST, plaats alleen de concepten die verwijzen naar een ander persoon dan de patient tussen tussen verticale streepjes |, \n",
    "    dus bijvoorbeeld: 'Een zusje van #Name# is elders operatief behandeld in verband met recidiverende patella |luxaties|'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_res(USER_TEXT='Good day', \n",
    "                 SYSTEM_PROMPT=SYSTEM_PROMPT_HYPOTHETICAL, \n",
    "                 n = 10,\n",
    "                 MODEL=\"gpt-4\"):\n",
    "    return OAI_CLIENT.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            n = n,\n",
    "            messages=[\n",
    "                        {\"role\": \"system\",\n",
    "                        \"content\": SYSTEM_PROMPT\n",
    "                        },\n",
    "                        {\"role\": \"user\", \n",
    "                        \"content\": USER_TEXT\n",
    "                        }],\n",
    "            stream=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_VERSION = 'gpt-4-1106-preview'\n",
    "CURRENT_DATE = datetime.datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re_extract = re.compile(r'NIEUWE_ZIN\\:(.*)')\n",
    "if run_temporality:\n",
    "    nieuwe_zinnen_hypothetisch = []\n",
    "    for i, doc in tqdm(enumerate(relevant_docs_hypothetical)):\n",
    "        EXAMPLE = doc['text'].replace('|', ' ')\n",
    "        # add | vertical bars around the concept that needs to be replaced\n",
    "        LOCS = [(d['start'],d['end']) for d in doc['annotations'] \n",
    "                    if d['meta_anns']['Temporality']['value']=='hypothetical']\n",
    "        for loc in LOCS:\n",
    "            EXAMPLE = EXAMPLE[:loc[0]] + '|' + EXAMPLE[loc[0]:loc[1]] + '|' + EXAMPLE[loc[1]:]\n",
    "        \n",
    "        res = get_chat_res(SYSTEM_PROMPT=SYSTEM_PROMPT_HYPOTHETICAL, \n",
    "                        n=10,\n",
    "                        MODEL=GPT_VERSION, # gpt-3.5-turbo-instruct-0914\n",
    "                        USER_TEXT=\"VOORBEELDTEKST: \" + EXAMPLE)\n",
    "\n",
    "        for j, _res in enumerate(res.choices):\n",
    "            txt = _res.message.content\n",
    "            nieuwe_zinnen_hypothetisch.append((\n",
    "                doc['name'],\n",
    "                'hypothetical',\n",
    "                j,\n",
    "                txt[txt.find('NIEUWE_TEKST')+12:].strip()))    \n",
    "    \n",
    "    hypothetical_df = pd.DataFrame(nieuwe_zinnen_hypothetisch, columns=['doc_id', 'class_value', 'synth_num', 'text'])\n",
    "    hypothetical_df.to_parquet(f'../data/synth_temporality_{GPT_VERSION.replace(\"-\", \"_\")}_{CURRENT_DATE}.parquet')\n",
    "else:\n",
    "    hypothetical_df = pd.read_parquet(hypothetical_file)\n",
    "    hypothetical_df['class_value'] = 'hypothetical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re_extract = re.compile(r'NIEUWE_ZIN\\:(.*)')\n",
    "if run_experiencer:\n",
    "    nieuwe_zinnen_experiencer = []\n",
    "    for i, doc in tqdm(enumerate(relevant_docs_experiencer)):\n",
    "        EXAMPLE = doc['text'].replace('|', ' ')\n",
    "        # add | vertical bars around the concept that needs to be replaced\n",
    "        LOCS = [(d['start'],d['end']) for d in doc['annotations'] \n",
    "                    if d['meta_anns']['Experiencer']['value']=='other']\n",
    "        for loc in LOCS:\n",
    "            EXAMPLE = EXAMPLE[:loc[0]] + '|' + EXAMPLE[loc[0]:loc[1]] + '|' + EXAMPLE[loc[1]:]\n",
    "        \n",
    "        res = get_chat_res(SYSTEM_PROMPT=SYSTEM_PROMPT_EXPERIENCER, \n",
    "                        n=10,\n",
    "                        MODEL=GPT_VERSION, # gpt-3.5-turbo-instruct-0914\n",
    "                        USER_TEXT=\"VOORBEELDTEKST: \" + EXAMPLE)\n",
    "\n",
    "        for j, _res in enumerate(res.choices):\n",
    "            txt = _res.message.content\n",
    "            nieuwe_zinnen_experiencer.append((\n",
    "                doc['name'],\n",
    "                'other',\n",
    "                j,\n",
    "                txt[txt.find('NIEUWE_TEKST')+12:].strip()))\n",
    "    experiencer_df = pd.DataFrame(nieuwe_zinnen_experiencer, columns=['doc_id', 'class_value', 'synth_num', 'text'])\n",
    "    experiencer_df.to_parquet(f'../data/synth_experiencer_{GPT_VERSION.replace(\"-\", \"_\")}.parquet')\n",
    "else:\n",
    "    experiencer_df = pd.read_parquet(experiencer_file)\n",
    "    experiencer_df['class_value'] = 'other'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the spans from the synthetic set and add them to the original dataset with an additional label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_in_dict(data, original_documents, Class_name='Experiencer', Class_value=None):\n",
    "    new_documents = original_documents.copy()\n",
    "    \n",
    "    if type(data)==pd.DataFrame:\n",
    "        data = [(r.doc_id, r.class_value, r.synth_num, r.text)  for r in data.itertuples()]\n",
    "    \n",
    "    for i, (name, Class_value_spec, subid, text) in enumerate(data):\n",
    "        Class_value_spec = Class_value_spec if Class_value_spec is not None else Class_value\n",
    "        \n",
    "        clean_text = text.replace('|', '')\n",
    "        _doc = {\n",
    "            'id': i,\n",
    "            'source': 'synthetic',\n",
    "            'source_version': f\"{GPT_VERSION}|{CURRENT_DATE}\",     \n",
    "            'name': f\"{name}|synth|{Class_name}|{subid}\",\n",
    "            'text': clean_text,\n",
    "            'annotations' : []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            for concept_count, match in enumerate(re.finditer('\\|[A-zÀ-ÿ\\s]+\\|', text)):\n",
    "                start, end = match.span()\n",
    "                start_clean = start + 1 -1-concept_count*2 #  (+1,-1)  (+1,-3), (+1,-5), (+1,-7)...\n",
    "                end_clean = end - 1 -1-concept_count*2 # (-1,-1), (-1,-3), (-1,-5), (-1,-7)...\n",
    "                _doc['annotations'].append(\n",
    "                            {\n",
    "                                'id': 1,\n",
    "                                'user': 'emc_dcc_synth',\n",
    "                                'cui': 1,\n",
    "                                'start': start_clean,\n",
    "                                'end': end_clean,\n",
    "                                'value': text[start+1:end-1],\n",
    "                                'validated': False,\n",
    "                                'correct': True,\n",
    "                                'alternative': False,\n",
    "                                'killed': False,\n",
    "                                'meta_anns': {\n",
    "                                    Class_name: {'value': Class_value_spec,\n",
    "                                                    'name': Class_name,\n",
    "                                                    'validated': False,\n",
    "                                                    'acc': 1.0\n",
    "                                                    },\n",
    "                                }            \n",
    "                            }\n",
    "                        )         \n",
    "        except:\n",
    "            print(i, text)\n",
    "        new_documents.append(_doc)\n",
    "    return new_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiencer_df = pd.read_parquet('../data/synth_experiencer_gpt_4_1106_preview_20231214.parquet')\n",
    "hypothetical_df = pd.read_parquet('../data/synth_temporality_gpt_4_1106_preview_20231218.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiencer_df = experiencer_df[['doc_id', 'class_value', 'synth_num', 'text']]\n",
    "hypothetical_df = hypothetical_df[['doc_id', 'class_value', 'synth_num', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs_experiencer = put_in_dict(experiencer_df, update_docs, Class_name='Experiencer', Class_value='other')\n",
    "new_docs_temporality = put_in_dict(hypothetical_df, new_docs_experiencer, Class_name='Temporality', Class_value='hypothetical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6845, 6155)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_docs_temporality), len(new_docs_experiencer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write new samples to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCC['projects'][0]['documents'] = new_docs_temporality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write DCC back to json \n",
    "with open('../data/emc-dcc_ann_Augmented.json', 'w') as f:\n",
    "    json.dump(DCC, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to DCC_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6845/6845 [00:00<00:00, 81225.63it/s]\n"
     ]
    }
   ],
   "source": [
    "texts, labels, ids = preprocess_dcc_for_robbert.get_tuples_from_medcat_json('../data/emc-dcc_ann_Augmented.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Error, in DL1870|synth|Experiencer|8, for line:['T1', 'Other 689 702', 'Fysiotherapie']\n",
      "Index Error, in GP1650|synth|Experiencer|4, for line:['T1', 'Other 126 135', 'hemofilie']\n",
      "Index Error, in GP1905|synth|Experiencer|1, for line:['T1', 'Other 162 191', 'chronische nierinsufficiëntie']\n",
      "Index Error, in GP1905|synth|Experiencer|3, for line:['T1', 'Other 146 157', 'osteoporose']\n",
      "Index Error, in GP2029|synth|Experiencer|5, for line:['T1', 'Other 329 337', 'fenomeen']\n",
      "Index Error, in GP2306|synth|Experiencer|0, for line:['T3', 'Other 403 411', 'infectie']\n",
      "Index Error, in SP1179|synth|Temporality|2, for line:['T4', 'Hypothetical 485 492', 'sedatie']\n",
      "Index Error, in SP1179|synth|Temporality|8, for line:['T6', 'Hypothetical 433 440', 'sedatie']\n",
      "Index Error, in SP1201|synth|Temporality|1, for line:['T3', 'Hypothetical 609 629', 'chronische urticaria']\n",
      "Index Error, in SP1476|synth|Temporality|5, for line:['T3', 'Hypothetical 512 520', 'syndroom']\n",
      "Index Error, in SP1519|synth|Temporality|0, for line:['T6', 'Hypothetical 1156 1179', 'diabetische neuropathie']\n",
      "Index Error, in SP1938|synth|Temporality|1, for line:['T2', 'Hypothetical 393 402', 'coeliakie']\n",
      "Index Error, in SP2053|synth|Temporality|1, for line:['T3', 'Hypothetical 1014 1040', 'opportunistische infecties']\n",
      "Index Error, in SP2053|synth|Temporality|2, for line:['T2', 'Hypothetical 1088 1115', 'risico van cardiotoxiciteit']\n",
      "Index Error, in SP2053|synth|Temporality|9, for line:['T3', 'Hypothetical 1080 1100', 'risico op leukopenie']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, errors = preprocess_dcc_for_robbert.get_dataset(texts, labels, ids)\n",
    "\n",
    "# index error:\n",
    "# only one label, label for word at the end\n",
    "\n",
    "# Mismatch:\n",
    "# \\# preceding, part of compound\n",
    "\n",
    "len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['DL1212', 'Opname', 0, 0, 0, 0, 0, 0],\n",
       " ['DL1212', 'indikatie', 0, 0, 0, 0, 0, 0],\n",
       " ['DL1212', ':', 0, 0, 0, 0, 0, 0],\n",
       " ['DL1212', 'Coxarthrosis', 1, 2, 1, 1, 18, 30],\n",
       " ['DL1212', 'links', 0, 0, 0, 0, 0, 0],\n",
       " ['DL1212', 'bij', 0, 0, 0, 0, 0, 0],\n",
       " ['DL1212', 'oude', 0, 0, 0, 0, 0, 0],\n",
       " ['DL1212', 'Perthes', 0, 0, 0, 0, 0, 0],\n",
       " ['DL1212', '.', 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tProcessed 6845 files\n"
     ]
    }
   ],
   "source": [
    "df, ids = preprocess_dcc_for_robbert.get_dataframe(dataset)\n",
    "print(f\"\\tProcessed {len(set(ids))} files\")\n",
    "df.to_csv(\"../data/DCC_df.csv\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experiencer\n",
       "O        49139\n",
       "Other     1915\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Id.str.contains('Experiencer')].Experiencer.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate English corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* English -- BioScope; [HF](https://huggingface.co/datasets/bigbio/bioscope), [src](https://rgai.inf.u-szeged.hu/downloads)\n",
    "* English -- [Genia](http://www.geniaproject.org/genia-corpus/term-corpus)\n",
    "* English -- Sherlock, SFU review corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
