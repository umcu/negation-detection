{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset (DCC) with a shortage of certain labels. We want to generate \n",
    "new samples synthetically using GPT-4. We will use the following approach:\n",
    "1. We take the existing samples for each document type and present these to GPT-4\n",
    "2. we ask to generate new sentences like it, where the token labels are provided in the BIO format\n",
    "\n",
    "We care specifically about the following labels:\n",
    "* Experiencer: Other\n",
    "* Historical: Hypothetical\n",
    "\n",
    "The task of the GPT model is to generate new sentences that are similar to the input sentences but with variations of the medical concepts. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions\n",
    "The definitions are taken from the ConText/ConTextD papers:\n",
    "\n",
    "## Negation\n",
    "\n",
    "This property has two values, ‘Negated’ or ‘Not negated’. A clinical condition or term is labeled as ‘Negated’ if there is evidence in the text suggesting that the condition does not occur or exist, e.g., ‘There was no sign of sinus infection’, otherwise it is ‘Not negated’.\n",
    "\n",
    "## Temporality\n",
    "\n",
    "The temporality property places a condition along a time line. There are three possible values for this property: ‘Recent’, ‘Historical’, and ‘Hypothetical’. A condition is considered ‘Recent’ if it is maximally 2 weeks old. Conditions that developed more than 2 weeks ago are labeled as ‘Historical’. A condition is labeled as ‘Hypothetical’ if it is not ‘Recent’ or ‘Historical’, e.g., ‘patient should return if she develops fever’ [13].\n",
    "\n",
    "**Adaptation**: *'Hypothetical' is specifically about (theoretical) concepts, concepts that are not (yet) realized, i.e. concepts that may materialize in the future. 'Historical' and 'Recent' can be used for realized concepts, in which we also include their negations. I.e. if a concept is explicitly denied historically or recently, we can label it as 'Historical' or 'Recent' respectively.*\n",
    "\n",
    "## Experiencer\n",
    "\n",
    "Clinical text may refer to subjects other than the actual patient. The experiencer property describes whether the patient experienced the condition or someone else. For simplicity, we have defined only two possible values for this property: ‘Patient’ or ‘Other’, where ‘Other’ refers to anyone but the actual patient, e.g., ‘Mother is recently diagnosed with cancer’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASL#: Experiencer, check labels **->** Temporality(Hypothetical), check labels **->** Temporality(Historical), check labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, re\n",
    "import json, dotenv\n",
    "import pprint\n",
    "\n",
    "import openai\n",
    "import asyncio\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from utils import preprocess_dcc_for_robbert\n",
    "from utils import active_synthesis\n",
    "from utils import synthesis_prompts\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_counts(docs: dict):\n",
    "    class_counts = {\n",
    "                    'Negation':    defaultdict(int),\n",
    "                    'Temporality': defaultdict(int),\n",
    "                    'Experiencer': defaultdict(int)\n",
    "                   }\n",
    "\n",
    "    for doc in docs:\n",
    "        for ann in doc['annotations']:\n",
    "            for _class, val in ann['meta_anns'].items():\n",
    "                class_counts[_class][val['value']] += 1\n",
    "    return class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_hypothetical = False\n",
    "run_experiencer = False\n",
    "run_historical = True\n",
    "run_patient = False\n",
    "run_negation = True\n",
    "\n",
    "experiencer_file = '../data/synth_experiencer_gpt_4_1106_preview_20240207.parquet'\n",
    "hypothetical_file = '../data/synth_temporality_gpt_4_1106_preview_20240209_checked.parquet'\n",
    "historical_file = '../data/synth_historical_gpt_4_1106_preview_20240213_checked.parquet'\n",
    "patient_file = '../data/synth_patient_gpt_4_1106_preview_20240214_checked.parquet'\n",
    "\n",
    "ASL=1\n",
    "\n",
    "DCC_file = '../data/emc-dcc_ann_ADJ.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCC = json.load(open(DCC_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  First load \n",
    "#update_dcc = DCC.copy()\n",
    "#for c in update_dcc['projects'][0]['documents']:\n",
    "#    c['source'] = 'EMC_DCC_ORIGINAL'\n",
    "#with open('../data/emc-dcc_ann_ORIGNAL.json', 'w') as f:\n",
    "#    json.dump(update_dcc, f, indent=2)\n",
    "#docs = update_dcc['projects'][0]['documents']\n",
    "docs = DCC['projects'][0]['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(get_class_counts(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check all to_remove_ jsons\n",
    "remove_dict = json.load(open('../artifacts/to_remove_temporality_base_medroberta_5_32_64__centeredVal_temporality_ASL_1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = active_synthesis.remove_flagged_annotations(docs, remove_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correction_of_original = active_synthesis.Annotation_correction_original\n",
    "#Correction_of_synthetic = active_synthesis.Annotation_correction_synthetic\n",
    "\n",
    "# update the docs\n",
    "'''\n",
    "update_docs = docs.copy()\n",
    "for c in Correction_of_original:\n",
    "    for d in update_docs:        \n",
    "        if d['name']==c['doc_id']:\n",
    "            d['source'] = 'EMC_DCC_ORIGINAL_ADJUSTED'\n",
    "            for a in d['annotations']:\n",
    "                if a['id']==c['annotation_id']:\n",
    "                    a['meta_anns'][c['meta']]['value'] = c['value']\n",
    "# put updated docs in DCC\n",
    "\n",
    "update_docs_ = update_docs.copy()\n",
    "for c in Correction_of_synthetic:\n",
    "    for d in update_docs_:        \n",
    "        if c['doc_id'] in d['name']:\n",
    "            d['source'] = 'synthetic_ADJUSTED'\n",
    "            for a in d['annotations']:\n",
    "                if a['start']==c['start']:\n",
    "                    print(a['start'], c['start'])\n",
    "                    a['meta_anns'][c['meta']]['value'] = c['value']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the source to \"synthetic\" whenever the \"name\" contains \"synth\"\n",
    "#for doc in docs:\n",
    "#    if 'synth' in doc['name']:\n",
    "#        doc['source'] = 'synthetic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_counts = defaultdict(int)\n",
    "for d in docs:\n",
    "    source_counts[d['source']] += 1\n",
    "\n",
    "pprint.pprint(source_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_counts = defaultdict(int)\n",
    "for d in docs:\n",
    "    source_counts[d['source']] += 1\n",
    "\n",
    "pprint.pprint(source_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCC['projects'][0]['documents'] = update_docs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(get_class_counts(update_docs_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write DCC back to json \n",
    "with open('../data/emc-dcc_ann_ADJ.json', 'w') as f:\n",
    "    json.dump(DCC, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minority classes\n",
    "relevant_docs_hypothetical = []\n",
    "for i, doc in enumerate(update_docs_):\n",
    "    for concept in doc['annotations']:\n",
    "        try:\n",
    "            if (concept['meta_anns']['Temporality']['value']=='hypothetical'):\n",
    "                doc['index'] = i\n",
    "                relevant_docs_hypothetical.append(doc)\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "relevant_docs_other = []\n",
    "for i, doc in enumerate(update_docs_):\n",
    "    for concept in doc['annotations']:\n",
    "        try:\n",
    "            if (concept['meta_anns']['Experiencer']['value']=='other'):\n",
    "                doc['index'] = i\n",
    "                relevant_docs_other.append(doc)\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "relevant_docs_historical = []\n",
    "for i, doc in enumerate(update_docs_):\n",
    "    for concept in doc['annotations']:\n",
    "        try:\n",
    "            if (concept['meta_anns']['Temporality']['value']=='historical'):\n",
    "                doc['index'] = i\n",
    "                relevant_docs_historical.append(doc)\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "relevant_docs_patient_names = [d.split(\"_\")[0] for d \n",
    "                               in active_synthesis.Experiencer_patient_ASL1]\n",
    "\n",
    "relevant_docs_patient = []\n",
    "\n",
    "for i, doc in enumerate(update_docs_):\n",
    "    if doc['name'] in relevant_docs_patient_names:\n",
    "        relevant_docs_patient.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(relevant_docs_patient)} patients docs for upsampling\")\n",
    "print(f\"{len(relevant_docs_historical)} historical docs for upsampling\")\n",
    "print(f\"{len(relevant_docs_other)} other docs for upsampling\")\n",
    "print(f\"{len(relevant_docs_hypothetical)} hypothetical docs for upsampling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OAI_ASYNC_CLIENT = AsyncOpenAI(api_key=os.getenv(\"OPENAI_KEY\"), max_retries=2)\n",
    "OAI_CLIENT = OpenAI(api_key=os.getenv(\"OPENAI_KEY\"), max_retries=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_HYPOTHETICAL = synthesis_prompts.SYSTEM_PROMPT_HYPOTHETICAL\n",
    "SYSTEM_PROMPT_HYPOTHETICAL_CHECK = synthesis_prompts.SYSTEM_PROMPT_HYPOTHETICAL_CHECK\n",
    "\n",
    "SYSTEM_PROMPT_EXPERIENCER = synthesis_prompts.SYSTEM_PROMPT_EXPERIENCER\n",
    "SYSTEM_PROMPT_EXPERIENCER_CHECK = synthesis_prompts.SYSTEM_PROMPT_EXPERIENCER_CHECK\n",
    "\n",
    "SYSTEM_PROMPT_HISTORICAL = synthesis_prompts.SYSTEM_PROMPT_HISTORICAL\n",
    "SYSTEM_PROMPT_HISTORICAL_CHECK = synthesis_prompts.SYSTEM_PROMPT_HISTORICAL_CHECK\n",
    "\n",
    "SYSTEM_PROMPT_PATIENT = synthesis_prompts.SYSTEM_PROMPT_PATIENT\n",
    "SYSTEM_PROMPT_PATIENT_CHECK = synthesis_prompts.SYSTEM_PROMPT_PATIENT_CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_res(USER_TEXT='Good day', \n",
    "                 SYSTEM_PROMPT=\"Please be kind in 20 years\", \n",
    "                 n = 5,\n",
    "                 MODEL=\"gpt-4\"):\n",
    "    return OAI_CLIENT.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            n = n,\n",
    "            temperature=0.,\n",
    "            logprobs=True,\n",
    "            messages=[\n",
    "                        {\"role\": \"system\",\n",
    "                        \"content\": SYSTEM_PROMPT\n",
    "                        },\n",
    "                        {\"role\": \"user\", \n",
    "                        \"content\": USER_TEXT\n",
    "                        }],\n",
    "            stream=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prmpt = 'What is the fastest bird alive, and how it be so fast?'\n",
    "sys_prmpt = 'You are a bird expert, and you\\\n",
    "are talking to a friend who is not a bird expert.'\n",
    "\n",
    "test_response = get_chat_res(USER_TEXT=prmpt,\n",
    "                             SYSTEM_PROMPT=sys_prmpt,\n",
    "                             n=10,\n",
    "                             MODEL=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = []\n",
    "for i, r in enumerate(test_response.choices):\n",
    "    _log_probs = []\n",
    "    for t in r.logprobs.content:\n",
    "        _log_probs.append(t.logprob)\n",
    "    log_probs.append(_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(14, 5))\n",
    "for i in range(10):\n",
    "    ax[0].hist(np.exp(log_probs[i]), bins=20, alpha=1,\n",
    "               histtype='step', linewidth=2, label=f\"sample {i}\");\n",
    "    ax[1].scatter(x=np.exp(log_probs[i]), \n",
    "                  y=[i in range(len(log_probs[i]))],\n",
    "                  label=f\"sample {i}\",\n",
    "                  alpha=0.2)\n",
    "plt.legend()\n",
    "plt.suptitle(\"Token proba's\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_VERSION = 'gpt-4-1106-preview'\n",
    "CURRENT_DATE = datetime.datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(DocList: list[dict], \n",
    "                      SystemPrompt: str, \n",
    "                      GPT_version: str='gpt-4-1106-preview',\n",
    "                      write_out: bool=True,\n",
    "                      N_samples: int=5,\n",
    "                      Class_name: str=None)-> pd.DataFrame:\n",
    "    \n",
    "    CURRENT_DATE = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "    new_examples = []\n",
    "    for i, doc in tqdm(enumerate(DocList), total=len(DocList)):\n",
    "        EXAMPLE = doc['text'].replace('|', ' ')\n",
    "        # add | vertical bars around the concept that needs to be replaced\n",
    "        LOCS = [(d['start'],d['end']) for d in doc['annotations'] \n",
    "                    if d['meta_anns']['Temporality']['value']=='hypothetical']\n",
    "        for loc in LOCS:\n",
    "            EXAMPLE = EXAMPLE[:loc[0]] + '|' + EXAMPLE[loc[0]:loc[1]] + '|' + EXAMPLE[loc[1]:]\n",
    "        \n",
    "        res = get_chat_res(SYSTEM_PROMPT=SystemPrompt, \n",
    "                        n=N_samples,\n",
    "                        MODEL=GPT_version, # gpt-3.5-turbo-instruct-0914\n",
    "                        USER_TEXT=\"VOORBEELDTEKST: \" + EXAMPLE)\n",
    "\n",
    "        for j, _res in enumerate(res.choices):\n",
    "            txt = _res.message.content\n",
    "            new_examples.append((\n",
    "                doc['name'],\n",
    "                Class_name,\n",
    "                j,\n",
    "                txt[txt.find('NIEUWE_TEKST')+12:].strip()))    \n",
    "    \n",
    "    new_examples_df = pd.DataFrame(new_examples, columns=['doc_id', 'class_value', 'synth_num', 'text'])\n",
    "    \n",
    "    if write_out:\n",
    "        new_examples_df.to_parquet(f'../data/synth_{Class_name}_{GPT_version.replace(\"-\", \"_\")}_{CURRENT_DATE}_unchecked.parquet')\n",
    "        \n",
    "    return new_examples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re_extract = re.compile(r'NIEUWE_ZIN\\:(.*)')\n",
    "if run_hypothetical:\n",
    "    print(\"Running hypothetical\")\n",
    "    hypothetical_df = generate_examples(relevant_docs_hypothetical, \n",
    "                                        SYSTEM_PROMPT_HYPOTHETICAL, \n",
    "                                        GPT_VERSION, \n",
    "                                        write_out=True, \n",
    "                                        Class_name='hypothetical')\n",
    "#else:\n",
    "#    hypothetical_df = pd.read_parquet(hypothetical_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re_extract = re.compile(r'NIEUWE_ZIN\\:(.*)')\n",
    "if run_experiencer:\n",
    "    print(\"Running experiencer\")\n",
    "    experiencer_df = generate_examples(relevant_docs_other,\n",
    "                                       SYSTEM_PROMPT_EXPERIENCER, \n",
    "                                       GPT_VERSION, \n",
    "                                       write_out=True, \n",
    "                                       Class_name='experiencer')\n",
    "#else:\n",
    "#    experiencer_df = pd.read_parquet(experiencer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_historical:\n",
    "    print(\"Running historical\")\n",
    "    historical_df = generate_examples(relevant_docs_historical, \n",
    "                                      SYSTEM_PROMPT_HISTORICAL, \n",
    "                                      GPT_VERSION, \n",
    "                                      write_out=True, \n",
    "                                      Class_name='historical')\n",
    "#else:\n",
    "#    try:\n",
    "#        historical_df = pd.read_parquet(historical_file)\n",
    "#    except:\n",
    "#        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_patient:\n",
    "    print(\"Running patient\")\n",
    "    patient_df = generate_examples(relevant_docs_patient,\n",
    "                                   SYSTEM_PROMPT_PATIENT, \n",
    "                                   GPT_VERSION, \n",
    "                                   write_out=True, \n",
    "                                   Class_name='patient')\n",
    "#else:\n",
    "#    try:\n",
    "#        patient_df = pd.read_parquet(patient_file)\n",
    "#    except:\n",
    "#        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the checks\n",
    "def check_by_gpt(TXT: str='', \n",
    "                 N_checks: int=3,\n",
    "                 SYSTEM_PROMPT: str=\"Respect humans\",\n",
    "                 GPT_VERSION: str='gpt-4-1106-preview'\n",
    "                 )->str:\n",
    "    N_checks = 7\n",
    "    maj_vote = N_checks//2\n",
    "\n",
    "    RES = get_chat_res(SYSTEM_PROMPT=SYSTEM_PROMPT,\n",
    "                        USER_TEXT=\"VOORBEELDTEKST: \" + TXT,\n",
    "                        n=N_checks,\n",
    "                        MODEL=GPT_VERSION)\n",
    "\n",
    "    RES_sel = []\n",
    "    no_count = defaultdict(int)\n",
    "    list_lens = []\n",
    "    for j, _res in enumerate(RES.choices):\n",
    "        txt = _res.message.content\n",
    "        try:\n",
    "            _d = eval(txt)\n",
    "        except:\n",
    "            continue\n",
    "        RES_sel.append((j, _d))\n",
    "        \n",
    "        for k, v in _d.items():\n",
    "            if v=='nee':\n",
    "                no_count[k] += 1\n",
    "        \n",
    "        list_lens.append(len(_d.keys())) \n",
    "\n",
    "    if len(set(list_lens))>1:\n",
    "        return 'ERROR-checker concept count mismatch'\n",
    "    elif len(set(list_lens))==1:\n",
    "        if len(no_count.values())>0:    \n",
    "            # approach: if any of the concept is deemed incorrect we flag the TXT for removal \n",
    "            num_exc = sum([_v>maj_vote for _v in no_count.values()])\n",
    "            if num_exc>0:\n",
    "                if num_exc == len(_d.keys()):\n",
    "                    return False\n",
    "                else:\n",
    "                    spans = []                    \n",
    "                    for r in re.finditer(r'(\\|.*?\\|)', TXT):\n",
    "                        spans.append(r.span())  \n",
    "                    if len(spans)<max(_d.keys()):\n",
    "                        return 'ERROR-checker concept count mismatch'              \n",
    "                    rem = []\n",
    "                    for k, v in no_count.items():\n",
    "                        if v>maj_vote:\n",
    "                            #print(spans, no_count)\n",
    "                            rem.append(spans[k])\n",
    "                    if len(rem)>0:\n",
    "                        for rcount, r in enumerate(rem):\n",
    "                            TXT = TXT[:r[0]-rcount*2]+\\\n",
    "                                TXT[r[0]+1-rcount*2:r[1]-1-rcount*2]+\\\n",
    "                                TXT[r[1]-rcount*2:]\n",
    "    else:\n",
    "        return False\n",
    "    return TXT\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_synthetic_check(SyntDf: pd.DataFrame, \n",
    "                            NumChecks=3, \n",
    "                            SystemPrompt=\"Don't kill us, please\",\n",
    "                            GPT_VERSION =\"gpt-4-1106-preview\",\n",
    "                            text_checked: list=None)-> pd.DataFrame:\n",
    "    if (text_checked is not None):\n",
    "        assert len(text_checked)==SyntDf.shape[0], \\\n",
    "        \"text_checked should have the same length as SyntDf\"\n",
    "    else:\n",
    "        text_checked = [None]*SyntDf.shape[0]\n",
    "    texts = SyntDf['text'].values\n",
    "    \n",
    "    for k, text in enumerate(texts):\n",
    "        if text_checked[k] is None:\n",
    "            yield check_by_gpt(text, \n",
    "                            N_checks=NumChecks, \n",
    "                            GPT_VERSION=GPT_VERSION,\n",
    "                            SYSTEM_PROMPT=SystemPrompt)\n",
    "        else:\n",
    "            yield text_checked[k]\n",
    "\n",
    "\n",
    "def apply_checks(SyntDf: pd.DataFrame, \n",
    "                Class_name: str=\"Experiencer\",\n",
    "                text_checked: list=None,\n",
    "                write_out: bool=True)-> pd.DataFrame:\n",
    "            \n",
    "    SyntDf['checked_text'] = text_checked\n",
    "\n",
    "    c1 = SyntDf['checked_text']!=False \n",
    "    c2 = SyntDf['checked_text']!='ERROR-checker concept count mismatch'\n",
    "\n",
    "    SyntDf = SyntDf[c1 & c2]\n",
    "\n",
    "    # check token length\n",
    "    SyntDf['token_len'] = SyntDf['checked_text'].astype(str)\\\n",
    "                                .progress_apply(lambda x:\n",
    "                                    len(x.split()))\n",
    "\n",
    "    # check number of r'\\|.*?\\|' in the text\n",
    "    SyntDf['n_concepts'] = SyntDf['checked_text'].astype(str)\\\n",
    "                                .progress_apply(lambda x:\n",
    "                                    len(re.findall(r'\\|.*?\\|', x)))\n",
    "                                \n",
    "    c = SyntDf['n_concepts']>0\n",
    "    SyntDf = SyntDf[c]\n",
    "\n",
    "    SyntDf = SyntDf.drop(['text'], axis=1)\n",
    "    SyntDf = SyntDf.rename(columns={'checked_text': 'text'})\n",
    "    if write_out:\n",
    "        SyntDf.to_parquet(f'../data/synth_{Class_name}_{GPT_VERSION.replace(\"-\", \"_\")}_{CURRENT_DATE}_checked.parquet')\n",
    "    return SyntDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked_historical = [None]*historical_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_historical:\n",
    "    print(\"Checking historical\")    \n",
    "    checker = perform_synthetic_check(historical_df, \n",
    "                                        NumChecks=3, \n",
    "                                        SystemPrompt=SYSTEM_PROMPT_HISTORICAL_CHECK, \n",
    "                                        GPT_VERSION=GPT_VERSION,\n",
    "                                        text_checked=checked_historical)\n",
    "    for k, res in tqdm(enumerate(checker), total=historical_df.shape[0]):\n",
    "        checked_historical[k] = res    \n",
    "    \n",
    "    \n",
    "    print(\"Apply checks\")\n",
    "    historical_df_checked = apply_checks(historical_df, \n",
    "                                        Class_name='historical', \n",
    "                                        text_checked=checked_historical,\n",
    "                                        write_out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked_patient = [None]*patient_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_patient:\n",
    "    print(\"Checking patient\")    \n",
    "    checker = perform_synthetic_check(patient_df, \n",
    "                                        NumChecks=3, \n",
    "                                        SystemPrompt=SYSTEM_PROMPT_PATIENT_CHECK, \n",
    "                                        GPT_VERSION=GPT_VERSION,\n",
    "                                        text_checked=checked_patient)\n",
    "    for k, res in tqdm(enumerate(checker), total=patient_df.shape[0]):\n",
    "        checked_patient[k] = res    \n",
    "    \n",
    "    \n",
    "    print(\"Apply checks\")\n",
    "    patient_df_checked = apply_checks(patient_df, \n",
    "                                        Class_name='patient', \n",
    "                                        text_checked=checked_patient,\n",
    "                                        write_out=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the spans from the synthetic set and add them to the original dataset with an additional label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_in_dict(data, original_documents, Class_name='Experiencer', Class_value=None):\n",
    "    max_id = max([int(k['id']) for d in original_documents\n",
    "                           for k in d['annotations'] if k['id'] is not None])\n",
    "    \n",
    "    new_documents = original_documents.copy()\n",
    "    \n",
    "    if type(data)==pd.DataFrame:\n",
    "        data = [(r.doc_id, r.class_value, r.synth_num, r.text)  for r in data.itertuples()]\n",
    "    \n",
    "    for i, (name, Class_value_spec, subid, text) in enumerate(data): # start=max_id+1\n",
    "        Class_value_spec = Class_value_spec if Class_value_spec is not None else Class_value\n",
    "        \n",
    "        if i == 0:\n",
    "            true_i = i + max_id + 1\n",
    "        else:\n",
    "            true_i = true_i + 1\n",
    "        \n",
    "        clean_text = text.replace('|', '')\n",
    "        _doc = {\n",
    "            'id': true_i,\n",
    "            'source': 'synthetic',\n",
    "            'source_version': f\"{GPT_VERSION}|{CURRENT_DATE}\",     \n",
    "            'name': f\"{name}|synth|{Class_name}|{subid}|{CURRENT_DATE}\",\n",
    "            'text': clean_text,\n",
    "            'annotations' : []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            for concept_count, match in enumerate(re.finditer('\\|[A-zÀ-ÿ\\s]+\\|', text)):\n",
    "                start, end = match.span()\n",
    "                start_clean = start + 1 -1-concept_count*2 #  (+1,-1)  (+1,-3), (+1,-5), (+1,-7)...\n",
    "                end_clean = end - 1 -1-concept_count*2 # (-1,-1), (-1,-3), (-1,-5), (-1,-7)...\n",
    "                true_i = true_i + 1\n",
    "                _doc['annotations'].append(\n",
    "                            {\n",
    "                                'id': true_i,\n",
    "                                'user': 'emc_dcc_synth',\n",
    "                                'cui': 1,\n",
    "                                'start': start_clean,\n",
    "                                'end': end_clean,\n",
    "                                'value': text[start+1:end-1],\n",
    "                                'validated': False,\n",
    "                                'correct': True,\n",
    "                                'alternative': False,\n",
    "                                'killed': False,\n",
    "                                'meta_anns': {\n",
    "                                    Class_name: {'value': Class_value_spec,\n",
    "                                                    'name': Class_name,\n",
    "                                                    'validated': False,\n",
    "                                                    'acc': 1.0\n",
    "                                                    },\n",
    "                                }            \n",
    "                            }\n",
    "                        )         \n",
    "        except:\n",
    "            print(i, text)\n",
    "        new_documents.append(_doc)\n",
    "    return new_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiencer:\n",
    "    experiencer_df = experiencer_df_checked[['doc_id', 'class_value', 'synth_num', 'text']]\n",
    "if run_hypothetical:\n",
    "    hypothetical_df = hypothetical_df_checked[['doc_id', 'class_value', 'synth_num', 'text']]\n",
    "if run_historical:\n",
    "    historical_df = historical_df_checked[['doc_id', 'class_value', 'synth_num', 'text']]\n",
    "if run_patient:\n",
    "    patient_df = patient_df_checked[['doc_id', 'class_value', 'synth_num', 'text']]\n",
    "    patient_df['class_value'] = 'patient'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get class count of update_docs_\n",
    "pprint.pprint(get_class_counts(update_docs_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs = update_docs_.copy()\n",
    "if run_experiencer:\n",
    "    new_docs = put_in_dict(experiencer_df, new_docs.copy(), \n",
    "                                    Class_name='Experiencer', \n",
    "                                    Class_value='other')\n",
    "if run_hypothetical:\n",
    "    new_docs = put_in_dict(hypothetical_df, new_docs.copy(),\n",
    "                                   Class_name='Temporality', \n",
    "                                   Class_value='hypothetical')\n",
    "if run_historical:\n",
    "    new_docs = put_in_dict(historical_df, new_docs.copy(),\n",
    "                                   Class_name='Temporality', \n",
    "                                   Class_value='historical')\n",
    "    \n",
    "if run_patient:\n",
    "    new_docs = put_in_dict(patient_df, new_docs.copy(),\n",
    "                                   Class_name='Experiencer', \n",
    "                                   Class_value='patient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(get_class_counts(new_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = max([int(k['id']) for d in update_docs\n",
    "                           for k in d['annotations'] \n",
    "                           if k['id'] is not None])\n",
    "\n",
    "print(f\"Number of annotations in original DCC: {max_id}\") \n",
    "\n",
    "max_id = max([int(k['id']) for d in new_docs\n",
    "                           for k in d['annotations'] \n",
    "                           if k['id'] is not None])\n",
    "\n",
    "print(f\"Number of annotations in synthetic augmented DCC: {max_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write new samples to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCC['projects'][0]['documents'] = new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '../data/emc-dcc_ann_Augmented_ASL1_Historical.json'\n",
    "\n",
    "# write DCC back to json \n",
    "with open(fn, 'w', encoding='utf-8') as f:\n",
    "    json.dump(DCC, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '../data/emc-dcc_ann_ADJ.json'\n",
    "\n",
    "# write DCC back to json \n",
    "with open(fn, 'w', encoding='utf-8') as f:\n",
    "    json.dump(DCC, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to DCC_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels, ids = preprocess_dcc_for_robbert.get_tuples_from_medcat_json(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, errors = preprocess_dcc_for_robbert.get_dataset(texts, labels, ids)\n",
    "\n",
    "# index error:\n",
    "# only one label, label for word at the end\n",
    "\n",
    "# Mismatch:\n",
    "# \\# preceding, part of compound\n",
    "\n",
    "len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, ids = preprocess_dcc_for_robbert.get_dataframe(dataset)\n",
    "print(f\"\\tProcessed {len(set(ids))} files\")\n",
    "df.to_csv(\"../data/DCC_df_ASL1_Historical.csv\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Experiencer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Temporality.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Negation.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate English corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* English -- BioScope; [HF](https://huggingface.co/datasets/bigbio/bioscope), [src](https://rgai.inf.u-szeged.hu/downloads)\n",
    "* English -- [Genia](http://www.geniaproject.org/genia-corpus/term-corpus)\n",
    "* English -- Sherlock, SFU review corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
