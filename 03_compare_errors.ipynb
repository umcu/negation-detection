{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e9a40f",
   "metadata": {},
   "source": [
    "# Compare errors and errors categories\n",
    "In previous notebooks we did cross validation for a set of entities to show how well our methods are able to predict a negation. Afterwards we extracted the errors per fold, manually checked what kind of errors were made and grouped the errors into categories. This notebook compares the errors and error categories between the different methods, to see whether certain types of errors are made by specific methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ad49f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "\n",
    "# Solve issue with PDF images https://github.com/plotly/plotly.py/issues/3469\n",
    "pio.kaleido.scope.mathjax = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7e1a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pretty names for plots\n",
    "pretty_names_error_methods = {'rule_based': 'Rule-based',\n",
    "                              'bilstm': 'BiLSTM',\n",
    "                              'robbert': 'RobBERT',\n",
    "                              'bilstm_and_rule_based': 'BiLSTM & rule-based',\n",
    "                              'robbert_and_rule_based': 'RobBERT & rule-based',\n",
    "                              'robbert_and_bilstm': 'RobBERT & BiLSTM',\n",
    "                              'all': 'All'}\n",
    "\n",
    "pretty_names_error_categories = {'annotation_error': 'Annotation error',\n",
    "                                 'negation_of_different_term': 'Negation of different term',\n",
    "                                 'ambiguous': 'Ambiguous',\n",
    "                                 'speculation': 'Speculation',\n",
    "                                 'other': 'Other', \n",
    "                                 'punctuation': 'Punctuation',\n",
    "                                 'minus': 'Minus',\n",
    "                                 'scope': 'Scope',\n",
    "                                 'uncommon_negation': 'Uncommon negation',\n",
    "                                 'wrong_modality': 'Wrong modality'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf40d4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set file paths\n",
    "error_analysis_dir = Path('data/error_analyses/')\n",
    "fp_rule_based_file = error_analysis_dir / 'false-positives_rule-based.csv'\n",
    "fn_rule_based_file = error_analysis_dir / 'false-negatives_rule-based.csv'\n",
    "fp_bilstm_file = error_analysis_dir / 'false-positives_bilstm.ods'\n",
    "fn_bilstm_file = error_analysis_dir / 'false-negatives_bilstm.ods'\n",
    "fp_robbert_file = error_analysis_dir / 'false-positives_robbert.csv'\n",
    "fn_robbert_file = error_analysis_dir / 'false-negatives_robbert.csv'\n",
    "\n",
    "figure_dir = Path('figures')\n",
    "figure_dir.mkdir(exist_ok=True)\n",
    "\n",
    "predictions_file = Path('results/merged_predictions.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open files and normalize\n",
    "fp_rule_based = pd.read_csv(fp_rule_based_file, sep=';', usecols=['entity_id', 'category'], index_col='entity_id')\n",
    "fp_rule_based.rename(columns={'category': 'rule_based'}, inplace=True)\n",
    "fp_rule_based.replace(' ', '_', regex=True, inplace=True)\n",
    "\n",
    "fn_rule_based = pd.read_csv(fn_rule_based_file, sep=';', usecols=['entity_id', 'category'], index_col='entity_id')\n",
    "fn_rule_based.rename(columns={'category': 'rule_based'}, inplace=True)\n",
    "fn_rule_based.replace(' ', '_', regex=True, inplace=True)\n",
    "\n",
    "fp_bilstm = pd.read_excel(fp_bilstm_file, usecols=['entity id','category'], index_col='entity id')\n",
    "fp_bilstm.rename(columns={'category': 'bilstm'}, inplace=True)\n",
    "fp_bilstm.index.name = 'entity_id'\n",
    "fp_bilstm.replace(' ', '_', regex=True, inplace=True)\n",
    "\n",
    "fn_bilstm = pd.read_excel(fn_bilstm_file, usecols=['entity id','category'], index_col='entity id')\n",
    "fn_bilstm.rename(columns={'category': 'bilstm'}, inplace=True)\n",
    "fn_bilstm.index.name = 'entity_id'\n",
    "fn_bilstm.replace(' ', '_', regex=True, inplace=True)\n",
    "\n",
    "fp_robbert = pd.read_csv(fp_robbert_file, sep=',', usecols=['entity_id','error_type'], index_col='entity_id')\n",
    "fp_robbert.rename(columns={'error_type': 'robbert'}, inplace=True)\n",
    "\n",
    "fn_robbert = pd.read_csv(fn_robbert_file, sep=',', usecols=['entity_id','error_type'], index_col='entity_id')\n",
    "fn_robbert.rename(columns={'error_type': 'robbert'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2e701d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print statistics per method\n",
    "print(f'Number of false positives using rule based: {fp_rule_based.shape[0]}')\n",
    "print(f'Number of false negatives using rule based: {fn_rule_based.shape[0]}')\n",
    "print(f'Number of false positives using BiLSTM: {fp_bilstm.shape[0]}')\n",
    "print(f'Number of false negatives using BiLSTM: {fn_bilstm.shape[0]}')\n",
    "print(f'Number of false positives using RoBBERT: {fp_robbert.shape[0]}')\n",
    "print(f'Number of false negatives using RoBBERT: {fn_robbert.shape[0]}')\n",
    "print(f'Total number of errors: {fp_rule_based.shape[0] + fn_rule_based.shape[0] + fp_bilstm.shape[0] + fn_bilstm.shape[0] +fp_robbert.shape[0] +fn_robbert.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452bf5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_table(rule_based, bilstm, robbert):\n",
    "    count_table = pd.DataFrame({'rule_based': rule_based.value_counts(), \n",
    "                                'bilstm': bilstm.value_counts(), \n",
    "                                'robbert': robbert.value_counts()})\n",
    "    count_table.fillna(0, inplace=True)\n",
    "    count_table.reset_index(level=[0],inplace=True)\n",
    "    count_table.set_index('level_0', inplace=True)\n",
    "    count_table.index.name = None\n",
    "    count_table.loc['Total']= count_table.sum(numeric_only=True, axis=0)   \n",
    "    count_table = count_table.astype('int').astype('string')\n",
    "    count_table.rename(pretty_names_error_categories, inplace=True)\n",
    "    count_table.rename(columns=pretty_names_error_methods, inplace=True)\n",
    "    return count_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8767da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_table_with_perc(rule_based, bilstm, robbert):\n",
    "    count_table = pd.DataFrame({'rule_based': rule_based.value_counts(),\n",
    "                                'rule_based_perc': round(rule_based.value_counts(normalize=True), 2) * 100,\n",
    "                                'bilstm': bilstm.value_counts(), \n",
    "                                'bilstm_perc': round(bilstm.value_counts(normalize=True), 2) * 100, \n",
    "                                'robbert': robbert.value_counts(),\n",
    "                                'robbert_perc': round(robbert.value_counts(normalize=True), 2) * 100})\n",
    "    count_table.fillna(0, inplace=True)\n",
    "    count_table.reset_index(level=[0],inplace=True)\n",
    "    count_table.set_index('level_0', inplace=True)\n",
    "    count_table.index.name = None\n",
    "    count_table.loc['Total']= count_table.sum(numeric_only=True, axis=0)   \n",
    "    count_table = count_table.astype('int').astype('string')\n",
    "    count_table.rename(pretty_names_error_categories, inplace=True)\n",
    "    count_table.rename(columns=pretty_names_error_methods, inplace=True)\n",
    "    return count_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f549318",
   "metadata": {},
   "source": [
    "## Create unfiltered count tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8792a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# false_positive_counts = create_count_table(fp_rule_based, fp_bilstm, fp_robbert)\n",
    "# false_positive_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c48b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# false_negative_counts = create_count_table(fn_rule_based, fn_bilstm, fn_robbert)\n",
    "# false_negative_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b961a765",
   "metadata": {},
   "source": [
    "## Filter entities\n",
    "Remove entities that don't have a prediction in all methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd820b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entities that have a prediction in all methods\n",
    "predictions = pd.read_csv(predictions_file, usecols=['entity_id'])\n",
    "entities = predictions.entity_id.to_list()\n",
    "print(f'Total number of entities with a prediction: {len(entities)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82fbeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_rule_based = fp_rule_based[fp_rule_based.index.isin(entities)]\n",
    "fn_rule_based = fn_rule_based[fn_rule_based.index.isin(entities)]\n",
    "fp_bilstm = fp_bilstm[fp_bilstm.index.isin(entities)]\n",
    "fn_bilstm = fn_bilstm[fn_bilstm.index.isin(entities)]\n",
    "fp_robbert = fp_robbert[fp_robbert.index.isin(entities)]\n",
    "fn_robbert = fn_robbert[fn_robbert.index.isin(entities)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1537dc43",
   "metadata": {},
   "source": [
    "## Create filtered count tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_counts = create_count_table(fp_rule_based, fp_bilstm, fp_robbert)\n",
    "# false_positive_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff13fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(false_positive_counts.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positives\n",
    "false_positive_counts_perc = create_count_table_with_perc(fp_rule_based, fp_bilstm, fp_robbert)\n",
    "false_positive_counts_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(false_positive_counts_perc.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62556ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negative_counts = create_count_table(fn_rule_based, fn_bilstm, fn_robbert)\n",
    "# false_negative_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8480f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False negatives\n",
    "false_negative_counts_perc = create_count_table_with_perc(fn_rule_based, fn_bilstm, fn_robbert)\n",
    "false_negative_counts_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096bb8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(false_negative_counts_perc.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4ad4a",
   "metadata": {},
   "source": [
    "## Concat false positives and negatives\n",
    "For creating figures and comparing error categories, we concatenated false positives and negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b910d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_rule_based = pd.concat([fp_rule_based, fn_rule_based])\n",
    "errors_bilstm = pd.concat([fp_bilstm, fn_bilstm])\n",
    "errors_robbert = pd.concat([fp_robbert, fn_robbert])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d3ba8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merge all errors based on entity_id\n",
    "error_categories = pd.concat([errors_rule_based, errors_bilstm, errors_robbert], axis=1)\n",
    "print(f'Number of entities with an error in at least 1 method: {error_categories.shape[0]}')\n",
    "error_categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee12bfa",
   "metadata": {},
   "source": [
    "## Methods compared based on errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb946169",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert errors to binary format\n",
    "errors = error_categories.notna()\n",
    "errors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b72b5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def check_errors_multiple_methods(row):\n",
    "    \"\"\"Check whether errors are made in multiple methods.\n",
    "    \"\"\"\n",
    "    robbert_error = row['robbert']\n",
    "    bilstm_error = row['bilstm']\n",
    "    rule_based_error = row['rule_based']\n",
    "\n",
    "    if robbert_error and not bilstm_error and not rule_based_error:\n",
    "        return 'robbert'\n",
    "    if not robbert_error and bilstm_error and not rule_based_error:\n",
    "        return 'bilstm'\n",
    "    if not robbert_error and not bilstm_error and rule_based_error:\n",
    "        return 'rule_based'\n",
    "    \n",
    "    if robbert_error and bilstm_error and not rule_based_error:\n",
    "        return 'robbert_and_bilstm'\n",
    "    if robbert_error and not bilstm_error and rule_based_error:\n",
    "        return 'robbert_and_rule_based'\n",
    "    if not robbert_error and bilstm_error and rule_based_error:\n",
    "        return 'bilstm_and_rule_based'\n",
    "\n",
    "    return 'all'\n",
    "\n",
    "errors['errors'] = errors.apply(check_errors_multiple_methods, axis=1)\n",
    "errors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15420a45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a table of the number of errors per method\n",
    "errors_counts = errors.errors.value_counts()\n",
    "errors_counts=errors_counts.reindex([\"all\", \"rule_based\", \"bilstm\", \"robbert\", \"bilstm_and_rule_based\", \"robbert_and_rule_based\", \"robbert_and_bilstm\"])\n",
    "errors_counts.rename(pretty_names_error_methods, inplace=True)\n",
    "errors_counts.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b266702a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot whether methods make the same or different errors\n",
    "fig = px.bar(errors_counts,\n",
    "             title='Methods compared based on errors',\n",
    "             labels={'index': 'Method',\n",
    "                     'value': 'Number of errors'},\n",
    "             template='plotly_white')\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_layout(font_family=\"Serif\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9678c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.write_image(fig, figure_dir / 'fig1-methods-compared-on-errors.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011139e6",
   "metadata": {},
   "source": [
    "## Error categories compared between methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37a1356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create long format table of errors per method & category\n",
    "error_categories_m = error_categories.melt(ignore_index=False, var_name='Method', value_name='Category')\n",
    "error_categories_m.dropna(inplace=True)\n",
    "print(f'Total number of errors: {error_categories_m.shape[0]}')\n",
    "error_categories_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dabefd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add pretty names\n",
    "error_categories_m['Method'].replace(pretty_names_error_methods, inplace=True)\n",
    "error_categories_m['Category'].replace(pretty_names_error_categories, inplace=True)\n",
    "error_categories_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28355533",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create wide formatted table of number of errors per category & method\n",
    "error_category_counts = error_categories_m.value_counts().reset_index()\n",
    "error_category_counts.rename(columns={0: 'Count'}, inplace=True)\n",
    "error_category_counts.pivot(columns='Category', index='Method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e36c07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot errors per category\n",
    "fig = px.bar(error_category_counts, x='Category', color='Method', y='Count',\n",
    "             title='Error per category',\n",
    "             labels={'count': 'Number of errors'},\n",
    "             template='plotly_white')\n",
    "fig.update_layout(font_family=\"Serif\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e73d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.write_image(fig, figure_dir / 'fig2-errors-per-category.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fbf4ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot errors per method\n",
    "fig = px.bar(error_category_counts, x='Method', color='Category', y='Count',\n",
    "             title='Errors per method',\n",
    "             labels={'count': 'Number of errors'},\n",
    "             template='plotly_white')\n",
    "fig.update_layout(font_family=\"Serif\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab2e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.write_image(fig, figure_dir / 'fig3-errors-per-prediction-method.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45072bd",
   "metadata": {},
   "source": [
    "## Check error categorization\n",
    "A few errors were made by multiple methods. We categorized all errors per method, so there could be a discrepency of how these were categorized. This section assesses this difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfcbe30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Count number of errors in at least 2 methods\n",
    "errors_at_least_2_methods = error_categories[error_categories.notna().sum(axis=1) >= 2].copy()\n",
    "errors_at_least_2_methods.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b0af55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Total number of errors: {errors.shape[0]}')\n",
    "number_errors = errors_at_least_2_methods.shape[0]\n",
    "print(f'Number of errors in at least 2 methods: {number_errors} ({round((number_errors / errors.shape[0])*100,2)}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc1165",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def check_categorization(row):\n",
    "    \"\"\"Check whether the different annotators agree on the category of common errors.\"\"\"\n",
    "    categories = [category for category in [row.robbert, row.bilstm, row.rule_based] if category is not np.NaN]\n",
    "    if len(set(categories)) > 1:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "errors_at_least_2_methods['annotators_agree'] = errors_at_least_2_methods.apply(check_categorization, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac31e9b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "in_agreement = sum(errors_at_least_2_methods[\"annotators_agree\"])\n",
    "print(f'Number of errors for which annotators assigned same category: {in_agreement} ({round((in_agreement / number_errors)*100,2)}%)')\n",
    "not_in_agreement = errors_at_least_2_methods.shape[0] - sum(errors_at_least_2_methods[\"annotators_agree\"])\n",
    "print(f'Number of errors for which annotators assigned different category: {not_in_agreement} ({round((not_in_agreement / number_errors)*100,2)}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_at_least_2_methods[~errors_at_least_2_methods[\"annotators_agree\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571c7947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_error_category_combination(row):\n",
    "    return \" & \".join(set([x for x in [row['rule_based'], row['bilstm'], row['robbert']] if not pd.isna(x)]))\n",
    "\n",
    "errors_at_least_2_methods.replace(pretty_names_error_categories, inplace=True)\n",
    "errors_at_least_2_methods['combinations'] = errors_at_least_2_methods.apply(extract_error_category_combination, axis=1)\n",
    "errors_at_least_2_methods.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601be25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "different_errors_at_least_2_methods = errors_at_least_2_methods[~errors_at_least_2_methods.annotators_agree].copy()\n",
    "different_errors_at_least_2_methods.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c99227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a table of the number of errors per method\n",
    "combinations_counts = different_errors_at_least_2_methods.combinations.value_counts()\n",
    "\n",
    "# Show head in DataFrame\n",
    "combinations_counts.reset_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0300e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the counts\n",
    "fig = px.bar(combinations_counts.head(6),\n",
    "             title='Combinations of labeled errors',\n",
    "             labels={'index': 'Combination',\n",
    "                     'value': 'Number of occurences'},\n",
    "             template='plotly_white')\n",
    "fig.update_layout(showlegend=False,\n",
    "                  font_family=\"Serif\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a985c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.write_image(fig, figure_dir / 'fig4-errors-category-combinations.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42de581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paired = errors_at_least_2_methods[['rule_based','bilstm']].copy().dropna()\n",
    "print(f\"Cohen's Kappa score rule based & BiLSTM:  {round(cohen_kappa_score(paired.rule_based, paired.bilstm),2)}\")\n",
    "\n",
    "paired = errors_at_least_2_methods[['rule_based','robbert']].copy().dropna()\n",
    "print(f\"Cohen's Kappa score rule based & RobBERT: {round(cohen_kappa_score(paired.rule_based, paired.robbert),2)}\")\n",
    "\n",
    "paired = errors_at_least_2_methods[['bilstm','robbert']].copy().dropna()\n",
    "print(f\"Cohen's Kappa score BiLSTM & RobBERT:     {round(cohen_kappa_score(paired.bilstm, paired.robbert),2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
